<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jeff Helzner">
<meta name="dcterms.date" content="2026-02-15">
<meta name="description" content="A formal specification of the softmax choice model with sensitivity parameter α, including complete proofs of the three fundamental properties that characterize sensitivity to value maximization.">

<title>Abstract Formulation of the SEU Sensitivity Model – SEU Sensitivity</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-4d9afe2b8d18ee9fa5d0d57b5ed4214d.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-8bffb863ba02556ae9cbd60282933685.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-b681a0ec939e67307781ecabf18bfad0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-8bffb863ba02556ae9cbd60282933685.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles/seu-sensitivity.css">
<meta name="citation_title" content="Abstract Formulation of the SEU Sensitivity Model">
<meta name="citation_author" content="Jeff Helzner">
<meta name="citation_publication_date" content="2026-02-15">
<meta name="citation_cover_date" content="2026-02-15">
<meta name="citation_year" content="2026">
<meta name="citation_online_date" content="2026-02-15">
<meta name="citation_fulltext_html_url" content="https://jeffhelzner.github.io/seu-sensitivity/foundations/01_abstract_formulation.html">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="SEU Sensitivity Project">
<meta name="citation_reference" content="citation_title=Theory of games and economic behavior;,citation_author=John Neumann;,citation_author=Oskar Morgenstern;,citation_publication_date=1947;,citation_cover_date=1947;,citation_year=1947;">
<meta name="citation_reference" content="citation_title=The foundations of statistics;,citation_author=Leonard J. Savage;,citation_publication_date=1954;,citation_cover_date=1954;,citation_year=1954;,citation_publisher=John Wiley &amp;amp;amp; Sons;">
<meta name="citation_reference" content="citation_title=Individual choice behavior: A theoretical analysis;,citation_author=R. Duncan Luce;,citation_publication_date=1959;,citation_cover_date=1959;,citation_year=1959;,citation_publisher=John Wiley &amp;amp;amp; Sons;">
<meta name="citation_reference" content="citation_title=Conditional logit analysis of qualitative choice behavior;,citation_author=Daniel McFadden;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_journal_title=Frontiers in Econometrics;,citation_publisher=Academic Press;">
<meta name="citation_reference" content="citation_title=Discrete choice methods with simulation;,citation_author=Kenneth E. Train;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_publisher=Cambridge University Press;">
<meta name="citation_reference" content="citation_title=Validating Bayesian inference algorithms with simulation-based calibration;,citation_author=Sean Talts;,citation_author=Michael Betancourt;,citation_author=Daniel Simpson;,citation_author=Aki Vehtari;,citation_author=Andrew Gelman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_journal_title=arXiv preprint arXiv:1804.06788;">
<meta name="citation_reference" content="citation_title=Bayesian workflow;,citation_author=Andrew Gelman;,citation_author=Aki Vehtari;,citation_author=Daniel Simpson;,citation_author=Charles C. Margossian;,citation_author=Bob Carpenter;,citation_author=Yuling Yao;,citation_author=Lauren Kennedy;,citation_author=Jonah Gabry;,citation_author=Paul-Christian Bürkner;,citation_author=Martin Modrák;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2011.01808;">
<meta name="citation_reference" content="citation_title=Stan: A probabilistic programming language;,citation_author=Bob Carpenter;,citation_author=Andrew Gelman;,citation_author=Matthew D. Hoffman;,citation_author=Daniel Lee;,citation_author=Ben Goodrich;,citation_author=Michael Betancourt;,citation_author=Marcus Brubaker;,citation_author=Jiqiang Guo;,citation_author=Peter Li;,citation_author=Allen Riddell;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_issue=1;,citation_volume=76;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=Visualization in Bayesian workflow;,citation_author=Jonah Gabry;,citation_author=Daniel Simpson;,citation_author=Aki Vehtari;,citation_author=Michael Betancourt;,citation_author=Andrew Gelman;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=2;,citation_volume=182;,citation_journal_title=Journal of the Royal Statistical Society: Series A;">
<meta name="citation_reference" content="citation_title=The logic of decision;,citation_author=Richard C. Jeffrey;,citation_publication_date=1965;,citation_cover_date=1965;,citation_year=1965;">
<meta name="citation_reference" content="citation_title=Prospect theory: An analysis of decision under risk;,citation_author=Daniel Kahneman;,citation_author=Amos Tversky;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_issue=2;,citation_volume=47;,citation_journal_title=Econometrica;">
<meta name="citation_reference" content="citation_title=Advances in prospect theory: Cumulative representation of uncertainty;,citation_author=Amos Tversky;,citation_author=Daniel Kahneman;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_issue=4;,citation_volume=5;,citation_journal_title=Journal of Risk and Uncertainty;">
<meta name="citation_reference" content="citation_title=The enterprise of knowledge: An essay on knowledge, credal probability, and chance;,citation_author=Isaac Levi;,citation_publication_date=1980;,citation_cover_date=1980;,citation_year=1980;">
<meta name="citation_reference" content="citation_title=A behavioral model of rational choice;,citation_author=Herbert A. Simon;,citation_publication_date=1955;,citation_cover_date=1955;,citation_year=1955;,citation_issue=1;,citation_volume=69;,citation_journal_title=The Quarterly Journal of Economics;">
<meta name="citation_reference" content="citation_title=Reasoning the fast and frugal way: Models of bounded rationality;,citation_author=Gerd Gigerenzer;,citation_author=Daniel G. Goldstein;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_volume=103;,citation_inbook_title=Psychological review;">
<meta name="citation_reference" content="citation_title=The methodology of positive economics;,citation_author=Milton Friedman;,citation_publication_date=1953;,citation_cover_date=1953;,citation_year=1953;,citation_journal_title=Essays in Positive Economics;,citation_publisher=University of Chicago Press;">
<meta name="citation_reference" content="citation_title=Risk, uncertainty and profit;,citation_author=Frank H. Knight;,citation_publication_date=1921;,citation_cover_date=1921;,citation_year=1921;">
<meta name="citation_reference" content="citation_title=Theory of games and economic behavior;,citation_author=John Neumann;,citation_author=Oskar Morgenstern;,citation_publication_date=1944;,citation_cover_date=1944;,citation_year=1944;">
<meta name="citation_reference" content="citation_title=The foundations of statistics;,citation_author=Leonard J. Savage;,citation_publication_date=1954;,citation_cover_date=1954;,citation_year=1954;">
<meta name="citation_reference" content="citation_title=A definition of subjective probability;,citation_author=Francis J. Anscombe;,citation_author=Robert J. Aumann;,citation_publication_date=1963;,citation_cover_date=1963;,citation_year=1963;,citation_issue=1;,citation_volume=34;,citation_journal_title=Annals of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Notes on the theory of choice;,citation_author=David M. Kreps;,citation_publication_date=1988;,citation_cover_date=1988;,citation_year=1988;">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">SEU Sensitivity</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-foundations" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Foundations</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-foundations">    
        <li>
    <a class="dropdown-item" href="../foundations/01_abstract_formulation.html">
 <span class="dropdown-text">1. Abstract Formulation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../foundations/02_concrete_implementation.html">
 <span class="dropdown-text">2. Concrete Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../foundations/03_prior_analysis.html">
 <span class="dropdown-text">3. Prior Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../foundations/04_parameter_recovery.html">
 <span class="dropdown-text">4. Parameter Recovery</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../foundations/05_adding_risky_choices.html">
 <span class="dropdown-text">5. Adding Risky Choices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../foundations/06_sbc_validation.html">
 <span class="dropdown-text">6. SBC Validation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-applications" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Applications</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-applications">    
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">Prompt Framing Study</li>
        <li>
    <a class="dropdown-item" href="../applications/prompt_framing_study/01_background.html">
 <span class="dropdown-text">1. Background</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../applications/prompt_framing_study/02_pilot_study_1.html">
 <span class="dropdown-text">2. Pilot Study 1</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jeffhelzner/seu-sensitivity"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">0.1</span> Introduction</a></li>
  <li><a href="#general-softmax-choice-model" id="toc-general-softmax-choice-model" class="nav-link" data-scroll-target="#general-softmax-choice-model"><span class="header-section-number">0.2</span> General Softmax Choice Model</a>
  <ul class="collapse">
  <li><a href="#notation-and-definitions" id="toc-notation-and-definitions" class="nav-link" data-scroll-target="#notation-and-definitions"><span class="header-section-number">0.2.1</span> Notation and Definitions</a></li>
  <li><a href="#the-softmax-choice-rule" id="toc-the-softmax-choice-rule" class="nav-link" data-scroll-target="#the-softmax-choice-rule"><span class="header-section-number">0.2.2</span> The Softmax Choice Rule</a></li>
  </ul></li>
  <li><a href="#fundamental-properties-of-softmax-choice" id="toc-fundamental-properties-of-softmax-choice" class="nav-link" data-scroll-target="#fundamental-properties-of-softmax-choice"><span class="header-section-number">0.3</span> Fundamental Properties of Softmax Choice</a>
  <ul class="collapse">
  <li><a href="#sec-monotonicity" id="toc-sec-monotonicity" class="nav-link" data-scroll-target="#sec-monotonicity"><span class="header-section-number">0.3.1</span> Property 1: Monotonicity in Sensitivity</a></li>
  <li><a href="#sec-rationality" id="toc-sec-rationality" class="nav-link" data-scroll-target="#sec-rationality"><span class="header-section-number">0.3.2</span> Property 2: Perfect Optimization Limit</a></li>
  <li><a href="#sec-random" id="toc-sec-random" class="nav-link" data-scroll-target="#sec-random"><span class="header-section-number">0.3.3</span> Property 3: Random Choice Limit</a></li>
  <li><a href="#summary-the-three-properties" id="toc-summary-the-three-properties" class="nav-link" data-scroll-target="#summary-the-three-properties"><span class="header-section-number">0.3.4</span> Summary: The Three Properties</a></li>
  </ul></li>
  <li><a href="#application-to-subjective-expected-utility" id="toc-application-to-subjective-expected-utility" class="nav-link" data-scroll-target="#application-to-subjective-expected-utility"><span class="header-section-number">0.4</span> Application to Subjective Expected Utility</a>
  <ul class="collapse">
  <li><a href="#seu-as-a-value-function" id="toc-seu-as-a-value-function" class="nav-link" data-scroll-target="#seu-as-a-value-function"><span class="header-section-number">0.4.1</span> SEU as a Value Function</a></li>
  <li><a href="#corollaries-for-seu" id="toc-corollaries-for-seu" class="nav-link" data-scroll-target="#corollaries-for-seu"><span class="header-section-number">0.4.2</span> Corollaries for SEU</a></li>
  <li><a href="#what-seu-adds-to-the-framework" id="toc-what-seu-adds-to-the-framework" class="nav-link" data-scroll-target="#what-seu-adds-to-the-framework"><span class="header-section-number">0.4.3</span> What SEU Adds to the Framework</a></li>
  </ul></li>
  <li><a href="#scale-invariance-and-representation" id="toc-scale-invariance-and-representation" class="nav-link" data-scroll-target="#scale-invariance-and-representation"><span class="header-section-number">0.5</span> Scale Invariance and Representation</a>
  <ul class="collapse">
  <li><a href="#the-representation-problem" id="toc-the-representation-problem" class="nav-link" data-scroll-target="#the-representation-problem"><span class="header-section-number">0.5.1</span> The Representation Problem</a></li>
  <li><a href="#resolution-utility-standardization" id="toc-resolution-utility-standardization" class="nav-link" data-scroll-target="#resolution-utility-standardization"><span class="header-section-number">0.5.2</span> Resolution: Utility Standardization</a></li>
  <li><a href="#interpretation-of-α-under-standardization" id="toc-interpretation-of-α-under-standardization" class="nav-link" data-scroll-target="#interpretation-of-α-under-standardization"><span class="header-section-number">0.5.3</span> Interpretation of α Under Standardization</a></li>
  </ul></li>
  <li><a href="#rates-of-convergence" id="toc-rates-of-convergence" class="nav-link" data-scroll-target="#rates-of-convergence"><span class="header-section-number">0.6</span> Rates of Convergence</a>
  <ul class="collapse">
  <li><a href="#convergence-rate-for-property-2-alpha-to-infty" id="toc-convergence-rate-for-property-2-alpha-to-infty" class="nav-link" data-scroll-target="#convergence-rate-for-property-2-alpha-to-infty"><span class="header-section-number">0.6.1</span> Convergence Rate for Property 2 (<span class="math inline">\(\alpha \to \infty\)</span>)</a></li>
  <li><a href="#convergence-rate-for-property-3-alpha-to-0" id="toc-convergence-rate-for-property-3-alpha-to-0" class="nav-link" data-scroll-target="#convergence-rate-for-property-3-alpha-to-0"><span class="header-section-number">0.6.2</span> Convergence Rate for Property 3 (<span class="math inline">\(\alpha \to 0\)</span>)</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">0.7</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#what-the-model-describes" id="toc-what-the-model-describes" class="nav-link" data-scroll-target="#what-the-model-describes"><span class="header-section-number">0.7.1</span> What the Model Describes</a></li>
  <li><a href="#what-the-model-is-not" id="toc-what-the-model-is-not" class="nav-link" data-scroll-target="#what-the-model-is-not"><span class="header-section-number">0.7.2</span> What the Model Is Not</a></li>
  <li><a href="#a-conceptual-lens-commitment-and-performance" id="toc-a-conceptual-lens-commitment-and-performance" class="nav-link" data-scroll-target="#a-conceptual-lens-commitment-and-performance"><span class="header-section-number">0.7.3</span> A Conceptual Lens: Commitment and Performance</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">0.8</span> Summary</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">0.9</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Abstract Formulation of the SEU Sensitivity Model</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">Foundational Report 1</p>
  <div class="quarto-categories">
    <div class="quarto-category">foundations</div>
    <div class="quarto-category">theory</div>
    <div class="quarto-category">m_0</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>A formal specification of the softmax choice model with sensitivity parameter α, including complete proofs of the three fundamental properties that characterize sensitivity to value maximization.</p>
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://github.com/jeffhelzner">Jeff Helzner</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 15, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="0.1">
<h2 data-number="0.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">0.1</span> Introduction</h2>
<p>This report establishes the theoretical foundations for the intended interpretation of the models that are discussed in subsequent reports. We derive three properties of the softmax choice model with respect to an arbitrary value function, then show how these properties apply specifically when values are subjective expected utilities (SEU). While mathematically straightforward, these properties are essential to our intended interpretation: when value is taken to be subjective expected utility, <span class="math inline">\(\alpha\)</span> can be interpreted as measuring a decision maker’s alignment with the normative standard of SEU rationality—how consistently their choices track the SEU ranking.</p>
<p>This separation clarifies an important conceptual point: the core choice-theoretic results are independent of how values are constructed—they follow from the structure of softmax choice alone. The SEU interpretation provides the substantive behavioral content and connects our model to classical decision theory <span class="citation" data-cites="savage1954 vonneumann1947">(<a href="#ref-savage1954" role="doc-biblioref">Savage 1954</a>; <a href="#ref-vonneumann1947" role="doc-biblioref">Neumann and Morgenstern 1947</a>)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Why This Matters
</div>
</div>
<div class="callout-body-container callout-body">
<p>Understanding these properties is essential for interpreting the sensitivity parameter α:</p>
<ul>
<li><strong>For practitioners</strong>: α has a precise meaning as log-odds change per unit of standardized utility difference</li>
<li><strong>For theorists</strong>: The properties follow from any choice model of this functional form—a softmax transformation of utilities scaled by α—independent of the particular theory of value adopted</li>
</ul>
</div>
</div>
</section>
<section id="general-softmax-choice-model" class="level2" data-number="0.2">
<h2 data-number="0.2" class="anchored" data-anchor-id="general-softmax-choice-model"><span class="header-section-number">0.2</span> General Softmax Choice Model</h2>
<section id="notation-and-definitions" class="level3" data-number="0.2.1">
<h3 data-number="0.2.1" class="anchored" data-anchor-id="notation-and-definitions"><span class="header-section-number">0.2.1</span> Notation and Definitions</h3>
<p>We begin with abstract notation for the general softmax choice model, then show how it specializes to SEU. The notation is chosen to align with our Stan implementations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Notation Summary (Abstract Model)
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathcal{R} = \{1, 2, \ldots, R\}\)</span></td>
<td>Set of distinct alternatives</td>
</tr>
<tr class="even">
<td><span class="math inline">\(N_m\)</span></td>
<td>Number of alternatives available in problem <span class="math inline">\(m\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(V: \mathcal{R} \to \mathbb{R}\)</span></td>
<td>Value function assigning utilities to alternatives</td>
</tr>
<tr class="even">
<td><span class="math inline">\(V(r)\)</span></td>
<td>Value of alternative <span class="math inline">\(r\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha \in \mathbb{R}_+\)</span></td>
<td>Sensitivity parameter (non-negative)</td>
</tr>
</tbody>
</table>
<p><strong>Notation used in proofs:</strong> We write <span class="math inline">\(\mathcal{R}^* = \{r : V(r) = V^*\}\)</span> for the set of value-maximizing alternatives, where <span class="math inline">\(V^* = \max_r V(r)\)</span>, and <span class="math inline">\(\mathcal{R}^- = \mathcal{R} \setminus \mathcal{R}^*\)</span> for suboptimal alternatives.</p>
</div>
</div>
</section>
<section id="the-softmax-choice-rule" class="level3" data-number="0.2.2">
<h3 data-number="0.2.2" class="anchored" data-anchor-id="the-softmax-choice-rule"><span class="header-section-number">0.2.2</span> The Softmax Choice Rule</h3>
<p>The probability that a decision maker selects alternative <span class="math inline">\(r\)</span> from a choice set is given by the <strong>softmax</strong> rule:</p>
<p><span id="eq-softmax"><span class="math display">\[
P(\text{choose } r \mid \alpha, V) = \frac{\exp(\alpha \cdot V(r))}{\sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))}
\tag{1}\]</span></span></p>
<p>This functional form has historical precedents in several research traditions. Luce <span class="citation" data-cites="luce1959">(<a href="#ref-luce1959" role="doc-biblioref">Luce 1959</a>)</span> derived a ratio-scale choice rule from axiomatic foundations, working with abstract “scale values” rather than utilities per se. McFadden <span class="citation" data-cites="mcfadden1974">(<a href="#ref-mcfadden1974" role="doc-biblioref">McFadden 1974</a>)</span> arrived at the same functional form from random utility theory in econometrics. The rule also appears in statistical mechanics as the Boltzmann distribution, where the sensitivity parameter is called inverse temperature. We adopt the term “softmax” from machine learning, where this transformation is ubiquitous.</p>
<p>The sensitivity parameter <span class="math inline">\(\alpha\)</span> controls how deterministically choices track value differences.</p>
<p>In our Stan implementations, this corresponds to <code>chi[m] = softmax(alpha * eta_m)</code> where <code>eta_m</code> contains the expected utilities of alternatives available in problem <span class="math inline">\(m\)</span>.</p>
<p>This rule has several appealing properties:</p>
<ol type="1">
<li><strong>Probabilistic</strong>: Assigns positive probability to all alternatives</li>
<li><strong>Monotonic in value</strong>: Higher-value alternatives are more likely to be chosen</li>
<li><strong>Parameterized sensitivity</strong>: α controls how sharply choices concentrate on high-value alternatives</li>
</ol>
<div id="cell-fig-softmax-demo" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate softmax with varying alpha</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>])</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.array([softmax(a <span class="op">*</span> values) <span class="cf">for</span> a <span class="kw">in</span> alphas])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">'η=0.2 (low)'</span>, <span class="st">'η=0.5 (medium)'</span>, <span class="st">'η=0.8 (high)'</span>]):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    ax.plot(alphas, probs[:, i], label<span class="op">=</span>label, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Uniform (α→0)'</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sensitivity (α)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Choice Probability χ'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Softmax Choice Probabilities'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-softmax-demo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-softmax-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_abstract_formulation_files/figure-html/fig-softmax-demo-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-softmax-demo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Softmax choice probabilities for three alternatives with values η = (0.2, 0.5, 0.8) as sensitivity α varies. In the m_0 model, these values represent expected utilities computed as η = ψᵀυ.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="fundamental-properties-of-softmax-choice" class="level2" data-number="0.3">
<h2 data-number="0.3" class="anchored" data-anchor-id="fundamental-properties-of-softmax-choice"><span class="header-section-number">0.3</span> Fundamental Properties of Softmax Choice</h2>
<p>The following three properties hold for <strong>any</strong> value function <span class="math inline">\(V: \mathcal{R} \to \mathbb{R}\)</span>. This generality is important—the properties follow from any choice model of this functional form, not any particular theory of value.</p>
<section id="sec-monotonicity" class="level3" data-number="0.3.1">
<h3 data-number="0.3.1" class="anchored" data-anchor-id="sec-monotonicity"><span class="header-section-number">0.3.1</span> Property 1: Monotonicity in Sensitivity</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Theorem 1 (Monotonicity)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any value function <span class="math inline">\(V: \mathcal{R} \to \mathbb{R}\)</span>, holding <span class="math inline">\(V\)</span> fixed:</p>
<ul>
<li>For <span class="math inline">\(r \in \mathcal{R}^*\)</span> (value-maximizing): <span class="math inline">\(P(\text{choose } r \mid \alpha, V)\)</span> is <strong>strictly increasing</strong> in <span class="math inline">\(\alpha\)</span></li>
<li>For <span class="math inline">\(r \notin \mathcal{R}^*\)</span> (suboptimal): <span class="math inline">\(P(\text{choose } r \mid \alpha, V)\)</span> is <strong>strictly decreasing</strong> in <span class="math inline">\(\alpha\)</span></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proof of Theorem 1
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Part A: Value-maximizing alternatives (<span class="math inline">\(r \in \mathcal{R}^*\)</span>)</strong></p>
<p>Let <span class="math inline">\(r \in \mathcal{R}^*\)</span> such that <span class="math inline">\(V(r) = V^*\)</span>. Define the partition function: <span class="math display">\[
Z(\alpha) = \sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))
\]</span></p>
<p>Taking the derivative of <span class="math inline">\(P(r)\)</span> with respect to <span class="math inline">\(\alpha\)</span>: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = \frac{\partial}{\partial \alpha} \left[\frac{\exp(\alpha \cdot V(r))}{Z(\alpha)}\right]
\]</span></p>
<p>Using the quotient rule: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = \frac{Z(\alpha) \cdot V(r) \cdot \exp(\alpha \cdot V(r)) - \exp(\alpha \cdot V(r)) \cdot Z'(\alpha)}{Z(\alpha)^2}
\]</span></p>
<p>Simplifying: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot \left[V(r) - \frac{Z'(\alpha)}{Z(\alpha)}\right]
\]</span></p>
<p>Computing <span class="math inline">\(Z'(\alpha)\)</span>: <span class="math display">\[
Z'(\alpha) = \sum_{j \in \mathcal{R}} V(j) \cdot \exp(\alpha \cdot V(j))
\]</span></p>
<p>Therefore: <span class="math display">\[
\frac{Z'(\alpha)}{Z(\alpha)} = \sum_{j \in \mathcal{R}} V(j) \cdot P(j) = \mathbb{E}[V]
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[V]\)</span> is the expected value under the current choice distribution.</p>
<p>Thus: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot [V^* - \mathbb{E}[V]]
\]</span></p>
<p>Since <span class="math inline">\(V^* = \max_j V(j)\)</span> and <span class="math inline">\(\mathbb{E}[V]\)</span> is a weighted average: <span class="math display">\[
\mathbb{E}[V] = \sum_{j \in \mathcal{R}} P(j) \cdot V(j) \leq V^*
\]</span></p>
<p>with equality only when <span class="math inline">\(P(r) = 1\)</span> for some <span class="math inline">\(r \in \mathcal{R}^*\)</span> (which occurs only as <span class="math inline">\(\alpha \to \infty\)</span>).</p>
<p>For any finite <span class="math inline">\(\alpha\)</span>, all alternatives receive positive probability under the softmax rule (since <span class="math inline">\(\exp(x) &gt; 0\)</span> for all real <span class="math inline">\(x\)</span>), ensuring that both optimal and suboptimal alternatives contribute to the expectation. Hence <span class="math inline">\(\mathbb{E}[V] &lt; V^*\)</span> strictly, so: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot [V^* - \mathbb{E}[V]] &gt; 0 \quad \blacksquare
\]</span></p>
<p><strong>Part B: Suboptimal alternatives (<span class="math inline">\(r \notin \mathcal{R}^*\)</span>)</strong></p>
<p>For <span class="math inline">\(r \notin \mathcal{R}^*\)</span>, we have <span class="math inline">\(V(r) &lt; V^*\)</span>. Following the same derivation: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot [V(r) - \mathbb{E}[V]]
\]</span></p>
<p>Since <span class="math inline">\(r\)</span> is suboptimal and <span class="math inline">\(\mathcal{R}^*\)</span> is non-empty (so <span class="math inline">\(P(\mathcal{R}^*) &gt; 0\)</span> for all finite <span class="math inline">\(\alpha\)</span>): <span class="math display">\[
\mathbb{E}[V] \geq P(\mathcal{R}^*) \cdot V^* + P(r) \cdot V(r) &gt; P(\mathcal{R}^*) \cdot V(r) + P(r) \cdot V(r)
\]</span></p>
<p>The strict inequality follows because <span class="math inline">\(V^* &gt; V(r)\)</span>.</p>
<p>Therefore, <span class="math inline">\(V(r) - \mathbb{E}[V] &lt; 0\)</span>, and: <span class="math display">\[
\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot [V(r) - \mathbb{E}[V]] &lt; 0 \quad \blacksquare
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sec-rationality" class="level3" data-number="0.3.2">
<h3 data-number="0.3.2" class="anchored" data-anchor-id="sec-rationality"><span class="header-section-number">0.3.2</span> Property 2: Perfect Optimization Limit</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Theorem 2 (Convergence to Value Maximization)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any value function <span class="math inline">\(V: \mathcal{R} \to \mathbb{R}\)</span>, as <span class="math inline">\(\alpha \to \infty\)</span>:</p>
<p><span class="math display">\[
\lim_{\alpha \to \infty} P(\text{choose } r \mid \alpha, V) =
\begin{cases}
1/|\mathcal{R}^*| &amp; \text{if } r \in \mathcal{R}^* \\
0 &amp; \text{if } r \notin \mathcal{R}^*
\end{cases}
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proof of Theorem 2
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Case 1: <span class="math inline">\(r \in \mathcal{R}^*\)</span> (value-maximizing)</strong></p>
<p><span class="math display">\[
P(r) = \frac{\exp(\alpha \cdot V^*)}{|\mathcal{R}^*| \cdot \exp(\alpha \cdot V^*) + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot V(j))}
\]</span></p>
<p>Dividing numerator and denominator by <span class="math inline">\(\exp(\alpha \cdot V^*)\)</span>: <span class="math display">\[
P(r) = \frac{1}{|\mathcal{R}^*| + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot [V(j) - V^*])}
\]</span></p>
<p>For <span class="math inline">\(j \in \mathcal{R}^-\)</span>, we have <span class="math inline">\(V(j) &lt; V^*\)</span>, so <span class="math inline">\(V(j) - V^* &lt; 0\)</span>.</p>
<p>As <span class="math inline">\(\alpha \to \infty\)</span>: <span class="math display">\[
\exp(\alpha \cdot [V(j) - V^*]) \to 0 \quad \text{for all } j \in \mathcal{R}^-
\]</span></p>
<p>Thus: <span class="math display">\[
\lim_{\alpha \to \infty} P(r) = \frac{1}{|\mathcal{R}^*|} \quad \blacksquare
\]</span></p>
<p><strong>Case 2: <span class="math inline">\(r \notin \mathcal{R}^*\)</span> (suboptimal)</strong></p>
<p><span class="math display">\[
P(r) = \frac{\exp(\alpha \cdot V(r))}{\sum_{s \in \mathcal{R}^*} \exp(\alpha \cdot V^*) + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot V(j))}
\]</span></p>
<p>Dividing by <span class="math inline">\(\exp(\alpha \cdot V^*)\)</span>: <span class="math display">\[
P(r) = \frac{\exp(\alpha \cdot [V(r) - V^*])}{|\mathcal{R}^*| + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot [V(j) - V^*])}
\]</span></p>
<p>Since <span class="math inline">\(V(r) - V^* &lt; 0\)</span>:</p>
<ul>
<li>Numerator <span class="math inline">\(\to 0\)</span></li>
<li>Denominator <span class="math inline">\(\geq |\mathcal{R}^*| &gt; 0\)</span></li>
</ul>
<p>Therefore: <span class="math display">\[
\lim_{\alpha \to \infty} P(r) = 0 \quad \blacksquare
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sec-random" class="level3" data-number="0.3.3">
<h3 data-number="0.3.3" class="anchored" data-anchor-id="sec-random"><span class="header-section-number">0.3.3</span> Property 3: Random Choice Limit</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Theorem 3 (Convergence to Uniform Choice)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any value function <span class="math inline">\(V: \mathcal{R} \to \mathbb{R}\)</span>, as <span class="math inline">\(\alpha \to 0\)</span>:</p>
<p><span class="math display">\[
\lim_{\alpha \to 0} P(\text{choose } r \mid \alpha, V) = \frac{1}{|\mathcal{R}|} \quad \text{for all } r \in \mathcal{R}
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proof of Theorem 3
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using Taylor expansion <span class="math inline">\(\exp(x) = 1 + x + O(x^2)\)</span>:</p>
<p><span class="math display">\[
P(r) = \frac{1 + \alpha \cdot V(r) + O(\alpha^2)}{\sum_{j \in \mathcal{R}} (1 + \alpha \cdot V(j) + O(\alpha^2))}
\]</span></p>
<p><span class="math display">\[
= \frac{1 + \alpha \cdot V(r) + O(\alpha^2)}{|\mathcal{R}| + \alpha \cdot \sum_j V(j) + O(\alpha^2)}
\]</span></p>
<p>As <span class="math inline">\(\alpha \to 0\)</span>: <span class="math display">\[
\lim_{\alpha \to 0} P(r) = \frac{1}{|\mathcal{R}|} \quad \blacksquare
\]</span></p>
<p><strong>Alternative proof via logarithms:</strong></p>
<p><span class="math display">\[
\log P(r) = \alpha \cdot V(r) - \log\left[\sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))\right]
\]</span></p>
<p>Expanding the log-sum-exp: <span class="math display">\[
\log\left[\sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))\right] = \log|\mathcal{R}| + \frac{\alpha \cdot \sum_j V(j)}{|\mathcal{R}|} + O(\alpha^2)
\]</span></p>
<p>Therefore: <span class="math display">\[
\log P(r) = -\log|\mathcal{R}| + \alpha \cdot \left[V(r) - \frac{\sum_j V(j)}{|\mathcal{R}|}\right] + O(\alpha^2)
\]</span></p>
<p>As <span class="math inline">\(\alpha \to 0\)</span>: <span class="math inline">\(\log P(r) \to -\log|\mathcal{R}|\)</span>, hence <span class="math inline">\(P(r) \to 1/|\mathcal{R}|\)</span>. <span class="math inline">\(\blacksquare\)</span></p>
</div>
</div>
</div>
</section>
<section id="summary-the-three-properties" class="level3" data-number="0.3.4">
<h3 data-number="0.3.4" class="anchored" data-anchor-id="summary-the-three-properties"><span class="header-section-number">0.3.4</span> Summary: The Three Properties</h3>
<div id="cell-fig-three-properties" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Values for a 3-alternative problem</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>])  <span class="co"># Third is optimal</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Property 1: Monotonicity</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="dv">8</span>, <span class="dv">100</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.array([softmax(a <span class="op">*</span> values) <span class="cf">for</span> a <span class="kw">in</span> alphas])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(alphas, probs[:, <span class="dv">2</span>], <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Optimal (η=0.9)'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(alphas, probs[:, <span class="dv">1</span>], <span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Middle (η=0.5)'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(alphas, probs[:, <span class="dv">0</span>], <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Low (η=0.3)'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'α'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'χ (choice probability)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Property 1: Monotonicity'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Property 2: α → ∞</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>alpha_large <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>probs_large <span class="op">=</span> softmax(alpha_large <span class="op">*</span> values)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar([<span class="st">'η=0.3'</span>, <span class="st">'η=0.5'</span>, <span class="st">'η=0.9'</span>], probs_large, color<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'orange'</span>, <span class="st">'blue'</span>])</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'blue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Limit'</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'χ (choice probability)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Property 2: α → ∞</span><span class="ch">\n</span><span class="st">(Deterministic Optimal)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylim(<span class="dv">0</span>, <span class="fl">1.1</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Property 3: α → 0</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>alpha_small <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>probs_small <span class="op">=</span> softmax(alpha_small <span class="op">*</span> values)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].bar([<span class="st">'η=0.3'</span>, <span class="st">'η=0.5'</span>, <span class="st">'η=0.9'</span>], probs_small, color<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'orange'</span>, <span class="st">'blue'</span>])</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].axhline(y<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Uniform'</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'χ (choice probability)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Property 3: α → 0</span><span class="ch">\n</span><span class="st">(Uniform Random)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylim(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-three-properties" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-three-properties-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_abstract_formulation_files/figure-html/fig-three-properties-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-three-properties-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Visual summary of the three fundamental properties. Left: Monotonicity—optimal alternative probability increases with α. Middle: Limiting behavior at α→∞ (deterministic optimal choice). Right: Limiting behavior at α→0 (uniform random choice).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="application-to-subjective-expected-utility" class="level2" data-number="0.4">
<h2 data-number="0.4" class="anchored" data-anchor-id="application-to-subjective-expected-utility"><span class="header-section-number">0.4</span> Application to Subjective Expected Utility</h2>
<p>We now specialize the general softmax framework to the case where values are subjective expected utilities (SEU).</p>
<section id="seu-as-a-value-function" class="level3" data-number="0.4.1">
<h3 data-number="0.4.1" class="anchored" data-anchor-id="seu-as-a-value-function"><span class="header-section-number">0.4.1</span> SEU as a Value Function</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Notation Summary (SEU Specialization)
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 61%">
</colgroup>
<thead>
<tr class="header">
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(K\)</span></td>
<td>Number of possible consequences (outcomes)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\boldsymbol{\upsilon} \in \mathbb{R}^K\)</span></td>
<td>Utility vector over consequences</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\boldsymbol{\psi}_r \in \Delta^{K-1}\)</span></td>
<td>Subjective probability distribution over consequences for alternative <span class="math inline">\(r\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}\)</span></td>
<td>Expected utility of alternative <span class="math inline">\(r\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\boldsymbol{\chi}_m\)</span></td>
<td>Choice probability vector for problem <span class="math inline">\(m\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition: Subjective Expected Utility
</div>
</div>
<div class="callout-body-container callout-body">
<p>Each alternative <span class="math inline">\(r\)</span> is associated with:</p>
<ol type="1">
<li><strong>Subjective probabilities</strong> <span class="math inline">\(\boldsymbol{\psi}_r \in \Delta^{K-1}\)</span> over <span class="math inline">\(K\)</span> consequences</li>
<li><strong>An expected utility</strong> computed as: <span class="math display">\[
\eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon} = \sum_{k=1}^{K} \psi_{r,k} \cdot \upsilon_k
\]</span></li>
</ol>
<p>The <strong>choice probability</strong> for alternative <span class="math inline">\(r\)</span> in problem <span class="math inline">\(m\)</span> is then: <span class="math display">\[
\chi_{m,r} = \frac{\exp(\alpha \cdot \eta_r)}{\sum_{j: I_{m,j}=1} \exp(\alpha \cdot \eta_j)}
\]</span></p>
<p>where <span class="math inline">\(I_{m,r} = 1\)</span> indicates that alternative <span class="math inline">\(r\)</span> is available in problem <span class="math inline">\(m\)</span>.</p>
</div>
</div>
<p><strong>Key observation:</strong> The expected utility <span class="math inline">\(\eta_r\)</span> serves as our value function <span class="math inline">\(V(r) = \eta_r\)</span>. Therefore, all three properties proved above apply immediately.</p>
</section>
<section id="corollaries-for-seu" class="level3" data-number="0.4.2">
<h3 data-number="0.4.2" class="anchored" data-anchor-id="corollaries-for-seu"><span class="header-section-number">0.4.2</span> Corollaries for SEU</h3>
<p>By substituting <span class="math inline">\(V(r) = \eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}\)</span> into Properties 1-3:</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Corollary 1 (Monotonicity for SEU)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Holding utilities <span class="math inline">\(\boldsymbol{\upsilon}\)</span> and beliefs <span class="math inline">\(\boldsymbol{\psi}\)</span> fixed, higher sensitivity <span class="math inline">\(\alpha\)</span> increases the probability of choosing alternatives that maximize expected utility <span class="math inline">\(\eta\)</span>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Corollary 2 (Perfect Rationality)
</div>
</div>
<div class="callout-body-container callout-body">
<p>As <span class="math inline">\(\alpha \to \infty\)</span>, the decision maker chooses uniformly among SEU-maximizing alternatives (those with highest <span class="math inline">\(\eta\)</span>) with probability 1. When there is a unique maximizer, choice becomes deterministic.</p>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Corollary 3 (Random Choice)
</div>
</div>
<div class="callout-body-container callout-body">
<p>As <span class="math inline">\(\alpha \to 0\)</span>, the decision maker chooses uniformly at random over available alternatives, independent of <span class="math inline">\(\eta\)</span> values.</p>
</div>
</div>
</section>
<section id="what-seu-adds-to-the-framework" class="level3" data-number="0.4.3">
<h3 data-number="0.4.3" class="anchored" data-anchor-id="what-seu-adds-to-the-framework"><span class="header-section-number">0.4.3</span> What SEU Adds to the Framework</h3>
<p>While the mathematical properties of softmax choice hold for any value function, the SEU construction provides:</p>
<ol type="1">
<li><p><strong>Decomposition</strong>: Expected utilities <span class="math inline">\(\eta\)</span> decompose into beliefs (<span class="math inline">\(\boldsymbol{\psi}\)</span>) and utilities (<span class="math inline">\(\boldsymbol{\upsilon}\)</span>), allowing separate analysis of epistemic and preference components</p></li>
<li><p><strong>Normative content</strong>: SEU maximization is a rationality criterion—Properties 1-3 characterize adherence to this normative standard <span class="citation" data-cites="savage1954">(<a href="#ref-savage1954" role="doc-biblioref">Savage 1954</a>)</span></p></li>
<li><p><strong>Empirical predictions</strong>: The model predicts that choices will track <span class="math inline">\(\eta\)</span>, providing testable restrictions</p></li>
</ol>
</section>
</section>
<section id="scale-invariance-and-representation" class="level2" data-number="0.5">
<h2 data-number="0.5" class="anchored" data-anchor-id="scale-invariance-and-representation"><span class="header-section-number">0.5</span> Scale Invariance and Representation</h2>
<section id="the-representation-problem" class="level3" data-number="0.5.1">
<h3 data-number="0.5.1" class="anchored" data-anchor-id="the-representation-problem"><span class="header-section-number">0.5.1</span> The Representation Problem</h3>
<p>A fundamental property of utility functions is that they are unique only up to positive affine transformations. This raises a critical question: how can we meaningfully interpret <span class="math inline">\(\alpha\)</span> when the scale of utility is arbitrary?</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Theorem 4 (Scale Invariance)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\boldsymbol{\upsilon}\)</span> be a utility vector and define a rescaled utility: <span class="math display">\[
\tilde{\upsilon}_k = a \cdot \upsilon_k + b \quad \text{where } a &gt; 0
\]</span></p>
<p>Then:</p>
<ol type="1">
<li><span class="math inline">\(\tilde{\eta}_r = a \cdot \eta_r + b\)</span> for all alternatives <span class="math inline">\(r\)</span></li>
<li><span class="math inline">\(P(\text{choose } r \mid \alpha, \tilde{\boldsymbol{\upsilon}}) = P(\text{choose } r \mid \alpha \cdot a, \boldsymbol{\upsilon})\)</span></li>
</ol>
<p>The pair <span class="math inline">\((\alpha, \boldsymbol{\upsilon})\)</span> and <span class="math inline">\((\alpha \cdot a, \tilde{\boldsymbol{\upsilon}})\)</span> generate <strong>identical</strong> choice probabilities.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proof of Theorem 4
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Part 1:</strong> <span class="math display">\[
\tilde{\eta}_r = \sum_k \psi_{r,k} \cdot [a \cdot \upsilon_k + b]
= a \cdot \sum_k \psi_{r,k} \cdot \upsilon_k + b \cdot \sum_k \psi_{r,k}
= a \cdot \eta_r + b
\]</span></p>
<p>since <span class="math inline">\(\sum_k \psi_{r,k} = 1\)</span>.</p>
<p><strong>Part 2:</strong> <span class="math display">\[
P(\text{choose } r \mid \alpha, \tilde{\boldsymbol{\upsilon}}) = \frac{\exp(\alpha \cdot [a \cdot \eta_r + b])}{\sum_j \exp(\alpha \cdot [a \cdot \eta_j + b])}
\]</span></p>
<p><span class="math display">\[
= \frac{\exp(\alpha a \cdot \eta_r) \cdot \exp(\alpha b)}{\sum_j \exp(\alpha a \cdot \eta_j) \cdot \exp(\alpha b)}
= \frac{\exp(\alpha a \cdot \eta_r)}{\sum_j \exp(\alpha a \cdot \eta_j)}
= P(\text{choose } r \mid \alpha a, \boldsymbol{\upsilon}) \quad \blacksquare
\]</span></p>
</div>
</div>
</div>
<p><strong>Key Implication:</strong> Without fixing the utility scale, <span class="math inline">\(\alpha\)</span> and the scale of utility are confounded—they cannot be separately interpreted from choice behavior alone. Scaling utilities by a factor <span class="math inline">\(a\)</span> is equivalent to scaling sensitivity by <span class="math inline">\(1/a\)</span>.</p>
</section>
<section id="resolution-utility-standardization" class="level3" data-number="0.5.2">
<h3 data-number="0.5.2" class="anchored" data-anchor-id="resolution-utility-standardization"><span class="header-section-number">0.5.2</span> Resolution: Utility Standardization</h3>
<p>To make <span class="math inline">\(\alpha\)</span> interpretable as “sensitivity to expected utility differences,” we adopt a standard standardization convention.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Standardization Convention
</div>
</div>
<div class="callout-body-container callout-body">
<p>We constrain utilities to lie in <span class="math inline">\([0,1]\)</span> by assigning:</p>
<ul>
<li><span class="math inline">\(\upsilon_{\text{worst}} = 0\)</span> (utility of the worst consequence)</li>
<li><span class="math inline">\(\upsilon_{\text{best}} = 1\)</span> (utility of the best consequence)</li>
</ul>
<p>For <span class="math inline">\(K\)</span> ordered consequences, this means: <span class="math display">\[
0 = \upsilon_1 \leq \upsilon_2 \leq \cdots \leq \upsilon_K = 1
\]</span></p>
<p>This is the standard standardization in decision theory, where the utility function is anchored at the endpoints of the consequence space.</p>
</div>
</div>
<p>This standardization is without loss of generality—it simply fixes a representation from the equivalence class of utility functions related by positive affine transformations. Any utility function can be rescaled to satisfy this convention.</p>
<p><strong>Interpretive Result:</strong> Given this standardization, <span class="math inline">\(\alpha\)</span> measures sensitivity to expected utility differences on a standardized scale where the full range of possible utilities spans exactly one unit.</p>
</section>
<section id="interpretation-of-α-under-standardization" class="level3" data-number="0.5.3">
<h3 data-number="0.5.3" class="anchored" data-anchor-id="interpretation-of-α-under-standardization"><span class="header-section-number">0.5.3</span> Interpretation of α Under Standardization</h3>
<p>With utilities standardized to <span class="math inline">\([0,1]\)</span>, expected utilities satisfy <span class="math inline">\(\eta_r \in [0,1]\)</span> for all alternatives <span class="math inline">\(r\)</span> (since <span class="math inline">\(\eta_r\)</span> is a convex combination of utilities). The maximum possible difference in expected utility is therefore 1.</p>
<p>The sensitivity parameter <span class="math inline">\(\alpha\)</span> has a precise interpretation via the log-odds ratio:</p>
<p><span class="math display">\[
\log\left[\frac{\chi_{r}}{\chi_{s}}\right] = \alpha \cdot [\eta_r - \eta_s]
\]</span></p>
<div class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>alpha_vals <span class="op">=</span> [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> alpha_vals:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    odds_ratio <span class="op">=</span> np.exp(a)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    prob_better <span class="op">=</span> odds_ratio <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> odds_ratio)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    data.append({</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'α'</span>: a,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Log-odds'</span>: <span class="ss">f'</span><span class="sc">{</span>a<span class="sc">:.1f}</span><span class="ss">'</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Odds ratio'</span>: <span class="ss">f'</span><span class="sc">{</span>odds_ratio<span class="sc">:.2f}</span><span class="ss">'</span>,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'P(higher η)'</span>: <span class="ss">f'</span><span class="sc">{</span>prob_better<span class="sc">:.1%}</span><span class="ss">'</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div id="tbl-alpha-interpretation" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="4">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-alpha-interpretation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Interpretation of α for a one-unit expected utility difference (maximum possible difference with standardized utilities).
</figcaption>
<div aria-describedby="tbl-alpha-interpretation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">α</th>
<th data-quarto-table-cell-role="th">Log-odds</th>
<th data-quarto-table-cell-role="th">Odds ratio</th>
<th data-quarto-table-cell-role="th">P(higher η)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>0.5</td>
<td>0.5</td>
<td>1.65</td>
<td>62.2%</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>1.0</td>
<td>1.0</td>
<td>2.72</td>
<td>73.1%</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>2.0</td>
<td>2.0</td>
<td>7.39</td>
<td>88.1%</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>3.0</td>
<td>3.0</td>
<td>20.09</td>
<td>95.3%</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>5.0</td>
<td>5.0</td>
<td>148.41</td>
<td>99.3%</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">5</th>
<td>10.0</td>
<td>10.0</td>
<td>22026.47</td>
<td>100.0%</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p><strong>General interpretation:</strong> <span class="math inline">\(\alpha\)</span> measures the log-odds change per unit of expected utility difference. Higher <span class="math inline">\(\alpha\)</span> means choices become more deterministically aligned with <span class="math inline">\(\eta\)</span> rankings.</p>
</section>
</section>
<section id="rates-of-convergence" class="level2" data-number="0.6">
<h2 data-number="0.6" class="anchored" data-anchor-id="rates-of-convergence"><span class="header-section-number">0.6</span> Rates of Convergence</h2>
<p>The limiting behavior established in Properties 2 and 3 occurs at different rates, which we now characterize precisely.</p>
<section id="convergence-rate-for-property-2-alpha-to-infty" class="level3" data-number="0.6.1">
<h3 data-number="0.6.1" class="anchored" data-anchor-id="convergence-rate-for-property-2-alpha-to-infty"><span class="header-section-number">0.6.1</span> Convergence Rate for Property 2 (<span class="math inline">\(\alpha \to \infty\)</span>)</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Theorem 5 (Exponential Convergence to Optimality)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\Delta = \min\{V^* - V(r) : r \notin \mathcal{R}^*\}\)</span> be the minimum gap between optimal and suboptimal values. For any suboptimal alternative <span class="math inline">\(r \notin \mathcal{R}^*\)</span>:</p>
<p><span class="math display">\[
P(\text{choose } r \mid \alpha, V) = O\left(e^{-\alpha \Delta}\right) \quad \text{as } \alpha \to \infty
\]</span></p>
<p>Convergence to the optimality limit is exponential with rate <span class="math inline">\(\Delta\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proof of Theorem 5
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For <span class="math inline">\(r \notin \mathcal{R}^*\)</span>, recall from the proof of Property 2: <span class="math display">\[
P(\text{choose } r) = \frac{\exp(\alpha \cdot [V(r) - V^*])}{|\mathcal{R}^*| + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot [V(j) - V^*])}
\]</span></p>
<p>Since <span class="math inline">\(V(r) - V^* \leq -\Delta &lt; 0\)</span>: <span class="math display">\[
P(\text{choose } r) \leq \frac{\exp(-\alpha \Delta)}{|\mathcal{R}^*|} = \frac{1}{|\mathcal{R}^*|} e^{-\alpha \Delta}
\]</span></p>
<p>The denominator is bounded below by <span class="math inline">\(|\mathcal{R}^*| \geq 1\)</span>, giving: <span class="math display">\[
P(\text{choose } r) = O(e^{-\alpha \Delta}) \quad \blacksquare
\]</span></p>
</div>
</div>
</div>
<p><strong>Interpretation:</strong> Larger value gaps <span class="math inline">\(\Delta\)</span> lead to faster concentration on optimal alternatives. When the best alternative is clearly superior (large <span class="math inline">\(\Delta\)</span>), even moderate <span class="math inline">\(\alpha\)</span> yields near-deterministic choice.</p>
</section>
<section id="convergence-rate-for-property-3-alpha-to-0" class="level3" data-number="0.6.2">
<h3 data-number="0.6.2" class="anchored" data-anchor-id="convergence-rate-for-property-3-alpha-to-0"><span class="header-section-number">0.6.2</span> Convergence Rate for Property 3 (<span class="math inline">\(\alpha \to 0\)</span>)</h3>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Theorem 6 (Linear Convergence to Uniformity)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any alternative <span class="math inline">\(r \in \mathcal{R}\)</span>, let <span class="math inline">\(\bar{V} = \frac{1}{|\mathcal{R}|}\sum_j V(j)\)</span> denote the arithmetic mean of values. Then:</p>
<p><span class="math display">\[
P(\text{choose } r \mid \alpha, V) = \frac{1}{|\mathcal{R}|} + \alpha \cdot \left[V(r) - \bar{V}\right] \cdot \frac{1}{|\mathcal{R}|} + O(\alpha^2)
\]</span></p>
<p>Convergence to uniformity is first-order (linear) in <span class="math inline">\(\alpha\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proof of Theorem 6
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Expanding <span class="math inline">\(\exp(\alpha V(r)) = 1 + \alpha V(r) + \frac{\alpha^2 V(r)^2}{2} + O(\alpha^3)\)</span>:</p>
<p><span class="math display">\[
P(\text{choose } r) = \frac{1 + \alpha V(r) + O(\alpha^2)}{\sum_j [1 + \alpha V(j) + O(\alpha^2)]}
= \frac{1 + \alpha V(r) + O(\alpha^2)}{|\mathcal{R}| + \alpha \sum_j V(j) + O(\alpha^2)}
\]</span></p>
<p>Let <span class="math inline">\(S = \sum_j V(j) = |\mathcal{R}| \cdot \bar{V}\)</span>. Using the expansion <span class="math inline">\((1+x)^{-1} = 1 - x + O(x^2)\)</span>:</p>
<p><span class="math display">\[
P(\text{choose } r) = \frac{1 + \alpha V(r)}{|\mathcal{R}|} \cdot \left(1 + \frac{\alpha S}{|\mathcal{R}|}\right)^{-1} + O(\alpha^2)
\]</span></p>
<p><span class="math display">\[
= \frac{1 + \alpha V(r)}{|\mathcal{R}|} \cdot \left(1 - \frac{\alpha S}{|\mathcal{R}|}\right) + O(\alpha^2)
\]</span></p>
<p><span class="math display">\[
= \frac{1}{|\mathcal{R}|} + \frac{\alpha V(r)}{|\mathcal{R}|} - \frac{\alpha S}{|\mathcal{R}|^2} + O(\alpha^2)
\]</span></p>
<p><span class="math display">\[
= \frac{1}{|\mathcal{R}|} + \frac{\alpha}{|\mathcal{R}|} \left[V(r) - \frac{S}{|\mathcal{R}|}\right] + O(\alpha^2)
\]</span></p>
<p><span class="math display">\[
= \frac{1}{|\mathcal{R}|} + \frac{\alpha}{|\mathcal{R}|} \left[V(r) - \bar{V}\right] + O(\alpha^2) \quad \blacksquare
\]</span></p>
</div>
</div>
</div>
<p><strong>Interpretation:</strong> Near <span class="math inline">\(\alpha = 0\)</span>, deviations from uniform choice are proportional to <span class="math inline">\(\alpha\)</span> and to how much an alternative’s value exceeds the mean. The coefficient <span class="math inline">\([V(r) - \bar{V}]/|\mathcal{R}|\)</span> determines the direction and magnitude of the first-order effect.</p>
</section>
</section>
<section id="discussion" class="level2" data-number="0.7">
<h2 data-number="0.7" class="anchored" data-anchor-id="discussion"><span class="header-section-number">0.7</span> Discussion</h2>
<p>Having established the mathematical properties of the model, we turn to its interpretation.</p>
<section id="what-the-model-describes" class="level3" data-number="0.7.1">
<h3 data-number="0.7.1" class="anchored" data-anchor-id="what-the-model-describes"><span class="header-section-number">0.7.1</span> What the Model Describes</h3>
<p>Our model describes decision makers who are <strong>committed to SEU maximization but have limited sensitivity to its implications</strong>. The sensitivity parameter <span class="math inline">\(\alpha\)</span> measures how reliably their choices track the SEU ranking:</p>
<ul>
<li>High <span class="math inline">\(\alpha\)</span>: Choices reliably favor higher-<span class="math inline">\(\eta\)</span> alternatives</li>
<li>Low <span class="math inline">\(\alpha\)</span>: Choices are noisy relative to the SEU ranking</li>
<li><span class="math inline">\(\alpha \to \infty\)</span>: Near-deterministic choice of SEU-maximizing alternatives</li>
<li><span class="math inline">\(\alpha \to 0\)</span>: Choices become independent of SEU values</li>
</ul>
<p>This is not a model of rational agents in the classical sense. SEU maximization—the normative standard—corresponds only to the limit <span class="math inline">\(\alpha \to \infty\)</span>. For any finite <span class="math inline">\(\alpha\)</span>, the model permits systematic departures from SEU maximization: lower-<span class="math inline">\(\eta\)</span> alternatives are chosen with positive probability.</p>
</section>
<section id="what-the-model-is-not" class="level3" data-number="0.7.2">
<h3 data-number="0.7.2" class="anchored" data-anchor-id="what-the-model-is-not"><span class="header-section-number">0.7.2</span> What the Model Is Not</h3>
<p><strong>Not a cognitive process model.</strong> We make no claim about how decision makers actually deliberate. The model is silent on whether agents compute probabilities, form expectations, or engage in any particular mental procedure. It specifies a distribution over choices, not a mechanism that generates them. Our concern is to investigate the extent to which a decision maker’s behavior can be captured by viewing that behavior <em>as-if</em> it came from a decision maker who is committed to subjective expected utility maximization but has limited sensitivity to its implications.</p>
<p><strong>Not bounded rationality.</strong> Bounded rationality programs, in their various formulations <span class="citation" data-cites="simon1955 gigerenzer1996">(<a href="#ref-simon1955" role="doc-biblioref">Simon 1955</a>; <a href="#ref-gigerenzer1996" role="doc-biblioref">Gigerenzer and Goldstein 1996</a>)</span>, typically propose alternative decision procedures—heuristics, satisficing rules, or fast-and-frugal strategies—that agents use in place of optimization. Our model posits no such alternative procedures. Nor do we invoke notions of ecological rationality or fit between heuristics and environmental structure. The model simply describes a stochastic relationship between SEU values and choice probabilities.</p>
</section>
<section id="a-conceptual-lens-commitment-and-performance" class="level3" data-number="0.7.3">
<h3 data-number="0.7.3" class="anchored" data-anchor-id="a-conceptual-lens-commitment-and-performance"><span class="header-section-number">0.7.3</span> A Conceptual Lens: Commitment and Performance</h3>
<p>A useful conceptual lens comes from Isaac Levi’s distinction between <strong>commitment</strong> and <strong>performance</strong> <span class="citation" data-cites="levi1980">(<a href="#ref-levi1980" role="doc-biblioref">Levi 1980</a>)</span>. We may be committed to standards we fail to perform up to. Most of us are committed to the laws of arithmetic despite occasionally making calculation errors; the errors are failures of performance, not rejections of the standard.</p>
<p>If we take SEU theory as specifying the decision maker’s normative commitments, then <span class="math inline">\(\alpha\)</span> measures their <strong>tendency to perform in accordance with those commitments</strong>. This framing preserves SEU as normatively fundamental while allowing systematic departures in observed behavior.</p>
<p>Our framework differs from Levi’s in important ways:</p>
<ul>
<li><p><strong>Decision maker’s vs.&nbsp;observer’s perspective.</strong> Levi’s framework is primarily concerned with the decision maker’s own deliberative standards—what an agent should believe and prefer from the first-person standpoint. Our framework, by contrast, adopts the observer’s perspective: we model choice behavior as it appears to an external analyst who observes decisions and infers parameters.</p></li>
<li><p><strong>Probabilistic vs.&nbsp;algebraic theories.</strong> Following Luce’s <span class="citation" data-cites="luce1959">(<a href="#ref-luce1959" role="doc-biblioref">Luce 1959</a>)</span> distinction, our framework is a <em>probabilistic</em> theory of choice—it specifies a probability distribution over choices given the decision problem. Levi’s framework is more aligned with <em>algebraic</em> theories that characterize rational choice through axioms on preference orderings rather than stochastic choice rules.</p></li>
<li><p><strong>SEU as the normative standard.</strong> Levi famously rejected subjective expected utility theory as the standard of rational choice, instead advocating for generalizations that accommodate indeterminate probabilities and utilities <span class="citation" data-cites="levi1980">(<a href="#ref-levi1980" role="doc-biblioref">Levi 1980</a>)</span>. Our framework, by contrast, takes SEU maximization as the normative standard and models departures from it through the sensitivity parameter <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
</section>
</section>
<section id="summary" class="level2" data-number="0.8">
<h2 data-number="0.8" class="anchored" data-anchor-id="summary"><span class="header-section-number">0.8</span> Summary</h2>
<p>We have established three fundamental properties of the softmax choice model:</p>
<ol type="1">
<li><strong>Monotonicity</strong>: Higher <span class="math inline">\(\alpha\)</span> increases probability of choosing alternatives with higher value <span class="math inline">\(V(r)\)</span></li>
<li><strong>Perfect optimization limit</strong>: As <span class="math inline">\(\alpha \to \infty\)</span>, choices become deterministically concentrated on value-maximizing alternatives</li>
<li><strong>Random choice limit</strong>: As <span class="math inline">\(\alpha \to 0\)</span>, choices become uniformly random</li>
</ol>
<p>Additionally, we characterized the rates at which these limits are approached:</p>
<ul>
<li>Convergence to optimality is <strong>exponential</strong> with rate determined by the value gap <span class="math inline">\(\Delta\)</span></li>
<li>Convergence to uniformity is <strong>linear</strong> (first-order) in <span class="math inline">\(\alpha\)</span></li>
</ul>
<p>These properties hold for any value function <span class="math inline">\(V\)</span>. When <span class="math inline">\(V\)</span> is taken to be subjective expected utility, the framework provides a model of decision-making that interpolates between random choice and SEU maximization, with <span class="math inline">\(\alpha\)</span> governing the degree of sensitivity to expected utility differences.</p>
<p>The standardization of utilities to <span class="math inline">\([0,1]\)</span> fixes a representation from the equivalence class of utility functions, making <span class="math inline">\(\alpha\)</span> interpretable as sensitivity to standardized expected utility differences.</p>
</section>
<section id="references" class="level2" data-number="0.9">
<h2 data-number="0.9" class="anchored" data-anchor-id="references"><span class="header-section-number">0.9</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-gigerenzer1996" class="csl-entry" role="listitem">
Gigerenzer, Gerd, and Daniel G. Goldstein. 1996. <span>“Reasoning the Fast and Frugal Way: Models of Bounded Rationality.”</span> In <em>Psychological Review</em>, 103:650–69. 4.
</div>
<div id="ref-levi1980" class="csl-entry" role="listitem">
Levi, Isaac. 1980. <em>The Enterprise of Knowledge: An Essay on Knowledge, Credal Probability, and Chance</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-luce1959" class="csl-entry" role="listitem">
Luce, R. Duncan. 1959. <span>“Individual Choice Behavior: A Theoretical Analysis.”</span>
</div>
<div id="ref-mcfadden1974" class="csl-entry" role="listitem">
McFadden, Daniel. 1974. <span>“Conditional Logit Analysis of Qualitative Choice Behavior.”</span> <em>Frontiers in Econometrics</em>, 105–42.
</div>
<div id="ref-vonneumann1947" class="csl-entry" role="listitem">
Neumann, John von, and Oskar Morgenstern. 1947. <em>Theory of Games and Economic Behavior</em>. 2nd ed. Princeton, NJ: Princeton University Press.
</div>
<div id="ref-savage1954" class="csl-entry" role="listitem">
Savage, Leonard J. 1954. <span>“The Foundations of Statistics.”</span>
</div>
<div id="ref-simon1955" class="csl-entry" role="listitem">
Simon, Herbert A. 1955. <span>“A Behavioral Model of Rational Choice.”</span> <em>The Quarterly Journal of Economics</em> 69 (1): 99–118.
</div>
</div>


<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{helzner2026,
  author = {Helzner, Jeff},
  title = {Abstract {Formulation} of the {SEU} {Sensitivity} {Model}},
  date = {2026-02-15},
  url = {https://jeffhelzner.github.io/seu-sensitivity/foundations/01_abstract_formulation.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-helzner2026" class="csl-entry quarto-appendix-citeas" role="listitem">
Helzner, Jeff. 2026. <span>“Abstract Formulation of the SEU Sensitivity
Model.”</span> SEU Sensitivity Project. February 15, 2026. <a href="https://jeffhelzner.github.io/seu-sensitivity/foundations/01_abstract_formulation.html">https://jeffhelzner.github.io/seu-sensitivity/foundations/01_abstract_formulation.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/jeffhelzner\.github\.io\/seu-sensitivity\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Abstract Formulation of the SEU Sensitivity Model"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Foundational Report 1"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">  A formal specification of the softmax choice model with sensitivity parameter α,</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">  including complete proofs of the three fundamental properties that characterize</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">  sensitivity to value maximization.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [foundations, theory, m_0]</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: setup</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>sys.path.insert(<span class="dv">0</span>, os.path.join(os.getcwd(), <span class="st">'..'</span>))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> softmax</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>This report establishes the theoretical foundations for the intended interpretation of the models that are discussed in subsequent reports. We derive three properties of the softmax choice model with respect to an arbitrary value function, then show how these properties apply specifically when values are subjective expected utilities (SEU). While mathematically straightforward, these properties are essential to our intended interpretation: when value is taken to be subjective expected utility, $\alpha$ can be interpreted as measuring a decision maker's alignment with the normative standard of SEU rationality—how consistently their choices track the SEU ranking.</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>This separation clarifies an important conceptual point: the core choice-theoretic results are independent of how values are constructed—they follow from the structure of softmax choice alone. The SEU interpretation provides the substantive behavioral content and connects our model to classical decision theory <span class="co">[</span><span class="ot">@savage1954; @vonneumann1947</span><span class="co">]</span>.</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why This Matters</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>Understanding these properties is essential for interpreting the sensitivity parameter α:</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For practitioners**: α has a precise meaning as log-odds change per unit of standardized utility difference</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For theorists**: The properties follow from any choice model of this functional form—a softmax transformation of utilities scaled by α—independent of the particular theory of value adopted</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="fu">## General Softmax Choice Model</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="fu">### Notation and Definitions</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>We begin with abstract notation for the general softmax choice model, then show how it specializes to SEU. The notation is chosen to align with our Stan implementations.</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="fu">## Notation Summary (Abstract Model)</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Symbol <span class="pp">|</span> Description <span class="pp">|</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-------------|</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\mathcal{R} = <span class="sc">\{</span>1, 2, \ldots, R<span class="sc">\}</span>$ <span class="pp">|</span> Set of distinct alternatives <span class="pp">|</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $N_m$ <span class="pp">|</span> Number of alternatives available in problem $m$ <span class="pp">|</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $V: \mathcal{R} \to \mathbb{R}$ <span class="pp">|</span> Value function assigning utilities to alternatives <span class="pp">|</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $V(r)$ <span class="pp">|</span> Value of alternative $r$ <span class="pp">|</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\alpha \in \mathbb{R}_+$ <span class="pp">|</span> Sensitivity parameter (non-negative) <span class="pp">|</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>**Notation used in proofs:** We write $\mathcal{R}^* = <span class="sc">\{</span>r : V(r) = V^*\}$ for the set of value-maximizing alternatives, where $V^* = \max_r V(r)$, and $\mathcal{R}^- = \mathcal{R} \setminus \mathcal{R}^*$ for suboptimal alternatives.</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Softmax Choice Rule</span></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>The probability that a decision maker selects alternative $r$ from a choice set is given by the **softmax** rule:</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r \mid \alpha, V) = \frac{\exp(\alpha \cdot V(r))}{\sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))}</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>$$ {#eq-softmax}</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>This functional form has historical precedents in several research traditions. Luce <span class="co">[</span><span class="ot">@luce1959</span><span class="co">]</span> derived a ratio-scale choice rule from axiomatic foundations, working with abstract "scale values" rather than utilities per se. McFadden <span class="co">[</span><span class="ot">@mcfadden1974</span><span class="co">]</span> arrived at the same functional form from random utility theory in econometrics. The rule also appears in statistical mechanics as the Boltzmann distribution, where the sensitivity parameter is called inverse temperature. We adopt the term "softmax" from machine learning, where this transformation is ubiquitous.</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>The sensitivity parameter $\alpha$ controls how deterministically choices track value differences.</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>In our Stan implementations, this corresponds to <span class="in">`chi[m] = softmax(alpha * eta_m)`</span> where <span class="in">`eta_m`</span> contains the expected utilities of alternatives available in problem $m$.</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>This rule has several appealing properties:</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Probabilistic**: Assigns positive probability to all alternatives</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Monotonic in value**: Higher-value alternatives are more likely to be chosen</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Parameterized sensitivity**: α controls how sharply choices concentrate on high-value alternatives</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-softmax-demo</span></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Softmax choice probabilities for three alternatives with values η = (0.2, 0.5, 0.8) as sensitivity α varies. In the m_0 model, these values represent expected utilities computed as η = ψᵀυ."</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate softmax with varying alpha</span></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>])</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.array([softmax(a <span class="op">*</span> values) <span class="cf">for</span> a <span class="kw">in</span> alphas])</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">'η=0.2 (low)'</span>, <span class="st">'η=0.5 (medium)'</span>, <span class="st">'η=0.8 (high)'</span>]):</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>    ax.plot(alphas, probs[:, i], label<span class="op">=</span>label, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Uniform (α→0)'</span>)</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Sensitivity (α)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Choice Probability χ'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Softmax Choice Probabilities'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fundamental Properties of Softmax Choice</span></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>The following three properties hold for **any** value function $V: \mathcal{R} \to \mathbb{R}$. This generality is important—the properties follow from any choice model of this functional form, not any particular theory of value.</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### Property 1: Monotonicity in Sensitivity {#sec-monotonicity}</span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 1 (Monotonicity)</span></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>For any value function $V: \mathcal{R} \to \mathbb{R}$, holding $V$ fixed:</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For $r \in \mathcal{R}^*$ (value-maximizing): $P(\text{choose } r \mid \alpha, V)$ is **strictly increasing** in $\alpha$</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For $r \notin \mathcal{R}^*$ (suboptimal): $P(\text{choose } r \mid \alpha, V)$ is **strictly decreasing** in $\alpha$</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Theorem 1</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>**Part A: Value-maximizing alternatives ($r \in \mathcal{R}^*$)**</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>Let $r \in \mathcal{R}^*$ such that $V(r) = V^*$. Define the partition function:</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>Z(\alpha) = \sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>Taking the derivative of $P(r)$ with respect to $\alpha$:</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = \frac{\partial}{\partial \alpha} \left<span class="co">[</span><span class="ot">\frac{\exp(\alpha \cdot V(r))}{Z(\alpha)}\right</span><span class="co">]</span></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>Using the quotient rule:</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = \frac{Z(\alpha) \cdot V(r) \cdot \exp(\alpha \cdot V(r)) - \exp(\alpha \cdot V(r)) \cdot Z'(\alpha)}{Z(\alpha)^2}</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a>Simplifying:</span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot \left<span class="co">[</span><span class="ot">V(r) - \frac{Z'(\alpha)}{Z(\alpha)}\right</span><span class="co">]</span></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>Computing $Z'(\alpha)$:</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>Z'(\alpha) = \sum_{j \in \mathcal{R}} V(j) \cdot \exp(\alpha \cdot V(j))</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>\frac{Z'(\alpha)}{Z(\alpha)} = \sum_{j \in \mathcal{R}} V(j) \cdot P(j) = \mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>where $\mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span>$ is the expected value under the current choice distribution.</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>Thus:</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot <span class="co">[</span><span class="ot">V^* - \mathbb{E}[V]</span><span class="co">]</span></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>Since $V^* = \max_j V(j)$ and $\mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span>$ is a weighted average:</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span> = \sum_{j \in \mathcal{R}} P(j) \cdot V(j) \leq V^*</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>with equality only when $P(r) = 1$ for some $r \in \mathcal{R}^*$ (which occurs only as $\alpha \to \infty$).</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>For any finite $\alpha$, all alternatives receive positive probability under the softmax rule (since $\exp(x) &gt; 0$ for all real $x$), ensuring that both optimal and suboptimal alternatives contribute to the expectation. Hence $\mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span> &lt; V^*$ strictly, so:</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot <span class="co">[</span><span class="ot">V^* - \mathbb{E}[V]</span><span class="co">]</span> &gt; 0 \quad \blacksquare</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>**Part B: Suboptimal alternatives ($r \notin \mathcal{R}^*$)**</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>For $r \notin \mathcal{R}^*$, we have $V(r) &lt; V^*$. Following the same derivation:</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot <span class="co">[</span><span class="ot">V(r) - \mathbb{E}[V]</span><span class="co">]</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>Since $r$ is suboptimal and $\mathcal{R}^*$ is non-empty (so $P(\mathcal{R}^*) &gt; 0$ for all finite $\alpha$):</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>\mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span> \geq P(\mathcal{R}^*) \cdot V^* + P(r) \cdot V(r) &gt; P(\mathcal{R}^*) \cdot V(r) + P(r) \cdot V(r)</span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>The strict inequality follows because $V^* &gt; V(r)$.</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>Therefore, $V(r) - \mathbb{E}<span class="co">[</span><span class="ot">V</span><span class="co">]</span> &lt; 0$, and:</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>\frac{\partial P(r)}{\partial \alpha} = P(r) \cdot <span class="co">[</span><span class="ot">V(r) - \mathbb{E}[V]</span><span class="co">]</span> &lt; 0 \quad \blacksquare</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a><span class="fu">### Property 2: Perfect Optimization Limit {#sec-rationality}</span></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 2 (Convergence to Value Maximization)</span></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>For any value function $V: \mathcal{R} \to \mathbb{R}$, as $\alpha \to \infty$:</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>\lim_{\alpha \to \infty} P(\text{choose } r \mid \alpha, V) = </span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>1/|\mathcal{R}^*| &amp; \text{if } r \in \mathcal{R}^* <span class="sc">\\</span></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>0 &amp; \text{if } r \notin \mathcal{R}^*</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Theorem 2</span></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>**Case 1: $r \in \mathcal{R}^*$ (value-maximizing)**</span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>P(r) = \frac{\exp(\alpha \cdot V^*)}{|\mathcal{R}^*| \cdot \exp(\alpha \cdot V^*) + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot V(j))}</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>Dividing numerator and denominator by $\exp(\alpha \cdot V^*)$:</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>P(r) = \frac{1}{|\mathcal{R}^*| + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot [V(j) - V^*])}</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>For $j \in \mathcal{R}^-$, we have $V(j) &lt; V^*$, so $V(j) - V^* &lt; 0$.</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>As $\alpha \to \infty$:</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>\exp(\alpha \cdot <span class="co">[</span><span class="ot">V(j) - V^*</span><span class="co">]</span>) \to 0 \quad \text{for all } j \in \mathcal{R}^-</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>Thus:</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>\lim_{\alpha \to \infty} P(r) = \frac{1}{|\mathcal{R}^*|} \quad \blacksquare</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>**Case 2: $r \notin \mathcal{R}^*$ (suboptimal)**</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>P(r) = \frac{\exp(\alpha \cdot V(r))}{\sum_{s \in \mathcal{R}^*} \exp(\alpha \cdot V^*) + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot V(j))}</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>Dividing by $\exp(\alpha \cdot V^*)$:</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>P(r) = \frac{\exp(\alpha \cdot <span class="co">[</span><span class="ot">V(r) - V^*</span><span class="co">]</span>)}{|\mathcal{R}^*| + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot [V(j) - V^*])}</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>Since $V(r) - V^* &lt; 0$:</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Numerator $\to 0$</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Denominator $\geq |\mathcal{R}^*| &gt; 0$</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>\lim_{\alpha \to \infty} P(r) = 0 \quad \blacksquare</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a><span class="fu">### Property 3: Random Choice Limit {#sec-random}</span></span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 3 (Convergence to Uniform Choice)</span></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>For any value function $V: \mathcal{R} \to \mathbb{R}$, as $\alpha \to 0$:</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>\lim_{\alpha \to 0} P(\text{choose } r \mid \alpha, V) = \frac{1}{|\mathcal{R}|} \quad \text{for all } r \in \mathcal{R}</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Theorem 3</span></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>Using Taylor expansion $\exp(x) = 1 + x + O(x^2)$:</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>P(r) = \frac{1 + \alpha \cdot V(r) + O(\alpha^2)}{\sum_{j \in \mathcal{R}} (1 + \alpha \cdot V(j) + O(\alpha^2))}</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>= \frac{1 + \alpha \cdot V(r) + O(\alpha^2)}{|\mathcal{R}| + \alpha \cdot \sum_j V(j) + O(\alpha^2)}</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>As $\alpha \to 0$:</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>\lim_{\alpha \to 0} P(r) = \frac{1}{|\mathcal{R}|} \quad \blacksquare</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>**Alternative proof via logarithms:**</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>\log P(r) = \alpha \cdot V(r) - \log\left<span class="co">[</span><span class="ot">\sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))\right</span><span class="co">]</span></span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>Expanding the log-sum-exp:</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>\log\left<span class="co">[</span><span class="ot">\sum_{j \in \mathcal{R}} \exp(\alpha \cdot V(j))\right</span><span class="co">]</span> = \log|\mathcal{R}| + \frac{\alpha \cdot \sum_j V(j)}{|\mathcal{R}|} + O(\alpha^2)</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>Therefore:</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>\log P(r) = -\log|\mathcal{R}| + \alpha \cdot \left<span class="co">[</span><span class="ot">V(r) - \frac{\sum_j V(j)}{|\mathcal{R}|}\right</span><span class="co">]</span> + O(\alpha^2)</span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>As $\alpha \to 0$: $\log P(r) \to -\log|\mathcal{R}|$, hence $P(r) \to 1/|\mathcal{R}|$. $\blacksquare$</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary: The Three Properties</span></span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-three-properties</span></span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Visual summary of the three fundamental properties. Left: Monotonicity—optimal alternative probability increases with α. Middle: Limiting behavior at α→∞ (deterministic optimal choice). Right: Limiting behavior at α→0 (uniform random choice)."</span></span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a><span class="co"># Values for a 3-alternative problem</span></span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.array([<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>])  <span class="co"># Third is optimal</span></span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a><span class="co"># Property 1: Monotonicity</span></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="dv">8</span>, <span class="dv">100</span>)</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.array([softmax(a <span class="op">*</span> values) <span class="cf">for</span> a <span class="kw">in</span> alphas])</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(alphas, probs[:, <span class="dv">2</span>], <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="fl">2.5</span>, label<span class="op">=</span><span class="st">'Optimal (η=0.9)'</span>)</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(alphas, probs[:, <span class="dv">1</span>], <span class="st">'orange'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Middle (η=0.5)'</span>)</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].plot(alphas, probs[:, <span class="dv">0</span>], <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Low (η=0.3)'</span>)</span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'α'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'χ (choice probability)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Property 1: Monotonicity'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].legend(fontsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Property 2: α → ∞</span></span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a>alpha_large <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>probs_large <span class="op">=</span> softmax(alpha_large <span class="op">*</span> values)</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar([<span class="st">'η=0.3'</span>, <span class="st">'η=0.5'</span>, <span class="st">'η=0.9'</span>], probs_large, color<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'orange'</span>, <span class="st">'blue'</span>])</span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'blue'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Limit'</span>)</span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'χ (choice probability)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'Property 2: α → ∞</span><span class="ch">\n</span><span class="st">(Deterministic Optimal)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylim(<span class="dv">0</span>, <span class="fl">1.1</span>)</span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a><span class="co"># Property 3: α → 0</span></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>alpha_small <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a>probs_small <span class="op">=</span> softmax(alpha_small <span class="op">*</span> values)</span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].bar([<span class="st">'η=0.3'</span>, <span class="st">'η=0.5'</span>, <span class="st">'η=0.9'</span>], probs_small, color<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'orange'</span>, <span class="st">'blue'</span>])</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].axhline(y<span class="op">=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Uniform'</span>)</span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'χ (choice probability)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'Property 3: α → 0</span><span class="ch">\n</span><span class="st">(Uniform Random)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylim(<span class="dv">0</span>, <span class="fl">0.5</span>)</span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, axis<span class="op">=</span><span class="st">'y'</span>)</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a><span class="fu">## Application to Subjective Expected Utility</span></span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a>We now specialize the general softmax framework to the case where values are subjective expected utilities (SEU).</span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a><span class="fu">### SEU as a Value Function</span></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a><span class="fu">## Notation Summary (SEU Specialization)</span></span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Symbol <span class="pp">|</span> Description <span class="pp">|</span></span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-------------|</span></span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $K$ <span class="pp">|</span> Number of possible consequences (outcomes) <span class="pp">|</span></span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\boldsymbol{\upsilon} \in \mathbb{R}^K$ <span class="pp">|</span> Utility vector over consequences <span class="pp">|</span></span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\boldsymbol{\psi}_r \in \Delta^{K-1}$ <span class="pp">|</span> Subjective probability distribution over consequences for alternative $r$ <span class="pp">|</span></span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}$ <span class="pp">|</span> Expected utility of alternative $r$ <span class="pp">|</span></span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\boldsymbol{\chi}_m$ <span class="pp">|</span> Choice probability vector for problem $m$ <span class="pp">|</span></span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a><span class="fu">## Definition: Subjective Expected Utility</span></span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>Each alternative $r$ is associated with:</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Subjective probabilities** $\boldsymbol{\psi}_r \in \Delta^{K-1}$ over $K$ consequences</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**An expected utility** computed as:</span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a>   \eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon} = \sum_{k=1}^{K} \psi_{r,k} \cdot \upsilon_k</span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>The **choice probability** for alternative $r$ in problem $m$ is then:</span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>\chi_{m,r} = \frac{\exp(\alpha \cdot \eta_r)}{\sum_{j: I_{m,j}=1} \exp(\alpha \cdot \eta_j)}</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a>where $I_{m,r} = 1$ indicates that alternative $r$ is available in problem $m$.</span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>**Key observation:** The expected utility $\eta_r$ serves as our value function $V(r) = \eta_r$. Therefore, all three properties proved above apply immediately.</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a><span class="fu">### Corollaries for SEU</span></span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>By substituting $V(r) = \eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}$ into Properties 1-3:</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a><span class="fu">## Corollary 1 (Monotonicity for SEU)</span></span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>Holding utilities $\boldsymbol{\upsilon}$ and beliefs $\boldsymbol{\psi}$ fixed, higher sensitivity $\alpha$ increases the probability of choosing alternatives that maximize expected utility $\eta$.</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a><span class="fu">## Corollary 2 (Perfect Rationality)</span></span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>As $\alpha \to \infty$, the decision maker chooses uniformly among SEU-maximizing alternatives (those with highest $\eta$) with probability 1. When there is a unique maximizer, choice becomes deterministic.</span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a><span class="fu">## Corollary 3 (Random Choice)</span></span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>As $\alpha \to 0$, the decision maker chooses uniformly at random over available alternatives, independent of $\eta$ values.</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a><span class="fu">### What SEU Adds to the Framework</span></span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a>While the mathematical properties of softmax choice hold for any value function, the SEU construction provides:</span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Decomposition**: Expected utilities $\eta$ decompose into beliefs ($\boldsymbol{\psi}$) and utilities ($\boldsymbol{\upsilon}$), allowing separate analysis of epistemic and preference components</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Normative content**: SEU maximization is a rationality criterion—Properties 1-3 characterize adherence to this normative standard <span class="co">[</span><span class="ot">@savage1954</span><span class="co">]</span></span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Empirical predictions**: The model predicts that choices will track $\eta$, providing testable restrictions</span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a><span class="fu">## Scale Invariance and Representation</span></span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Representation Problem</span></span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>A fundamental property of utility functions is that they are unique only up to positive affine transformations. This raises a critical question: how can we meaningfully interpret $\alpha$ when the scale of utility is arbitrary?</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 4 (Scale Invariance)</span></span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>Let $\boldsymbol{\upsilon}$ be a utility vector and define a rescaled utility:</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>\tilde{\upsilon}_k = a \cdot \upsilon_k + b \quad \text{where } a &gt; 0</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a>Then:</span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\tilde{\eta}_r = a \cdot \eta_r + b$ for all alternatives $r$</span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$P(\text{choose } r \mid \alpha, \tilde{\boldsymbol{\upsilon}}) = P(\text{choose } r \mid \alpha \cdot a, \boldsymbol{\upsilon})$</span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>The pair $(\alpha, \boldsymbol{\upsilon})$ and $(\alpha \cdot a, \tilde{\boldsymbol{\upsilon}})$ generate **identical** choice probabilities.</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Theorem 4</span></span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>**Part 1:** </span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a>\tilde{\eta}_r = \sum_k \psi_{r,k} \cdot <span class="co">[</span><span class="ot">a \cdot \upsilon_k + b</span><span class="co">]</span></span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a>= a \cdot \sum_k \psi_{r,k} \cdot \upsilon_k + b \cdot \sum_k \psi_{r,k}</span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a>= a \cdot \eta_r + b</span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>since $\sum_k \psi_{r,k} = 1$.</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a>**Part 2:**</span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r \mid \alpha, \tilde{\boldsymbol{\upsilon}}) = \frac{\exp(\alpha \cdot <span class="co">[</span><span class="ot">a \cdot \eta_r + b</span><span class="co">]</span>)}{\sum_j \exp(\alpha \cdot <span class="co">[</span><span class="ot">a \cdot \eta_j + b</span><span class="co">]</span>)}</span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a>= \frac{\exp(\alpha a \cdot \eta_r) \cdot \exp(\alpha b)}{\sum_j \exp(\alpha a \cdot \eta_j) \cdot \exp(\alpha b)}</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a>= \frac{\exp(\alpha a \cdot \eta_r)}{\sum_j \exp(\alpha a \cdot \eta_j)}</span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>= P(\text{choose } r \mid \alpha a, \boldsymbol{\upsilon}) \quad \blacksquare</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a>**Key Implication:** Without fixing the utility scale, $\alpha$ and the scale of utility are confounded—they cannot be separately interpreted from choice behavior alone. Scaling utilities by a factor $a$ is equivalent to scaling sensitivity by $1/a$.</span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resolution: Utility Standardization</span></span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a>To make $\alpha$ interpretable as "sensitivity to expected utility differences," we adopt a standard standardization convention.</span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a><span class="fu">## Standardization Convention</span></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a>We constrain utilities to lie in $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ by assigning:</span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\upsilon_{\text{worst}} = 0$ (utility of the worst consequence)</span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\upsilon_{\text{best}} = 1$ (utility of the best consequence)</span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a>For $K$ ordered consequences, this means:</span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a>0 = \upsilon_1 \leq \upsilon_2 \leq \cdots \leq \upsilon_K = 1</span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a>This is the standard standardization in decision theory, where the utility function is anchored at the endpoints of the consequence space.</span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a>This standardization is without loss of generality—it simply fixes a representation from the equivalence class of utility functions related by positive affine transformations. Any utility function can be rescaled to satisfy this convention.</span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-495"><a href="#cb4-495" aria-hidden="true" tabindex="-1"></a>**Interpretive Result:** Given this standardization, $\alpha$ measures sensitivity to expected utility differences on a standardized scale where the full range of possible utilities spans exactly one unit.</span>
<span id="cb4-496"><a href="#cb4-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpretation of α Under Standardization</span></span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a>With utilities standardized to $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$, expected utilities satisfy $\eta_r \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ for all alternatives $r$ (since $\eta_r$ is a convex combination of utilities). The maximum possible difference in expected utility is therefore 1.</span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a>The sensitivity parameter $\alpha$ has a precise interpretation via the log-odds ratio:</span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a>\log\left<span class="co">[</span><span class="ot">\frac{\chi_{r}}{\chi_{s}}\right</span><span class="co">]</span> = \alpha \cdot <span class="co">[</span><span class="ot">\eta_r - \eta_s</span><span class="co">]</span></span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-alpha-interpretation</span></span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: "Interpretation of α for a one-unit expected utility difference (maximum possible difference with standardized utilities)."</span></span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a>alpha_vals <span class="op">=</span> [<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">10</span>]</span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> a <span class="kw">in</span> alpha_vals:</span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a>    odds_ratio <span class="op">=</span> np.exp(a)</span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a>    prob_better <span class="op">=</span> odds_ratio <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> odds_ratio)</span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a>    data.append({</span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a>        <span class="st">'α'</span>: a,</span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Log-odds'</span>: <span class="ss">f'</span><span class="sc">{</span>a<span class="sc">:.1f}</span><span class="ss">'</span>,</span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Odds ratio'</span>: <span class="ss">f'</span><span class="sc">{</span>odds_ratio<span class="sc">:.2f}</span><span class="ss">'</span>,</span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a>        <span class="st">'P(higher η)'</span>: <span class="ss">f'</span><span class="sc">{</span>prob_better<span class="sc">:.1%}</span><span class="ss">'</span></span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb4-528"><a href="#cb4-528" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb4-529"><a href="#cb4-529" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a>**General interpretation:** $\alpha$ measures the log-odds change per unit of expected utility difference. Higher $\alpha$ means choices become more deterministically aligned with $\eta$ rankings.</span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rates of Convergence</span></span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-535"><a href="#cb4-535" aria-hidden="true" tabindex="-1"></a>The limiting behavior established in Properties 2 and 3 occurs at different rates, which we now characterize precisely.</span>
<span id="cb4-536"><a href="#cb4-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-537"><a href="#cb4-537" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence Rate for Property 2 ($\alpha \to \infty$)</span></span>
<span id="cb4-538"><a href="#cb4-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-539"><a href="#cb4-539" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-540"><a href="#cb4-540" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 5 (Exponential Convergence to Optimality)</span></span>
<span id="cb4-541"><a href="#cb4-541" aria-hidden="true" tabindex="-1"></a>Let $\Delta = \min<span class="sc">\{</span>V^* - V(r) : r \notin \mathcal{R}^*\}$ be the minimum gap between optimal and suboptimal values. For any suboptimal alternative $r \notin \mathcal{R}^*$:</span>
<span id="cb4-542"><a href="#cb4-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-543"><a href="#cb4-543" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-544"><a href="#cb4-544" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r \mid \alpha, V) = O\left(e^{-\alpha \Delta}\right) \quad \text{as } \alpha \to \infty</span>
<span id="cb4-545"><a href="#cb4-545" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-546"><a href="#cb4-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-547"><a href="#cb4-547" aria-hidden="true" tabindex="-1"></a>Convergence to the optimality limit is exponential with rate $\Delta$.</span>
<span id="cb4-548"><a href="#cb4-548" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-549"><a href="#cb4-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-550"><a href="#cb4-550" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-551"><a href="#cb4-551" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Theorem 5</span></span>
<span id="cb4-552"><a href="#cb4-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-553"><a href="#cb4-553" aria-hidden="true" tabindex="-1"></a>For $r \notin \mathcal{R}^*$, recall from the proof of Property 2:</span>
<span id="cb4-554"><a href="#cb4-554" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-555"><a href="#cb4-555" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r) = \frac{\exp(\alpha \cdot <span class="co">[</span><span class="ot">V(r) - V^*</span><span class="co">]</span>)}{|\mathcal{R}^*| + \sum_{j \in \mathcal{R}^-} \exp(\alpha \cdot [V(j) - V^*])}</span>
<span id="cb4-556"><a href="#cb4-556" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-557"><a href="#cb4-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-558"><a href="#cb4-558" aria-hidden="true" tabindex="-1"></a>Since $V(r) - V^* \leq -\Delta &lt; 0$:</span>
<span id="cb4-559"><a href="#cb4-559" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-560"><a href="#cb4-560" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r) \leq \frac{\exp(-\alpha \Delta)}{|\mathcal{R}^*|} = \frac{1}{|\mathcal{R}^*|} e^{-\alpha \Delta}</span>
<span id="cb4-561"><a href="#cb4-561" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-562"><a href="#cb4-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-563"><a href="#cb4-563" aria-hidden="true" tabindex="-1"></a>The denominator is bounded below by $|\mathcal{R}^*| \geq 1$, giving:</span>
<span id="cb4-564"><a href="#cb4-564" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-565"><a href="#cb4-565" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r) = O(e^{-\alpha \Delta}) \quad \blacksquare</span>
<span id="cb4-566"><a href="#cb4-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-567"><a href="#cb4-567" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-568"><a href="#cb4-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-569"><a href="#cb4-569" aria-hidden="true" tabindex="-1"></a>**Interpretation:** Larger value gaps $\Delta$ lead to faster concentration on optimal alternatives. When the best alternative is clearly superior (large $\Delta$), even moderate $\alpha$ yields near-deterministic choice.</span>
<span id="cb4-570"><a href="#cb4-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-571"><a href="#cb4-571" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convergence Rate for Property 3 ($\alpha \to 0$)</span></span>
<span id="cb4-572"><a href="#cb4-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-573"><a href="#cb4-573" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="minimal"}</span>
<span id="cb4-574"><a href="#cb4-574" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 6 (Linear Convergence to Uniformity)</span></span>
<span id="cb4-575"><a href="#cb4-575" aria-hidden="true" tabindex="-1"></a>For any alternative $r \in \mathcal{R}$, let $\bar{V} = \frac{1}{|\mathcal{R}|}\sum_j V(j)$ denote the arithmetic mean of values. Then:</span>
<span id="cb4-576"><a href="#cb4-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-577"><a href="#cb4-577" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-578"><a href="#cb4-578" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r \mid \alpha, V) = \frac{1}{|\mathcal{R}|} + \alpha \cdot \left<span class="co">[</span><span class="ot">V(r) - \bar{V}\right</span><span class="co">]</span> \cdot \frac{1}{|\mathcal{R}|} + O(\alpha^2)</span>
<span id="cb4-579"><a href="#cb4-579" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-580"><a href="#cb4-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-581"><a href="#cb4-581" aria-hidden="true" tabindex="-1"></a>Convergence to uniformity is first-order (linear) in $\alpha$.</span>
<span id="cb4-582"><a href="#cb4-582" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-583"><a href="#cb4-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-584"><a href="#cb4-584" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb4-585"><a href="#cb4-585" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of Theorem 6</span></span>
<span id="cb4-586"><a href="#cb4-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-587"><a href="#cb4-587" aria-hidden="true" tabindex="-1"></a>Expanding $\exp(\alpha V(r)) = 1 + \alpha V(r) + \frac{\alpha^2 V(r)^2}{2} + O(\alpha^3)$:</span>
<span id="cb4-588"><a href="#cb4-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-589"><a href="#cb4-589" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-590"><a href="#cb4-590" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r) = \frac{1 + \alpha V(r) + O(\alpha^2)}{\sum_j <span class="co">[</span><span class="ot">1 + \alpha V(j) + O(\alpha^2)</span><span class="co">]</span>}</span>
<span id="cb4-591"><a href="#cb4-591" aria-hidden="true" tabindex="-1"></a>= \frac{1 + \alpha V(r) + O(\alpha^2)}{|\mathcal{R}| + \alpha \sum_j V(j) + O(\alpha^2)}</span>
<span id="cb4-592"><a href="#cb4-592" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-593"><a href="#cb4-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-594"><a href="#cb4-594" aria-hidden="true" tabindex="-1"></a>Let $S = \sum_j V(j) = |\mathcal{R}| \cdot \bar{V}$. Using the expansion $(1+x)^{-1} = 1 - x + O(x^2)$:</span>
<span id="cb4-595"><a href="#cb4-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-596"><a href="#cb4-596" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-597"><a href="#cb4-597" aria-hidden="true" tabindex="-1"></a>P(\text{choose } r) = \frac{1 + \alpha V(r)}{|\mathcal{R}|} \cdot \left(1 + \frac{\alpha S}{|\mathcal{R}|}\right)^{-1} + O(\alpha^2)</span>
<span id="cb4-598"><a href="#cb4-598" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-599"><a href="#cb4-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-600"><a href="#cb4-600" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-601"><a href="#cb4-601" aria-hidden="true" tabindex="-1"></a>= \frac{1 + \alpha V(r)}{|\mathcal{R}|} \cdot \left(1 - \frac{\alpha S}{|\mathcal{R}|}\right) + O(\alpha^2)</span>
<span id="cb4-602"><a href="#cb4-602" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-603"><a href="#cb4-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-604"><a href="#cb4-604" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-605"><a href="#cb4-605" aria-hidden="true" tabindex="-1"></a>= \frac{1}{|\mathcal{R}|} + \frac{\alpha V(r)}{|\mathcal{R}|} - \frac{\alpha S}{|\mathcal{R}|^2} + O(\alpha^2)</span>
<span id="cb4-606"><a href="#cb4-606" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-607"><a href="#cb4-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-608"><a href="#cb4-608" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-609"><a href="#cb4-609" aria-hidden="true" tabindex="-1"></a>= \frac{1}{|\mathcal{R}|} + \frac{\alpha}{|\mathcal{R}|} \left<span class="co">[</span><span class="ot">V(r) - \frac{S}{|\mathcal{R}|}\right</span><span class="co">]</span> + O(\alpha^2)</span>
<span id="cb4-610"><a href="#cb4-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-611"><a href="#cb4-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-612"><a href="#cb4-612" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-613"><a href="#cb4-613" aria-hidden="true" tabindex="-1"></a>= \frac{1}{|\mathcal{R}|} + \frac{\alpha}{|\mathcal{R}|} \left<span class="co">[</span><span class="ot">V(r) - \bar{V}\right</span><span class="co">]</span> + O(\alpha^2) \quad \blacksquare</span>
<span id="cb4-614"><a href="#cb4-614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-615"><a href="#cb4-615" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-616"><a href="#cb4-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-617"><a href="#cb4-617" aria-hidden="true" tabindex="-1"></a>**Interpretation:** Near $\alpha = 0$, deviations from uniform choice are proportional to $\alpha$ and to how much an alternative's value exceeds the mean. The coefficient $<span class="co">[</span><span class="ot">V(r) - \bar{V}</span><span class="co">]</span>/|\mathcal{R}|$ determines the direction and magnitude of the first-order effect.</span>
<span id="cb4-618"><a href="#cb4-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-619"><a href="#cb4-619" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discussion</span></span>
<span id="cb4-620"><a href="#cb4-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-621"><a href="#cb4-621" aria-hidden="true" tabindex="-1"></a>Having established the mathematical properties of the model, we turn to its interpretation.</span>
<span id="cb4-622"><a href="#cb4-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-623"><a href="#cb4-623" aria-hidden="true" tabindex="-1"></a><span class="fu">### What the Model Describes</span></span>
<span id="cb4-624"><a href="#cb4-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-625"><a href="#cb4-625" aria-hidden="true" tabindex="-1"></a>Our model describes decision makers who are **committed to SEU maximization but have limited sensitivity to its implications**. The sensitivity parameter $\alpha$ measures how reliably their choices track the SEU ranking:</span>
<span id="cb4-626"><a href="#cb4-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-627"><a href="#cb4-627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High $\alpha$: Choices reliably favor higher-$\eta$ alternatives</span>
<span id="cb4-628"><a href="#cb4-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Low $\alpha$: Choices are noisy relative to the SEU ranking</span>
<span id="cb4-629"><a href="#cb4-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha \to \infty$: Near-deterministic choice of SEU-maximizing alternatives</span>
<span id="cb4-630"><a href="#cb4-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha \to 0$: Choices become independent of SEU values</span>
<span id="cb4-631"><a href="#cb4-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-632"><a href="#cb4-632" aria-hidden="true" tabindex="-1"></a>This is not a model of rational agents in the classical sense. SEU maximization—the normative standard—corresponds only to the limit $\alpha \to \infty$. For any finite $\alpha$, the model permits systematic departures from SEU maximization: lower-$\eta$ alternatives are chosen with positive probability.</span>
<span id="cb4-633"><a href="#cb4-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-634"><a href="#cb4-634" aria-hidden="true" tabindex="-1"></a><span class="fu">### What the Model Is Not</span></span>
<span id="cb4-635"><a href="#cb4-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-636"><a href="#cb4-636" aria-hidden="true" tabindex="-1"></a>**Not a cognitive process model.** We make no claim about how decision makers actually deliberate. The model is silent on whether agents compute probabilities, form expectations, or engage in any particular mental procedure. It specifies a distribution over choices, not a mechanism that generates them. Our concern is to investigate the extent to which a decision maker's behavior can be captured by viewing that behavior *as-if* it came from a decision maker who is committed to subjective expected utility maximization but has limited sensitivity to its implications.</span>
<span id="cb4-637"><a href="#cb4-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-638"><a href="#cb4-638" aria-hidden="true" tabindex="-1"></a>**Not bounded rationality.** Bounded rationality programs, in their various formulations <span class="co">[</span><span class="ot">@simon1955; @gigerenzer1996</span><span class="co">]</span>, typically propose alternative decision procedures—heuristics, satisficing rules, or fast-and-frugal strategies—that agents use in place of optimization. Our model posits no such alternative procedures. Nor do we invoke notions of ecological rationality or fit between heuristics and environmental structure. The model simply describes a stochastic relationship between SEU values and choice probabilities.</span>
<span id="cb4-639"><a href="#cb4-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-640"><a href="#cb4-640" aria-hidden="true" tabindex="-1"></a><span class="fu">### A Conceptual Lens: Commitment and Performance</span></span>
<span id="cb4-641"><a href="#cb4-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-642"><a href="#cb4-642" aria-hidden="true" tabindex="-1"></a>A useful conceptual lens comes from Isaac Levi's distinction between **commitment** and **performance** <span class="co">[</span><span class="ot">@levi1980</span><span class="co">]</span>. We may be committed to standards we fail to perform up to. Most of us are committed to the laws of arithmetic despite occasionally making calculation errors; the errors are failures of performance, not rejections of the standard.</span>
<span id="cb4-643"><a href="#cb4-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-644"><a href="#cb4-644" aria-hidden="true" tabindex="-1"></a>If we take SEU theory as specifying the decision maker's normative commitments, then $\alpha$ measures their **tendency to perform in accordance with those commitments**. This framing preserves SEU as normatively fundamental while allowing systematic departures in observed behavior.</span>
<span id="cb4-645"><a href="#cb4-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-646"><a href="#cb4-646" aria-hidden="true" tabindex="-1"></a>Our framework differs from Levi's in important ways:</span>
<span id="cb4-647"><a href="#cb4-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-648"><a href="#cb4-648" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decision maker's vs. observer's perspective.** Levi's framework is primarily concerned with the decision maker's own deliberative standards—what an agent should believe and prefer from the first-person standpoint. Our framework, by contrast, adopts the observer's perspective: we model choice behavior as it appears to an external analyst who observes decisions and infers parameters.</span>
<span id="cb4-649"><a href="#cb4-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-650"><a href="#cb4-650" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Probabilistic vs. algebraic theories.** Following Luce's <span class="co">[</span><span class="ot">@luce1959</span><span class="co">]</span> distinction, our framework is a *probabilistic* theory of choice—it specifies a probability distribution over choices given the decision problem. Levi's framework is more aligned with *algebraic* theories that characterize rational choice through axioms on preference orderings rather than stochastic choice rules.</span>
<span id="cb4-651"><a href="#cb4-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-652"><a href="#cb4-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SEU as the normative standard.** Levi famously rejected subjective expected utility theory as the standard of rational choice, instead advocating for generalizations that accommodate indeterminate probabilities and utilities <span class="co">[</span><span class="ot">@levi1980</span><span class="co">]</span>. Our framework, by contrast, takes SEU maximization as the normative standard and models departures from it through the sensitivity parameter $\alpha$.</span>
<span id="cb4-653"><a href="#cb4-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-654"><a href="#cb4-654" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb4-655"><a href="#cb4-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-656"><a href="#cb4-656" aria-hidden="true" tabindex="-1"></a>We have established three fundamental properties of the softmax choice model:</span>
<span id="cb4-657"><a href="#cb4-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-658"><a href="#cb4-658" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Monotonicity**: Higher $\alpha$ increases probability of choosing alternatives with higher value $V(r)$</span>
<span id="cb4-659"><a href="#cb4-659" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Perfect optimization limit**: As $\alpha \to \infty$, choices become deterministically concentrated on value-maximizing alternatives</span>
<span id="cb4-660"><a href="#cb4-660" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Random choice limit**: As $\alpha \to 0$, choices become uniformly random</span>
<span id="cb4-661"><a href="#cb4-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-662"><a href="#cb4-662" aria-hidden="true" tabindex="-1"></a>Additionally, we characterized the rates at which these limits are approached:</span>
<span id="cb4-663"><a href="#cb4-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-664"><a href="#cb4-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence to optimality is **exponential** with rate determined by the value gap $\Delta$</span>
<span id="cb4-665"><a href="#cb4-665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convergence to uniformity is **linear** (first-order) in $\alpha$</span>
<span id="cb4-666"><a href="#cb4-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-667"><a href="#cb4-667" aria-hidden="true" tabindex="-1"></a>These properties hold for any value function $V$. When $V$ is taken to be subjective expected utility, the framework provides a model of decision-making that interpolates between random choice and SEU maximization, with $\alpha$ governing the degree of sensitivity to expected utility differences.</span>
<span id="cb4-668"><a href="#cb4-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-669"><a href="#cb4-669" aria-hidden="true" tabindex="-1"></a>The standardization of utilities to $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$ fixes a representation from the equivalence class of utility functions, making $\alpha$ interpretable as sensitivity to standardized expected utility differences.</span>
<span id="cb4-670"><a href="#cb4-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-671"><a href="#cb4-671" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb4-672"><a href="#cb4-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-673"><a href="#cb4-673" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb4-674"><a href="#cb4-674" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>