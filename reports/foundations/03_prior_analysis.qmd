---
title: "Prior Predictive Analysis"
subtitle: "Foundational Report 3"
description: |
  Analysis of the prior predictive distribution for model m_0, examining 
  what range of choice behaviors the prior permits before seeing any data.
categories: [foundations, validation, m_0]
execute:
  cache: true
---

```{python}
#| label: setup
#| include: false

import sys
import os

# Add parent directories to path
sys.path.insert(0, os.path.join(os.getcwd(), '..'))
project_root = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, project_root)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)
```

## Introduction

Having established the abstract formulation ([Report 1](01_abstract_formulation.qmd)) and its concrete implementation ([Report 2](02_concrete_implementation.qmd)), we now examine the **prior predictive distribution**: what range of choice behaviors does the model permit before observing any data?

Prior predictive analysis serves several purposes:

1. **Prior validation**: Do the priors produce sensible behaviors?
2. **Model understanding**: What parameter combinations are a priori plausible?
3. **Experimental design**: Is the design rich enough to distinguish different behaviors?

::: {.callout-note}
## The Prior Predictive Distribution
The prior predictive distribution is the distribution over observables (choices) induced by:

1. Drawing parameters from the prior: $(\alpha, \boldsymbol{\beta}, \boldsymbol{\delta}) \sim p(\theta)$
2. Computing derived quantities: $\boldsymbol{\psi}, \boldsymbol{\upsilon}, \boldsymbol{\eta}, \boldsymbol{\chi}$
3. Simulating choices: $y_m \sim \text{Categorical}(\boldsymbol{\chi}_m)$

This gives us a distribution over possible datasets *before* conditioning on actual observations.
:::

## Study Design

We use a moderately complex study design to illustrate the prior predictive distribution:

```{python}
#| label: study-design-config
#| echo: true

# Study design configuration
config = {
    "M": 25,                    # Number of decision problems
    "K": 3,                     # Number of consequences
    "D": 5,                     # Feature dimensions
    "R": 15,                    # Number of distinct alternatives
    "min_alts_per_problem": 2,  # Minimum alternatives per problem
    "max_alts_per_problem": 5,  # Maximum alternatives per problem
    "feature_dist": "normal",   # Feature distribution
    "feature_params": {"loc": 0, "scale": 1}
}

print(f"Study Design Configuration:")
print(f"  Decision problems (M): {config['M']}")
print(f"  Consequences (K): {config['K']}")
print(f"  Feature dimensions (D): {config['D']}")
print(f"  Distinct alternatives (R): {config['R']}")
print(f"  Alternatives per problem: {config['min_alts_per_problem']}-{config['max_alts_per_problem']}")
```

```{python}
#| label: generate-study-design
#| output: false

from utils.study_design import StudyDesign

# Create and generate the study design
study = StudyDesign(
    M=config["M"],
    K=config["K"],
    D=config["D"],
    R=config["R"],
    min_alts_per_problem=config["min_alts_per_problem"],
    max_alts_per_problem=config["max_alts_per_problem"],
    feature_dist=config["feature_dist"],
    feature_params=config["feature_params"],
    design_name="prior_analysis"
)
study.generate()
```

```{python}
#| label: design-summary
#| echo: false

# Extract design properties
w = np.array(study.w)
I = np.array(study.I)
N = I.sum(axis=1)

print(f"\nGenerated Study Design:")
print(f"  Total alternatives across problems: {I.sum()}")
print(f"  Alternatives per problem: min={N.min()}, max={N.max()}, mean={N.mean():.1f}")
print(f"  Alternative appearances: min={I.sum(axis=0).min()}, max={I.sum(axis=0).max()}, mean={I.sum(axis=0).mean():.1f}")
```

### Design Characteristics

```{python}
#| label: fig-design-characteristics
#| fig-cap: "Study design characteristics. Left: Number of alternatives in each decision problem. Right: Frequency with which each alternative appears across problems."

M = config["M"]
R = config["R"]

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Alternatives per problem
axes[0].bar(range(1, M+1), N, color='steelblue', alpha=0.7)
axes[0].axhline(y=N.mean(), color='red', linestyle='--', label=f'Mean = {N.mean():.1f}')
axes[0].set_xlabel('Decision Problem')
axes[0].set_ylabel('Number of Alternatives')
axes[0].set_title('Alternatives per Problem')
axes[0].legend()

# Alternative frequency
alt_freq = I.sum(axis=0)
axes[1].bar(range(1, R+1), alt_freq, color='coral', alpha=0.7)
axes[1].axhline(y=alt_freq.mean(), color='red', linestyle='--', label=f'Mean = {alt_freq.mean():.1f}')
axes[1].set_xlabel('Alternative')
axes[1].set_ylabel('Frequency (# problems)')
axes[1].set_title('Alternative Appearance Frequency')
axes[1].legend()

plt.tight_layout()
plt.show()
```

## Prior Predictive Simulation Using m_0_sim.stan

We use the simulation model `m_0_sim.stan` to generate prior predictive samples. This ensures that our analysis matches exactly how the model generates data, including all numerical details of the Stan implementation.

```{python}
#| label: run-prior-predictive
#| output: false

from analysis.prior_predictive import PriorPredictiveAnalysis
import tempfile

# Create a temporary output directory for this analysis
output_dir = tempfile.mkdtemp(prefix="prior_pred_")

# Run prior predictive analysis using m_0_sim.stan
analysis = PriorPredictiveAnalysis(
    model_path=None,  # Uses default m_0_sim.stan
    study_design=study,
    output_dir=output_dir,
    n_param_samples=200,    # Number of parameter configurations
    n_choice_samples=5      # Choice samples per parameter config
)

# Run the analysis (this calls Stan)
samples = analysis.run()
```

```{python}
#| label: samples-info
#| echo: false

print(f"Prior Predictive Simulation Complete:")
print(f"  Parameter configurations sampled: {samples['param_set'].nunique()}")
print(f"  Total samples: {len(samples)}")
print(f"  Columns available: {len(samples.columns)}")
```

## Prior Distribution of Parameters

### Sensitivity Parameter ($\alpha$)

The sensitivity parameter is sampled from a Lognormal(0, 1) prior in `m_0_sim.stan`:

```{python}
#| label: fig-alpha-prior
#| fig-cap: "Prior distribution of the sensitivity parameter α sampled from m_0_sim.stan. The distribution has median ≈ 1 with substantial probability mass on both low sensitivity (random-like choice) and high sensitivity (near-optimal choice)."

# Extract unique alpha values (one per parameter set)
alpha_samples = samples.groupby('param_set')['sim_alpha'].first().values

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Histogram with truncation for visualization
alpha_plot = alpha_samples[alpha_samples < 15]
axes[0].hist(alpha_plot, bins=40, density=True, alpha=0.7, color='steelblue', edgecolor='white')
axes[0].axvline(x=np.median(alpha_samples), color='red', linestyle='--', linewidth=2, 
                label=f'Median = {np.median(alpha_samples):.2f}')
axes[0].axvline(x=1, color='orange', linestyle=':', linewidth=2, label='α = 1')
axes[0].set_xlabel('Sensitivity (α)')
axes[0].set_ylabel('Density')
axes[0].set_title('Prior Distribution of α')
axes[0].legend()
axes[0].set_xlim(0, 15)

# Log scale to see full distribution
axes[1].hist(np.log10(alpha_samples + 0.01), bins=40, density=True, alpha=0.7, 
             color='steelblue', edgecolor='white')
axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='α = 1')
axes[1].set_xlabel('log₁₀(α)')
axes[1].set_ylabel('Density')
axes[1].set_title('Prior Distribution of α (log scale)')
axes[1].legend()

plt.tight_layout()
plt.show()

print(f"\nAlpha summary statistics:")
print(f"  Mean: {np.mean(alpha_samples):.2f}")
print(f"  Median: {np.median(alpha_samples):.2f}")
print(f"  Std: {np.std(alpha_samples):.2f}")
print(f"  95% interval: [{np.percentile(alpha_samples, 2.5):.2f}, {np.percentile(alpha_samples, 97.5):.2f}]")
```

The lognormal prior covers a wide range of sensitivity values:

- **Low α (< 0.5)**: Near-random choice, weak sensitivity to SEU
- **Moderate α (0.5–3)**: Probabilistic choice with clear SEU preference
- **High α (> 5)**: Near-deterministic choice, strong SEU maximization

### Utility Parameters

```{python}
#| label: fig-utility-prior
#| fig-cap: "Prior distribution of utilities under Dirichlet(1,1) as sampled from m_0_sim.stan. Left: Distribution of the middle utility υ₂. Right: Distribution of utility increments δ."

# Extract unique delta and upsilon values
delta_cols = [col for col in samples.columns if 'sim_delta' in col]
upsilon_cols = [col for col in samples.columns if 'sim_upsilon' in col]

# Get one value per parameter set
param_samples = samples.groupby('param_set').first()

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Middle utility distribution (upsilon[2])
if 'sim_upsilon[2]' in param_samples.columns:
    upsilon_2 = param_samples['sim_upsilon[2]'].values
    axes[0].hist(upsilon_2, bins=40, density=True, alpha=0.7, color='forestgreen', edgecolor='white')
    axes[0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label=f'Mean ≈ 0.5')
    axes[0].set_xlabel('Middle Utility (υ₂)')
    axes[0].set_ylabel('Density')
    axes[0].set_title('Prior Distribution of υ₂')
    axes[0].legend()
    axes[0].set_xlim(0, 1)

# Delta distribution
if delta_cols:
    delta_1 = param_samples[delta_cols[0]].values
    delta_2 = param_samples[delta_cols[1]].values if len(delta_cols) > 1 else 1 - delta_1
    
    axes[1].scatter(delta_1, delta_2, alpha=0.3, s=20, c='forestgreen')
    axes[1].plot([0, 1], [1, 0], 'r--', linewidth=2, label='δ₁ + δ₂ = 1')
    axes[1].set_xlabel('δ₁ (first increment)')
    axes[1].set_ylabel('δ₂ (second increment)')
    axes[1].set_title('Prior Distribution on Utility Simplex')
    axes[1].set_xlim(-0.05, 1.05)
    axes[1].set_ylim(-0.05, 1.05)
    axes[1].legend()
    axes[1].set_aspect('equal')

plt.tight_layout()
plt.show()
```

The symmetric Dirichlet(1,1) prior induces a **uniform distribution** over the simplex of valid utility configurations.

### Feature-to-Probability Weights ($\boldsymbol{\beta}$)

The matrix $\boldsymbol{\beta} \in \mathbb{R}^{K \times D}$ maps alternative features to (unnormalized) log-probabilities over consequences. In `m_0_sim.stan`, each element is drawn independently from $\text{Normal}(0, \sigma_\beta)$:

```{python}
#| label: fig-beta-prior
#| fig-cap: "Prior distribution of the feature-to-probability weights β sampled from m_0_sim.stan. Left: Distribution of all β coefficients. Right: Heatmap of median |β| values showing the structure of a typical draw from the prior."

# Extract beta coefficients
beta_cols = [col for col in samples.columns if 'sim_beta' in col]

# Get unique values per parameter set
param_samples = samples.groupby('param_set').first()

# Collect all beta values
all_betas = []
for col in beta_cols:
    all_betas.extend(param_samples[col].values)
all_betas = np.array(all_betas)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Distribution of all beta coefficients
axes[0].hist(all_betas, bins=50, density=True, alpha=0.7, color='purple', edgecolor='white')
# Overlay theoretical normal
x_range = np.linspace(all_betas.min(), all_betas.max(), 100)
axes[0].plot(x_range, stats.norm.pdf(x_range, 0, 1), 'r-', linewidth=2, label='Normal(0,1)')
axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)
axes[0].set_xlabel('β coefficient value')
axes[0].set_ylabel('Density')
axes[0].set_title('Prior Distribution of β Coefficients')
axes[0].legend()

# Heatmap of absolute beta magnitudes (median across samples)
K, D = config['K'], config['D']
beta_medians = np.zeros((K, D))
for k in range(K):
    for d in range(D):
        col = f'sim_beta[{k+1},{d+1}]'
        if col in param_samples.columns:
            beta_medians[k, d] = np.median(np.abs(param_samples[col].values))

im = axes[1].imshow(beta_medians, cmap='Purples', aspect='auto')
axes[1].set_xlabel('Feature Dimension (d)')
axes[1].set_ylabel('Consequence (k)')
axes[1].set_title('Median |β| by Position')
axes[1].set_xticks(range(D))
axes[1].set_xticklabels([f'{d+1}' for d in range(D)])
axes[1].set_yticks(range(K))
axes[1].set_yticklabels([f'{k+1}' for k in range(K)])
plt.colorbar(im, ax=axes[1], label='Median |β|')

plt.tight_layout()
plt.show()

print(f"\nBeta coefficient summary:")
print(f"  Mean: {np.mean(all_betas):.3f}")
print(f"  Std: {np.std(all_betas):.3f}")
print(f"  95% interval: [{np.percentile(all_betas, 2.5):.2f}, {np.percentile(all_betas, 97.5):.2f}]")
```

The prior on $\boldsymbol{\beta}$ is deliberately uninformative—it does not favor any particular feature-consequence relationship. This allows the data to determine how features predict consequences.

### Subjective Probabilities Over Consequences ($\boldsymbol{\psi}$)

The weights $\boldsymbol{\beta}$ interact with alternative features to produce **subjective probabilities** over consequences via the softmax transformation:
$$
\psi_{rk} = \frac{\exp(\boldsymbol{\beta}_k^\top \mathbf{x}_r)}{\sum_{k'=1}^K \exp(\boldsymbol{\beta}_{k'}^\top \mathbf{x}_r)}
$$

This is computed in `m_0_sim.stan` as `psi[i] = softmax(beta * x[i])`.

```{python}
#| label: fig-psi-prior
#| fig-cap: "Prior distribution of subjective probabilities over consequences. Left: Distribution of all ψ values. Right: Ternary plot showing the distribution on the 3-simplex for a single alternative."

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Collect psi values - we need to compute them from beta and x
# Since psi isn't directly stored in samples, we'll visualize what the prior implies

# For visualization, we simulate some psi values directly
# Using the beta samples and feature matrix
np.random.seed(42)

# Get a sample of beta matrices and compute psi for first few alternatives
n_viz_samples = 500
K, D = config['K'], config['D']
R = config['R']

# Simulate beta matrices
beta_samples_viz = np.random.normal(0, 1, size=(n_viz_samples, K, D))

# Use the feature matrix from our study design
w_array = np.array(study.w)  # Shape: (R, D)

# Compute psi for each alternative under each beta sample
all_psi = []
for beta in beta_samples_viz:
    for r in range(R):
        logits = beta @ w_array[r]  # K-dimensional
        psi = np.exp(logits) / np.sum(np.exp(logits))  # softmax
        all_psi.append(psi)

all_psi = np.array(all_psi)

# Left: Distribution of all psi values
axes[0].hist(all_psi.flatten(), bins=50, density=True, alpha=0.7, 
             color='darkorange', edgecolor='white')
axes[0].axvline(x=1/K, color='red', linestyle='--', linewidth=2, 
                label=f'Uniform = {1/K:.2f}')
axes[0].set_xlabel('ψ (subjective probability)')
axes[0].set_ylabel('Density')
axes[0].set_title('Prior Distribution of Subjective Probabilities')
axes[0].legend()
axes[0].set_xlim(0, 1)

# Right: Ternary plot for K=3 case (if applicable)
if K == 3:
    # Select probabilities for first alternative only (for clarity)
    psi_alt1 = all_psi[::R]  # Every R-th row = first alternative
    
    # Create ternary coordinates
    # Map (p1, p2, p3) to 2D using standard ternary transformation
    sqrt3_2 = np.sqrt(3) / 2
    x_tern = 0.5 * (2 * psi_alt1[:, 1] + psi_alt1[:, 2])
    y_tern = sqrt3_2 * psi_alt1[:, 2]
    
    # Draw simplex boundary
    axes[1].plot([0, 1, 0.5, 0], [0, 0, sqrt3_2, 0], 'k-', linewidth=2)
    
    # Plot points
    axes[1].scatter(x_tern, y_tern, alpha=0.15, s=15, c='darkorange')
    
    # Mark center (uniform)
    axes[1].scatter([0.5], [sqrt3_2/3], c='red', s=100, marker='*', 
                    zorder=5, label='Uniform (1/3, 1/3, 1/3)')
    
    # Label vertices
    axes[1].text(-0.05, -0.05, 'k=1', fontsize=11, ha='center')
    axes[1].text(1.05, -0.05, 'k=2', fontsize=11, ha='center')
    axes[1].text(0.5, sqrt3_2 + 0.05, 'k=3', fontsize=11, ha='center')
    
    axes[1].set_xlim(-0.1, 1.1)
    axes[1].set_ylim(-0.1, sqrt3_2 + 0.15)
    axes[1].set_aspect('equal')
    axes[1].axis('off')
    axes[1].set_title('ψ Distribution on 3-Simplex\n(Alternative 1)')
    axes[1].legend(loc='lower right')
else:
    # For K != 3, show pairwise relationship
    axes[1].scatter(all_psi[:, 0], all_psi[:, 1], alpha=0.1, s=10, c='darkorange')
    axes[1].set_xlabel('ψ₁')
    axes[1].set_ylabel('ψ₂')
    axes[1].set_title('First Two Subjective Probabilities')

plt.tight_layout()
plt.show()

print(f"\nSubjective probability summary:")
print(f"  Mean ψ: {np.mean(all_psi):.3f} (theoretical uniform = {1/K:.3f})")
print(f"  Std ψ: {np.std(all_psi):.3f}")
print(f"  P(ψ < 0.1): {np.mean(all_psi < 0.1):.3f}")
print(f"  P(ψ > 0.9): {np.mean(all_psi > 0.9):.3f}")
```

::: {.callout-important}
## Interpretation of Subjective Probabilities
The vector $\boldsymbol{\psi}_r$ represents the decision-maker's **subjective beliefs** about which consequence will obtain if alternative $r$ is chosen. The Normal(0,1) prior on $\boldsymbol{\beta}$ induces a prior on $\boldsymbol{\psi}$ that:

1. **Centers on uniformity** when features are balanced
2. **Allows extreme beliefs** ($\psi_{rk} \approx 0$ or $\approx 1$) when features strongly predict consequences
3. **Varies across alternatives** based on their feature profiles

This captures the idea that alternatives with different feature profiles may induce quite different probability distributions over consequences.
:::

```{python}
#| label: fig-psi-across-alternatives
#| fig-cap: "Variation in subjective probabilities across alternatives under a single parameter draw. Each row shows how the distribution ψ over K=3 consequences differs based on alternative features."

# Show how psi varies across alternatives for a few beta samples
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.flatten()

# Select 10 diverse parameter draws
np.random.seed(123)
sample_indices = np.random.choice(n_viz_samples, 10, replace=False)

for idx, (ax, sample_idx) in enumerate(zip(axes, sample_indices)):
    beta = beta_samples_viz[sample_idx]
    
    # Compute psi for all R alternatives
    psi_matrix = np.zeros((R, K))
    for r in range(R):
        logits = beta @ w_array[r]
        psi_matrix[r] = np.exp(logits) / np.sum(np.exp(logits))
    
    # Stacked bar chart
    bottom = np.zeros(R)
    colors = plt.cm.Set2(np.linspace(0, 1, K))
    for k in range(K):
        ax.bar(range(R), psi_matrix[:, k], bottom=bottom, color=colors[k], 
               label=f'k={k+1}' if idx == 0 else '', width=0.8)
        bottom += psi_matrix[:, k]
    
    ax.set_ylim(0, 1)
    ax.set_xlabel('Alternative')
    ax.set_ylabel('ψ')
    ax.set_title(f'Sample {idx+1}', fontsize=10)
    ax.set_xticks([0, R//2, R-1])
    ax.set_xticklabels(['1', f'{R//2+1}', f'{R}'])

axes[0].legend(title='Consequence', bbox_to_anchor=(0.5, 1.25), loc='upper left', ncol=K)
plt.suptitle('Subjective Probabilities Across Alternatives (Sample Parameter Draws)', 
             fontsize=13, y=1.02)
plt.tight_layout()
plt.show()
```

The diversity of $\boldsymbol{\psi}$ profiles across alternatives—driven by variation in $\boldsymbol{\beta}$—is what allows the model to capture differences in how decision-makers perceive distinct alternatives.

## Expected Utilities Under the Prior

The expected utilities $\eta_r = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}$ depend on both the subjective probabilities ($\boldsymbol{\psi}$, derived from $\boldsymbol{\beta}$) and the utility vector ($\boldsymbol{\upsilon}$).

```{python}
#| label: fig-expected-utilities
#| fig-cap: "Distribution of expected utilities across a subset of decision problems. Each boxplot shows the distribution of η for alternatives in that problem across prior samples."

# Extract problem_etas columns
eta_cols = [col for col in samples.columns if 'problem_etas' in col]

# Get data for first 10 problems
n_problems_to_show = min(10, config["M"])

fig, axes = plt.subplots(2, 5, figsize=(15, 8))
axes = axes.flatten()

for m in range(n_problems_to_show):
    ax = axes[m]
    
    # Get columns for this problem (1-indexed in Stan)
    problem_cols = [col for col in eta_cols if f'problem_etas[{m+1},' in col]
    
    # Filter out padding values (very negative)
    valid_data = []
    valid_labels = []
    for col in problem_cols:
        data = samples[col].values
        if np.median(data) > -1e9:  # Not padding
            valid_data.append(data)
            alt_idx = col.split(',')[1].rstrip(']')
            valid_labels.append(alt_idx)
    
    if valid_data:
        bp = ax.boxplot(valid_data, patch_artist=True)
        for patch in bp['boxes']:
            patch.set_facecolor('steelblue')
            patch.set_alpha(0.7)
        ax.set_xticklabels(valid_labels)
        ax.set_title(f'Problem {m+1}')
        ax.set_xlabel('Alternative')
        ax.set_ylabel('η')
        ax.set_ylim(0, 1)

plt.suptitle('Prior Distribution of Expected Utilities by Problem', fontsize=14)
plt.tight_layout()
plt.show()
```

## SEU Maximizer Selection

A key diagnostic is the **probability of selecting the SEU-maximizing alternative** under the prior. The simulation model `m_0_sim.stan` tracks this directly via the `selected_seu_max` variable.

### By Decision Problem

```{python}
#| label: fig-seu-max-by-problem
#| fig-cap: "Probability of selecting the SEU-maximizing alternative for each decision problem, computed from m_0_sim.stan samples. The coral bars show the random choice baseline (1/N for each problem)."

# Extract SEU maximizer selection for each problem
seu_max_cols = [col for col in samples.columns if 'selected_seu_max' in col and 'total' not in col]

M = config["M"]
prob_seu_max = np.zeros(M)

for m in range(M):
    col = f'selected_seu_max[{m+1}]'
    if col in samples.columns:
        prob_seu_max[m] = samples[col].mean()

# Random baseline
random_baseline = 1.0 / N

fig, ax = plt.subplots(figsize=(12, 5))

x = np.arange(1, M+1)
width = 0.35

bars1 = ax.bar(x - width/2, prob_seu_max, width, label='Prior Predictive', 
               color='steelblue', alpha=0.7)
bars2 = ax.bar(x + width/2, random_baseline, width, label='Random Choice Baseline', 
               color='coral', alpha=0.7)

ax.set_xlabel('Decision Problem')
ax.set_ylabel('P(SEU Maximizer Selected)')
ax.set_title('Probability of Selecting SEU Maximizer by Problem')
ax.set_xticks(x)
ax.legend()
ax.set_ylim(0, 1)

plt.tight_layout()
plt.show()

print(f"\nSummary:")
print(f"  Mean P(SEU max): {prob_seu_max.mean():.3f}")
print(f"  Mean random baseline: {random_baseline.mean():.3f}")
print(f"  Ratio (observed/random): {prob_seu_max.mean() / random_baseline.mean():.2f}x")
```

### Total SEU Maximizers Selected

```{python}
#| label: fig-total-seu-max
#| fig-cap: "Distribution of the total number of SEU maximizers selected (out of 25 problems) across prior samples. This is computed directly by m_0_sim.stan."

# Get total SEU max selected
if 'total_seu_max_selected' in samples.columns:
    total_seu_max = samples['total_seu_max_selected'].values
else:
    # Compute from individual columns
    total_seu_max = np.zeros(len(samples))
    for m in range(M):
        col = f'selected_seu_max[{m+1}]'
        if col in samples.columns:
            total_seu_max += samples[col].values

fig, ax = plt.subplots(figsize=(10, 5))

ax.hist(total_seu_max, bins=range(0, M+2), density=True, alpha=0.7, 
        color='steelblue', edgecolor='white', align='left')

# Expected under random choice
expected_random = sum(1/n for n in N)
ax.axvline(x=expected_random, color='coral', linestyle='--', linewidth=2, 
           label=f'Expected (random): {expected_random:.1f}')
ax.axvline(x=total_seu_max.mean(), color='red', linestyle='-', linewidth=2,
           label=f'Prior mean: {total_seu_max.mean():.1f}')

ax.set_xlabel(f'Number of SEU Maximizers Selected (out of {M})')
ax.set_ylabel('Density')
ax.set_title('Prior Predictive Distribution of Total SEU Maximizer Selections')
ax.legend()

plt.tight_layout()
plt.show()

print(f"\nTotal SEU Maximizer Selection Summary:")
print(f"  Mean: {total_seu_max.mean():.1f}")
print(f"  Std: {total_seu_max.std():.1f}")
print(f"  Min: {total_seu_max.min():.0f}")
print(f"  Max: {total_seu_max.max():.0f}")
print(f"  Expected under random: {expected_random:.1f}")
```

### Relationship with $\alpha$

```{python}
#| label: fig-seu-max-vs-alpha
#| fig-cap: "Relationship between sensitivity α and the proportion of SEU maximizers selected. Higher α leads to more consistent SEU maximization, as predicted by the theoretical properties from Report 1."

# Get alpha and total SEU max for each sample
alpha_per_sample = samples['sim_alpha'].values
seu_max_per_sample = total_seu_max / M  # Proportion

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Scatter plot
axes[0].scatter(alpha_per_sample, seu_max_per_sample, alpha=0.2, s=15, c='steelblue')
axes[0].set_xlabel('Sensitivity (α)')
axes[0].set_ylabel('Proportion SEU Max Selected')
axes[0].set_title('SEU Maximizer Selection vs. Sensitivity')
axes[0].set_xlim(0, min(15, np.percentile(alpha_per_sample, 99)))

# Binned averages
alpha_bins = [0, 0.25, 0.5, 1, 2, 3, 5, 10, 100]
bin_centers = []
prop_means = []
prop_stds = []

for i in range(len(alpha_bins) - 1):
    mask = (alpha_per_sample >= alpha_bins[i]) & (alpha_per_sample < alpha_bins[i+1])
    if mask.sum() > 5:
        bin_centers.append((alpha_bins[i] + alpha_bins[i+1])/2)
        prop_means.append(seu_max_per_sample[mask].mean())
        prop_stds.append(seu_max_per_sample[mask].std())

axes[1].errorbar(bin_centers, prop_means, yerr=prop_stds, fmt='o-', 
                 color='steelblue', linewidth=2, markersize=8, capsize=5)
axes[1].axhline(y=1/np.mean(N), color='coral', linestyle='--', 
                label=f'Random baseline: {1/np.mean(N):.2f}')
axes[1].set_xlabel('Sensitivity (α)')
axes[1].set_ylabel('Mean Proportion SEU Max Selected')
axes[1].set_title('SEU Maximizer Selection vs. α (Binned)')
axes[1].set_xscale('log')
axes[1].legend()
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()
```

This plot empirically confirms **Property 1 (Monotonicity)** from Report 1: as $\alpha$ increases, the probability of selecting SEU-maximizing alternatives increases.

## Choice Probability Analysis

```{python}
#| label: fig-choice-prob-distribution
#| fig-cap: "Distribution of choice probabilities under the prior. Left: Distribution of the maximum choice probability in each problem (how 'decisive' choices are). Right: Histogram of all non-zero choice probabilities."

# Extract choice probability columns
chi_cols = [col for col in samples.columns if 'choice_probabilities' in col]

# Compute max choice prob for each problem
max_probs = []
all_probs = []

for col in chi_cols:
    probs = samples[col].values
    # Filter out padding zeros
    valid_probs = probs[probs > 1e-10]
    all_probs.extend(valid_probs)

# For max probs, group by problem
for m in range(M):
    problem_cols = [col for col in chi_cols if f'choice_probabilities[{m+1},' in col]
    if problem_cols:
        problem_probs = np.column_stack([samples[col].values for col in problem_cols])
        # Max across alternatives for each sample
        max_per_sample = np.max(problem_probs, axis=1)
        max_probs.extend(max_per_sample)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Max probability distribution
if max_probs:
    axes[0].hist(max_probs, bins=50, density=True, alpha=0.7, 
                 color='steelblue', edgecolor='white')
    axes[0].set_xlabel('Maximum Choice Probability')
    axes[0].set_ylabel('Density')
    axes[0].set_title('Distribution of Max Choice Probability per Problem')
    axes[0].set_xlim(0, 1)

# All choice probabilities
if all_probs:
    axes[1].hist(all_probs, bins=50, density=True, alpha=0.7, 
                 color='coral', edgecolor='white')
    axes[1].set_xlabel('Choice Probability')
    axes[1].set_ylabel('Density')
    axes[1].set_title('Distribution of All Choice Probabilities')
    axes[1].set_xlim(0, 1)

plt.tight_layout()
plt.show()

if max_probs:
    print(f"Max choice probability: mean = {np.mean(max_probs):.2f}, std = {np.std(max_probs):.2f}")
```

## Prior Predictive Checks: Are the Priors Reasonable?

Based on the prior predictive analysis using `m_0_sim.stan`, we can assess whether the default priors produce sensible behaviors:

### ✓ The Prior Covers the Full Range of Behaviors

```{python}
#| label: behavior-range-check

# Summarize the range of behaviors
prop_seu_max_by_sample = total_seu_max / M

print("Proportion of SEU Maximizers Selected (across prior samples):")
print(f"  Minimum: {prop_seu_max_by_sample.min():.2%}")
print(f"  5th percentile: {np.percentile(prop_seu_max_by_sample, 5):.2%}")
print(f"  Median: {np.median(prop_seu_max_by_sample):.2%}")
print(f"  95th percentile: {np.percentile(prop_seu_max_by_sample, 95):.2%}")
print(f"  Maximum: {prop_seu_max_by_sample.max():.2%}")
```

The prior places substantial probability on:
- **Near-random choice** (~20-30% SEU max selection)
- **Moderate SEU-sensitivity** (~40-60% SEU max selection)
- **Strong SEU-maximization** (~80-100% SEU max selection)

### ✓ No Pathological Behaviors

The prior does not generate:
- Negative expected utilities (impossible given $\upsilon \in [0,1]$)
- Degenerate choice probabilities (softmax always assigns positive probability)
- Systematic biases toward particular alternatives (symmetric prior on $\boldsymbol{\beta}$)

### ✓ Alignment with Theoretical Properties

The simulations confirm the three properties from Report 1:
1. **Monotonicity**: Higher $\alpha$ → higher P(SEU max selection) ✓
2. **Perfect rationality limit**: Very high $\alpha$ → near-deterministic SEU max selection ✓
3. **Random choice limit**: Very low $\alpha$ → near-uniform choice ✓

## Summary

The prior predictive analysis using `m_0_sim.stan` reveals that the default priors for model m_0 produce a sensible range of choice behaviors:

| Aspect | Finding |
|--------|---------|
| Sensitivity ($\alpha$) | Lognormal(0,1) covers low, moderate, and high sensitivity |
| SEU max selection | Prior mean ~50%, ranging from ~20% to ~100% |
| Choice probabilities | Neither too concentrated nor too diffuse |
| Theoretical alignment | All three fundamental properties confirmed empirically |

The prior is **weakly informative**—it does not strongly constrain parameters but rules out implausible behaviors. This is appropriate for a default prior when we have limited domain knowledge about the specific decision context.

::: {.callout-note}
## Design Considerations
The study design used here (M=25, K=3, D=5, R=15) is moderately complex. Key features:

- **25 decision problems**: Provides reasonable statistical power
- **5 feature dimensions**: Allows rich feature-to-probability mapping
- **15 alternatives**: Creates varied choice sets across problems
- **2-5 alternatives per problem**: Realistic range for typical experiments

The [next report](04_parameter_recovery.qmd) examines whether this design supports accurate parameter recovery.
:::

```{python}
#| label: cleanup
#| include: false

# Clean up temporary directory
import shutil
try:
    shutil.rmtree(output_dir)
except:
    pass
```

## References

::: {#refs}
:::
