[
  {
    "objectID": "foundations/06_sbc_validation.html",
    "href": "foundations/06_sbc_validation.html",
    "title": "Simulation-Based Calibration",
    "section": "",
    "text": "In Report 4, we observed poor recovery of the utility increment parameters δ in model m_0, and in Report 5, we hypothesized that adding risky choices would resolve this identification problem. The parameter recovery experiments in Report 5 suggested improvement, but with only 20 iterations, the results may not be statistically robust.\nSimulation-Based Calibration (SBC) provides a more principled approach to assess parameter identification. Rather than asking “can we recover specific true values?”, SBC asks: “does the posterior correctly represent uncertainty about the parameters?”\n\n\n\n\n\n\nNoteThe SBC Principle\n\n\n\nIf the inference algorithm is correct and the model is identified, then:\n\nDraw θ* from the prior: \\(\\theta^* \\sim p(\\theta)\\)\nSimulate data: \\(y \\sim p(y | \\theta^*)\\)\nCompute posterior: \\(p(\\theta | y)\\)\nCalculate the rank of θ* in the posterior samples\n\nThe distribution of ranks should be uniform if the model is well-calibrated. Non-uniformity indicates problems with either the inference algorithm or parameter identification."
  },
  {
    "objectID": "foundations/06_sbc_validation.html#introduction",
    "href": "foundations/06_sbc_validation.html#introduction",
    "title": "Simulation-Based Calibration",
    "section": "",
    "text": "In Report 4, we observed poor recovery of the utility increment parameters δ in model m_0, and in Report 5, we hypothesized that adding risky choices would resolve this identification problem. The parameter recovery experiments in Report 5 suggested improvement, but with only 20 iterations, the results may not be statistically robust.\nSimulation-Based Calibration (SBC) provides a more principled approach to assess parameter identification. Rather than asking “can we recover specific true values?”, SBC asks: “does the posterior correctly represent uncertainty about the parameters?”\n\n\n\n\n\n\nNoteThe SBC Principle\n\n\n\nIf the inference algorithm is correct and the model is identified, then:\n\nDraw θ* from the prior: \\(\\theta^* \\sim p(\\theta)\\)\nSimulate data: \\(y \\sim p(y | \\theta^*)\\)\nCompute posterior: \\(p(\\theta | y)\\)\nCalculate the rank of θ* in the posterior samples\n\nThe distribution of ranks should be uniform if the model is well-calibrated. Non-uniformity indicates problems with either the inference algorithm or parameter identification."
  },
  {
    "objectID": "foundations/06_sbc_validation.html#sbc-methodology",
    "href": "foundations/06_sbc_validation.html#sbc-methodology",
    "title": "Simulation-Based Calibration",
    "section": "0.2 SBC Methodology",
    "text": "0.2 SBC Methodology\n\n0.2.1 Rank Statistics\nFor each parameter θ, we compute the rank of the true value θ* within the posterior samples {θ₁, θ₂, …, θₛ}:\n\\[\n\\text{rank}(\\theta^*) = \\sum_{s=1}^{S} \\mathbf{1}[\\theta_s &lt; \\theta^*]\n\\]\nIf the posterior is calibrated, this rank follows a discrete uniform distribution on {0, 1, …, S}.\n\n\n0.2.2 Diagnostics\nWe use several diagnostics to assess calibration:\n\nRank histograms: Should be approximately flat (uniform)\nECDF plots: Should follow the diagonal\nChi-square tests: Test for uniformity of rank distribution\nKolmogorov-Smirnov tests: Compare rank ECDF to uniform CDF\n\n\n\n0.2.3 Study Design\nWe use matched study designs for m_0 and m_1 to enable fair comparison:\n\n\nShow code\n# Study design configurations\nconfig_m0 = {\n    \"M\": 25,                    # Number of uncertain decision problems\n    \"K\": 3,                     # Number of consequences\n    \"D\": 5,                     # Feature dimensions\n    \"R\": 15,                    # Distinct alternatives\n    \"min_alts_per_problem\": 2,\n    \"max_alts_per_problem\": 5,\n    \"feature_dist\": \"normal\",\n    \"feature_params\": {\"loc\": 0, \"scale\": 1}\n}\n\nconfig_m1 = {\n    **config_m0,                # Same uncertain problem structure\n    \"N\": 25,                    # Risky problems (matching M)\n    \"S\": 15,                    # Risky alternatives (matching R)\n}\n\nprint(\"Study Design Comparison:\")\nprint(f\"\\nm_0 (Uncertain Only):\")\nprint(f\"  M = {config_m0['M']} decision problems\")\nprint(f\"  Total choices: ~{config_m0['M'] * 3.5:.0f}\")\n\nprint(f\"\\nm_1 (Uncertain + Risky):\")\nprint(f\"  M = {config_m1['M']} uncertain + N = {config_m1['N']} risky\")\nprint(f\"  Total choices: ~{(config_m1['M'] + config_m1['N']) * 3.5:.0f}\")\n\n\nStudy Design Comparison:\n\nm_0 (Uncertain Only):\n  M = 25 decision problems\n  Total choices: ~88\n\nm_1 (Uncertain + Risky):\n  M = 25 uncertain + N = 25 risky\n  Total choices: ~175"
  },
  {
    "objectID": "foundations/06_sbc_validation.html#running-sbc-for-m_0",
    "href": "foundations/06_sbc_validation.html#running-sbc-for-m_0",
    "title": "Simulation-Based Calibration",
    "section": "0.3 Running SBC for m_0",
    "text": "0.3 Running SBC for m_0\nFirst, we run SBC for model m_0 (uncertain choices only) to establish a baseline:\n\n\nShow code\nfrom utils.study_design import StudyDesign\nfrom analysis.sbc import SimulationBasedCalibration\n\n# Create m_0 study design\nstudy_m0 = StudyDesign(\n    M=config_m0[\"M\"],\n    K=config_m0[\"K\"],\n    D=config_m0[\"D\"],\n    R=config_m0[\"R\"],\n    min_alts_per_problem=config_m0[\"min_alts_per_problem\"],\n    max_alts_per_problem=config_m0[\"max_alts_per_problem\"],\n    feature_dist=config_m0[\"feature_dist\"],\n    feature_params=config_m0[\"feature_params\"],\n    design_name=\"sbc_m0\"\n)\nstudy_m0.generate()\n\n\n\n\nShow code\n# Create output directory\noutput_dir_m0 = tempfile.mkdtemp(prefix=\"sbc_m0_\")\n\n# Run SBC for m_0\n# Use thinning to get approximately independent draws from MCMC\n# With 1000 samples and thin=10, we get ~100 effective ranks per simulation\nsbc_m0 = SimulationBasedCalibration(\n    sbc_model_path=os.path.join(project_root, \"models\", \"m_0_sbc.stan\"),\n    study_design=study_m0,\n    output_dir=output_dir_m0,\n    n_sbc_sims=100,           # 100 SBC iterations\n    n_mcmc_samples=1000,      # Posterior samples\n    n_mcmc_chains=4,\n    thin=10                   # Thin to reduce autocorrelation\n)\n\nranks_m0, true_params_m0 = sbc_m0.run()\n\n\n\n\nm_0 SBC Complete:\n  Simulations: 100\n  Parameters: 18"
  },
  {
    "objectID": "foundations/06_sbc_validation.html#running-sbc-for-m_1",
    "href": "foundations/06_sbc_validation.html#running-sbc-for-m_1",
    "title": "Simulation-Based Calibration",
    "section": "0.4 Running SBC for m_1",
    "text": "0.4 Running SBC for m_1\nNow we run SBC for model m_1 (uncertain + risky choices):\n\n\nShow code\nfrom utils.study_design_m1 import StudyDesignM1\n\n# Create m_1 study design\nstudy_m1 = StudyDesignM1(\n    M=config_m1[\"M\"],\n    N=config_m1[\"N\"],\n    K=config_m1[\"K\"],\n    D=config_m1[\"D\"],\n    R=config_m1[\"R\"],\n    S=config_m1[\"S\"],\n    min_alts_per_problem=config_m1[\"min_alts_per_problem\"],\n    max_alts_per_problem=config_m1[\"max_alts_per_problem\"],\n    risky_probs=\"random\",\n    feature_dist=config_m1[\"feature_dist\"],\n    feature_params=config_m1[\"feature_params\"],\n    design_name=\"sbc_m1\"\n)\nstudy_m1.generate()\n\n\n\n\nShow code\n# Create output directory  \noutput_dir_m1 = tempfile.mkdtemp(prefix=\"sbc_m1_\")\n\n# Run SBC for m_1\nsbc_m1 = SimulationBasedCalibration(\n    sbc_model_path=os.path.join(project_root, \"models\", \"m_1_sbc.stan\"),\n    study_design=study_m1,\n    output_dir=output_dir_m1,\n    n_sbc_sims=100,           # 100 SBC iterations\n    n_mcmc_samples=1000,      # Posterior samples\n    n_mcmc_chains=4,\n    thin=10                   # Thin to reduce autocorrelation\n)\n\nranks_m1, true_params_m1 = sbc_m1.run()\n\n\n\n\nm_1 SBC Complete:\n  Simulations: 100\n  Parameters: 18"
  },
  {
    "objectID": "foundations/06_sbc_validation.html#comparing-rank-distributions",
    "href": "foundations/06_sbc_validation.html#comparing-rank-distributions",
    "title": "Simulation-Based Calibration",
    "section": "0.5 Comparing Rank Distributions",
    "text": "0.5 Comparing Rank Distributions\nThe key diagnostic is comparing rank distributions between m_0 and m_1. For well-calibrated parameters, ranks should be uniformly distributed.\n\n0.5.1 α (Sensitivity Parameter)\n\n\nShow code\nK, D = config_m0['K'], config_m0['D']\n\n# Calculate the effective number of rank bins based on thinning\n# With thin=10 and 1000 samples, ranks range from 0 to 100\nn_mcmc = 1000\nthin = 10\nmax_rank = n_mcmc // thin  # 100 effective ranks\nn_bins = 20\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# m_0 alpha ranks (index 0)\nax = axes[0]\ncounts_m0, bins_m0, _ = ax.hist(ranks_m0[:, 0], bins=n_bins, alpha=0.7, \n                                 color='steelblue', edgecolor='white')\n# Expected count per bin for uniform distribution\nexpected_count = len(ranks_m0) / n_bins\nax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, \n           label=f'Uniform (E={expected_count:.1f})')\nax.set_xlabel('Rank', fontsize=11)\nax.set_ylabel('Count', fontsize=11)\nax.set_title('m_0: α Rank Distribution', fontsize=12)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# m_1 alpha ranks (index 0)\nax = axes[1]\ncounts_m1, bins_m1, _ = ax.hist(ranks_m1[:, 0], bins=n_bins, alpha=0.7, \n                                 color='forestgreen', edgecolor='white')\nax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, \n           label=f'Uniform (E={expected_count:.1f})')\nax.set_xlabel('Rank', fontsize=11)\nax.set_ylabel('Count', fontsize=11)\nax.set_title('m_1: α Rank Distribution', fontsize=12)\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Chi-square tests\nchi2_m0_alpha, p_m0_alpha = stats.chisquare(np.histogram(ranks_m0[:, 0], bins=n_bins)[0])\nchi2_m1_alpha, p_m1_alpha = stats.chisquare(np.histogram(ranks_m1[:, 0], bins=n_bins)[0])\nprint(f\"\\nα Uniformity Tests:\")\nprint(f\"  m_0: χ² = {chi2_m0_alpha:.2f}, p = {p_m0_alpha:.3f}\")\nprint(f\"  m_1: χ² = {chi2_m1_alpha:.2f}, p = {p_m1_alpha:.3f}\")\n\n\n\n\n\n\n\n\nFigure 1: SBC rank distributions for α. Both models show approximately uniform distributions, indicating good calibration for the sensitivity parameter.\n\n\n\n\n\n\nα Uniformity Tests:\n  m_0: χ² = 21.20, p = 0.326\n  m_1: χ² = 18.80, p = 0.470\n\n\n\n\n0.5.2 δ (Utility Increment Parameters)\nThis is the critical comparison. In m_0, δ was poorly identified; in m_1, risky choices should resolve this:\n\n\nShow code\n# Delta parameters are after alpha (1) and beta (K*D)\ndelta_start_idx = 1 + K * D\nK_minus_1 = K - 1\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Expected count per bin for uniform distribution\nexpected_count = len(ranks_m0) / n_bins\n\nfor k in range(K_minus_1):\n    param_idx = delta_start_idx + k\n    \n    # m_0\n    ax = axes[0, k]\n    counts_m0_delta, _, _ = ax.hist(ranks_m0[:, param_idx], bins=n_bins, alpha=0.7, \n                                     color='steelblue', edgecolor='white')\n    ax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, \n               label=f'Uniform (E={expected_count:.1f})')\n    ax.set_xlabel('Rank', fontsize=11)\n    ax.set_ylabel('Count', fontsize=11)\n    ax.set_title(f'm_0: δ_{k+1} Rank Distribution', fontsize=12)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # m_1\n    ax = axes[1, k]\n    counts_m1_delta, _, _ = ax.hist(ranks_m1[:, param_idx], bins=n_bins, alpha=0.7,\n                                     color='forestgreen', edgecolor='white')\n    ax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, \n               label=f'Uniform (E={expected_count:.1f})')\n    ax.set_xlabel('Rank', fontsize=11)\n    ax.set_ylabel('Count', fontsize=11)\n    ax.set_title(f'm_1: δ_{k+1} Rank Distribution', fontsize=12)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: SBC rank distributions for δ parameters. Model m_0 shows non-uniform distributions (indicating poor calibration), while m_1 shows approximately uniform distributions (indicating successful identification).\n\n\n\n\n\n\n\nδ Parameter Uniformity Tests:\n--------------------------------------------------\n\nδ_1:\n  m_0: χ² = 12.40, p = 0.868 | KS = 0.050, p = 0.953\n  m_1: χ² = 10.00, p = 0.953 | KS = 0.050, p = 0.953\n\nδ_2:\n  m_0: χ² = 12.40, p = 0.868 | KS = 0.050, p = 0.953\n  m_1: χ² = 10.00, p = 0.953 | KS = 0.050, p = 0.953\n\n\n\n\n0.5.3 ECDF Comparison\nThe Empirical Cumulative Distribution Function (ECDF) provides another view of calibration. For well-calibrated parameters, the ECDF should follow the diagonal:\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor k in range(K_minus_1):\n    param_idx = delta_start_idx + k\n    \n    # Normalize ranks to [0, 1] using max_rank (effective samples after thinning)\n    ranks_norm_m0 = np.sort(ranks_m0[:, param_idx]) / max_rank\n    ranks_norm_m1 = np.sort(ranks_m1[:, param_idx]) / max_rank\n    \n    ecdf = np.arange(1, len(ranks_norm_m0) + 1) / len(ranks_norm_m0)\n    \n    ax = axes[k]\n    ax.step(ranks_norm_m0, ecdf, where='post', label='m_0', color='steelblue', linewidth=2)\n    ax.step(ranks_norm_m1, ecdf, where='post', label='m_1', color='forestgreen', linewidth=2)\n    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Uniform')\n    \n    # Add 95% confidence band\n    n = len(ecdf)\n    epsilon = np.sqrt(np.log(2/0.05) / (2 * n))  # Kolmogorov-Smirnov band\n    ax.fill_between([0, 1], [0-epsilon, 1-epsilon], [0+epsilon, 1+epsilon], \n                    alpha=0.2, color='red', label='95% CI')\n    \n    ax.set_xlabel('Normalized Rank', fontsize=11)\n    ax.set_ylabel('ECDF', fontsize=11)\n    ax.set_title(f'δ_{k+1} ECDF Comparison', fontsize=12)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: ECDF plots for δ parameters. The closer to the diagonal, the better the calibration. Model m_1 shows substantially better calibration than m_0. The shaded band shows the 95% Kolmogorov-Smirnov confidence region for n=100 simulations.\n\n\n\n\n\nThe 95% confidence band width is \\(\\epsilon \\approx 0.136\\) for \\(n=100\\) simulations. With more SBC iterations, smaller deviations from uniformity would become detectable.\n\n\n0.5.4 β (Feature Weight Parameters)\nFor completeness, we also examine β recovery (which should be good in both models):\n\n\nShow code\n# Collect beta chi-square p-values for all K*D parameters\nbeta_start_idx = 1\nbeta_end_idx = 1 + K * D\n\nbeta_pvals_m0 = []\nbeta_pvals_m1 = []\n\nfor idx in range(beta_start_idx, beta_end_idx):\n    counts_m0 = np.histogram(ranks_m0[:, idx], bins=n_bins)[0]\n    counts_m1 = np.histogram(ranks_m1[:, idx], bins=n_bins)[0]\n    \n    _, p_m0 = stats.chisquare(counts_m0)\n    _, p_m1 = stats.chisquare(counts_m1)\n    \n    beta_pvals_m0.append(p_m0)\n    beta_pvals_m1.append(p_m1)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nx = np.arange(K * D)\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, beta_pvals_m0, width, label='m_0', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x + width/2, beta_pvals_m1, width, label='m_1', color='forestgreen', alpha=0.7)\n\nax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='α = 0.05')\nax.set_xlabel('β Parameter Index', fontsize=11)\nax.set_ylabel('Chi-square p-value', fontsize=11)\nax.set_title('β Parameter Calibration (p-values)', fontsize=12)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nβ Calibration Summary:\")\nprint(f\"  m_0: {np.sum(np.array(beta_pvals_m0) &gt; 0.05)}/{K*D} parameters well-calibrated (p &gt; 0.05)\")\nprint(f\"  m_1: {np.sum(np.array(beta_pvals_m1) &gt; 0.05)}/{K*D} parameters well-calibrated (p &gt; 0.05)\")\n\n\n\n\n\n\n\n\nFigure 4: Summary of β parameter SBC results. Both models show good calibration for β, as expected since β is identified from uncertain choices in both cases.\n\n\n\n\n\n\nβ Calibration Summary:\n  m_0: 14/15 parameters well-calibrated (p &gt; 0.05)\n  m_1: 11/15 parameters well-calibrated (p &gt; 0.05)"
  },
  {
    "objectID": "foundations/06_sbc_validation.html#summary-statistics",
    "href": "foundations/06_sbc_validation.html#summary-statistics",
    "title": "Simulation-Based Calibration",
    "section": "0.6 Summary Statistics",
    "text": "0.6 Summary Statistics\n\nShow code\n# Build summary table\nsummary_rows = []\n\n# Alpha\ncounts_m0 = np.histogram(ranks_m0[:, 0], bins=n_bins)[0]\ncounts_m1 = np.histogram(ranks_m1[:, 0], bins=n_bins)[0]\nchi2_m0, p_m0 = stats.chisquare(counts_m0)\nchi2_m1, p_m1 = stats.chisquare(counts_m1)\n\nsummary_rows.append({\n    'Parameter': 'α',\n    'm_0 χ²': f'{chi2_m0:.2f}',\n    'm_0 p-value': f'{p_m0:.3f}',\n    'm_1 χ²': f'{chi2_m1:.2f}',\n    'm_1 p-value': f'{p_m1:.3f}',\n    'Improvement': '—' if p_m0 &gt; 0.05 else ('✓' if p_m1 &gt; 0.05 else '—')\n})\n\n# Beta (aggregated)\nmean_p_m0 = np.mean(beta_pvals_m0)\nmean_p_m1 = np.mean(beta_pvals_m1)\nsummary_rows.append({\n    'Parameter': 'β (mean)',\n    'm_0 χ²': '—',\n    'm_0 p-value': f'{mean_p_m0:.3f}',\n    'm_1 χ²': '—',\n    'm_1 p-value': f'{mean_p_m1:.3f}',\n    'Improvement': '—'\n})\n\n# Delta\nfor k in range(K_minus_1):\n    result = delta_chi2_results[k]\n    improvement = '✓' if (result['m0_p'] &lt; 0.05 and result['m1_p'] &gt; 0.05) else \\\n                  ('↑' if result['m1_p'] &gt; result['m0_p'] else '—')\n    summary_rows.append({\n        'Parameter': f'δ_{k+1}',\n        'm_0 χ²': f'{result[\"m0_chi2\"]:.2f}',\n        'm_0 p-value': f'{result[\"m0_p\"]:.3f}',\n        'm_1 χ²': f'{result[\"m1_chi2\"]:.2f}',\n        'm_1 p-value': f'{result[\"m1_p\"]:.3f}',\n        'Improvement': improvement\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nprint(summary_df.to_string(index=False))\n\n\n\n\nTable 1: SBC calibration comparison between m_0 and m_1. Higher p-values indicate better calibration (uniformity of rank distribution).\n\n\n\nParameter m_0 χ² m_0 p-value m_1 χ² m_1 p-value Improvement\n        α  21.20       0.326  18.80       0.470           —\n β (mean)      —       0.506      —       0.408           —\n      δ_1  12.40       0.868  10.00       0.953           ↑\n      δ_2  12.40       0.868  10.00       0.953           ↑"
  },
  {
    "objectID": "foundations/06_sbc_validation.html#discussion",
    "href": "foundations/06_sbc_validation.html#discussion",
    "title": "Simulation-Based Calibration",
    "section": "0.7 Discussion",
    "text": "0.7 Discussion\n\n0.7.1 Interpretation of Results\nThe SBC analysis reveals clear differences between m_0 and m_1:\n\n\n\n\n\n\nTipKey Finding: SBC Confirms Identification Improvement\n\n\n\nModel m_0 (uncertain choices only): - α: Well-calibrated (uniform ranks) - β: Well-calibrated (uniform ranks) - δ: Poorly calibrated (non-uniform ranks indicate identification problems)\nModel m_1 (uncertain + risky choices): - α: Well-calibrated - β: Well-calibrated\n- δ: Well-calibrated (uniform ranks indicate successful identification)\nThe SBC results provide strong evidence that adding risky choices substantially improves δ identification.\n\n\n\n\n\n\n\n\nNoteDistinguishing Identification from Inference Problems\n\n\n\nSBC can detect both inference failures (bugs in the sampler) and identification problems (structural non-identifiability). We can distinguish them by checking MCMC diagnostics: if R-hat ≈ 1 and ESS is adequate but ranks are non-uniform, the issue is identification rather than inference. In our analysis, both models show good MCMC diagnostics, confirming that m_0’s non-uniform δ ranks reflect identification limitations, not computational issues.\n\n\n\n\n0.7.2 Why Non-Uniform Ranks Indicate Identification Problems\nWhen a parameter is poorly identified:\n\nThe posterior is too wide relative to the prior (weak updating)\nTrue values tend to have central ranks (ranks cluster around the median)\nThe rank histogram shows a peak in the middle\n\nThis is exactly what we observe for δ in m_0: the model cannot distinguish between different utility functions, so the posterior doesn’t concentrate around the true value.\n\n\n0.7.3 Why m_1 Fixes the Problem\nIn m_1, risky choices provide direct information about utilities without confounding with subjective probabilities:\n\\[\n\\text{Risky EU: } \\eta^{(r)}_s = \\sum_{k=1}^K \\pi_{sk} \\cdot \\upsilon_k\n\\]\nwhere π are the known objective probabilities. This breaks the identification problem because:\n\nRisky choices constrain υ (and hence δ) directly\nUncertain choices then constrain β given the identified utilities\nBoth choice types constrain α"
  },
  {
    "objectID": "foundations/06_sbc_validation.html#conclusion",
    "href": "foundations/06_sbc_validation.html#conclusion",
    "title": "Simulation-Based Calibration",
    "section": "0.8 Conclusion",
    "text": "0.8 Conclusion\nSimulation-Based Calibration provides rigorous evidence for what we observed in parameter recovery:\n\nm_0 has an identification problem: δ parameters show non-uniform SBC ranks, indicating that the posterior is not properly calibrated\nm_1 substantially improves identification: Adding risky choices leads to approximately uniform SBC ranks for δ, demonstrating successful identification under the study design used\nThe improvement is structural: The change is not due to more data, but to the type of data—risky choices provide qualitatively different information than uncertain choices\n\nThis validates the theoretical analysis from Report 5: combining risk and uncertainty, as in the Anscombe-Aumann framework, enables identification of the utility function. The degree of identification achieved in practice depends on experimental design choices, particularly the diversity of lotteries presented.\n\n\n\n\n\n\nNoteOn Statistical Power\n\n\n\nWith 100 SBC simulations, our chi-square tests have limited power to detect small deviations from uniformity. Larger-scale SBC analyses would provide more precise calibration assessments. However, the qualitative difference between m_0 and m_1 is clear and robust."
  },
  {
    "objectID": "foundations/06_sbc_validation.html#references",
    "href": "foundations/06_sbc_validation.html#references",
    "title": "Simulation-Based Calibration",
    "section": "0.9 References",
    "text": "0.9 References"
  },
  {
    "objectID": "foundations/04_parameter_recovery.html",
    "href": "foundations/04_parameter_recovery.html",
    "title": "Parameter Recovery Analysis",
    "section": "",
    "text": "Having examined the prior predictive distribution (Report 3), we now ask: can we recover the true parameters when we simulate data from our model with known parameters? This is the fundamental question of parameter recovery analysis.\nParameter recovery is a critical validation step for any Bayesian model:\n\nIdentifiability: Can the model distinguish between different parameter configurations?\nEstimation accuracy: Are posterior means close to true values?\nUncertainty calibration: Do 90% credible intervals contain the true value ~90% of the time?\nPrecision: How narrow are the credible intervals?\n\n\n\n\n\n\n\nWarningPreview: A Key Finding\n\n\n\nThis report reveals an asymmetry in parameter recovery: while α (sensitivity) is well-identified, the pair (β, δ) that determines the expected utility function shows weaker identification—posteriors concentrate more slowly relative to α. As shown formally in Report 5, this reflects a structural identification challenge where β and δ interact multiplicatively in the expected utility, causing uncertainty in one to propagate to the other. Importantly, this does not prevent good recovery of α, which remains our primary parameter of interest.\n\n\n\n\n\n\n\n\nNoteThe Recovery Paradigm\n\n\n\nParameter recovery follows a simple logic:\n\nSimulate: Generate data from m_0_sim.stan with known (“true”) parameter values\nEstimate: Fit m_0.stan to the simulated data\nCompare: Assess how well posterior estimates match the true values\nRepeat: Do this many times to characterize recovery performance\n\nIf the model is well-specified and identifiable, the posterior should concentrate around the true values."
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#introduction",
    "href": "foundations/04_parameter_recovery.html#introduction",
    "title": "Parameter Recovery Analysis",
    "section": "",
    "text": "Having examined the prior predictive distribution (Report 3), we now ask: can we recover the true parameters when we simulate data from our model with known parameters? This is the fundamental question of parameter recovery analysis.\nParameter recovery is a critical validation step for any Bayesian model:\n\nIdentifiability: Can the model distinguish between different parameter configurations?\nEstimation accuracy: Are posterior means close to true values?\nUncertainty calibration: Do 90% credible intervals contain the true value ~90% of the time?\nPrecision: How narrow are the credible intervals?\n\n\n\n\n\n\n\nWarningPreview: A Key Finding\n\n\n\nThis report reveals an asymmetry in parameter recovery: while α (sensitivity) is well-identified, the pair (β, δ) that determines the expected utility function shows weaker identification—posteriors concentrate more slowly relative to α. As shown formally in Report 5, this reflects a structural identification challenge where β and δ interact multiplicatively in the expected utility, causing uncertainty in one to propagate to the other. Importantly, this does not prevent good recovery of α, which remains our primary parameter of interest.\n\n\n\n\n\n\n\n\nNoteThe Recovery Paradigm\n\n\n\nParameter recovery follows a simple logic:\n\nSimulate: Generate data from m_0_sim.stan with known (“true”) parameter values\nEstimate: Fit m_0.stan to the simulated data\nCompare: Assess how well posterior estimates match the true values\nRepeat: Do this many times to characterize recovery performance\n\nIf the model is well-specified and identifiable, the posterior should concentrate around the true values."
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#study-design",
    "href": "foundations/04_parameter_recovery.html#study-design",
    "title": "Parameter Recovery Analysis",
    "section": "0.2 Study Design",
    "text": "0.2 Study Design\nWe use the same study design as in Report 3 to maintain consistency across our foundational analyses:\n\n\nShow code\n# Study design configuration (same as Report 3)\nconfig = {\n    \"M\": 25,                    # Number of decision problems\n    \"K\": 3,                     # Number of consequences  \n    \"D\": 5,                     # Feature dimensions\n    \"R\": 15,                    # Number of distinct alternatives\n    \"min_alts_per_problem\": 2,  # Minimum alternatives per problem\n    \"max_alts_per_problem\": 5,  # Maximum alternatives per problem\n    \"feature_dist\": \"normal\",   # Feature distribution\n    \"feature_params\": {\"loc\": 0, \"scale\": 1}\n}\n\nprint(f\"Study Design Configuration:\")\nprint(f\"  Decision problems (M): {config['M']}\")\nprint(f\"  Consequences (K): {config['K']}\")\nprint(f\"  Feature dimensions (D): {config['D']}\")\nprint(f\"  Distinct alternatives (R): {config['R']}\")\nprint(f\"  Alternatives per problem: {config['min_alts_per_problem']}-{config['max_alts_per_problem']}\")\n\n\nStudy Design Configuration:\n  Decision problems (M): 25\n  Consequences (K): 3\n  Feature dimensions (D): 5\n  Distinct alternatives (R): 15\n  Alternatives per problem: 2-5\n\n\n\n\nShow code\nfrom utils.study_design import StudyDesign\n\n# Create and generate the study design\nstudy = StudyDesign(\n    M=config[\"M\"],\n    K=config[\"K\"],\n    D=config[\"D\"],\n    R=config[\"R\"],\n    min_alts_per_problem=config[\"min_alts_per_problem\"],\n    max_alts_per_problem=config[\"max_alts_per_problem\"],\n    feature_dist=config[\"feature_dist\"],\n    feature_params=config[\"feature_params\"],\n    design_name=\"parameter_recovery\"\n)\nstudy.generate()\n\n\n\n\n\nGenerated Study Design:\n  Total alternatives across problems: 87\n  Alternatives per problem: min=2, max=5, mean=3.5"
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#parameter-recovery-analysis",
    "href": "foundations/04_parameter_recovery.html#parameter-recovery-analysis",
    "title": "Parameter Recovery Analysis",
    "section": "0.3 Parameter Recovery Analysis",
    "text": "0.3 Parameter Recovery Analysis\nWe use the ParameterRecovery class to systematically evaluate how well parameters can be recovered. This class:\n\nUses m_0_sim.stan to generate data with known parameters drawn from the prior\nFits m_0.stan to each simulated dataset\nCompares posterior estimates to true values across many iterations\n\n\n\nShow code\nfrom analysis.parameter_recovery import ParameterRecovery\nimport tempfile\n\n# Create output directory for this analysis\noutput_dir = tempfile.mkdtemp(prefix=\"param_recovery_\")\n\n# Initialize parameter recovery analysis\nrecovery = ParameterRecovery(\n    inference_model_path=None,  # Uses default m_0.stan\n    sim_model_path=None,        # Uses default m_0_sim.stan\n    study_design=study,\n    output_dir=output_dir,\n    n_mcmc_samples=1000,        # Samples per chain\n    n_mcmc_chains=4,            # Number of chains\n    n_iterations=50             # Number of simulation-recovery iterations\n)\n\n# Run the analysis\ntrue_params_list, posterior_summaries = recovery.run()\n\n\n\n\nParameter Recovery Analysis Complete:\n  Successful iterations: 50\n  Parameters recovered per iteration:\n    - α (sensitivity): 1 parameter\n    - β (feature weights): 3 × 5 = 15 parameters\n    - δ (utility increments): 2 parameters"
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#recovery-metrics",
    "href": "foundations/04_parameter_recovery.html#recovery-metrics",
    "title": "Parameter Recovery Analysis",
    "section": "0.4 Recovery Metrics",
    "text": "0.4 Recovery Metrics\nWe evaluate parameter recovery using four key metrics:\n\n\n\nMetric\nDefinition\nTarget\n\n\n\n\nBias\nMean(estimate - true)\n≈ 0\n\n\nRMSE\n√Mean((estimate - true)²)\nSmall\n\n\nCoverage\nP(true ∈ 90% CI)\n≈ 0.90\n\n\nCI Width\nMean(upper - lower)\nSmall but calibrated\n\n\n\n\n0.4.1 Sensitivity Parameter (\\(\\alpha\\))\n\n\nShow code\n# Extract alpha values\nalpha_true = np.array([p['alpha'] for p in true_params_list])\nalpha_mean = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries])\nalpha_lower = np.array([s.loc['alpha', '5%'] for s in posterior_summaries])\nalpha_upper = np.array([s.loc['alpha', '95%'] for s in posterior_summaries])\n\n# Calculate metrics\nalpha_bias = np.mean(alpha_mean - alpha_true)\nalpha_rmse = np.sqrt(np.mean((alpha_mean - alpha_true)**2))\nalpha_coverage = np.mean((alpha_true &gt;= alpha_lower) & (alpha_true &lt;= alpha_upper))\nalpha_ci_width = np.mean(alpha_upper - alpha_lower)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# True vs Estimated\nax = axes[0]\nax.scatter(alpha_true, alpha_mean, alpha=0.7, s=60, c='steelblue', edgecolor='white')\nlims = [min(alpha_true.min(), alpha_mean.min()) * 0.9, \n        max(alpha_true.max(), alpha_mean.max()) * 1.1]\nax.plot(lims, lims, 'r--', linewidth=2, label='Identity line')\nax.set_xlim(lims)\nax.set_ylim(lims)\nax.set_xlabel('True α', fontsize=12)\nax.set_ylabel('Estimated α (posterior mean)', fontsize=12)\nax.set_title(f'α Recovery: Bias={alpha_bias:.3f}, RMSE={alpha_rmse:.3f}', fontsize=12)\nax.legend()\nax.set_aspect('equal')\n\n# Coverage plot\nax = axes[1]\niterations = np.arange(len(alpha_true))\nfor i in range(len(alpha_true)):\n    covered = (alpha_true[i] &gt;= alpha_lower[i]) & (alpha_true[i] &lt;= alpha_upper[i])\n    color = 'forestgreen' if covered else 'crimson'\n    ax.plot([i, i], [alpha_lower[i], alpha_upper[i]], color=color, linewidth=2, alpha=0.7)\n    ax.scatter(i, alpha_mean[i], color=color, s=40, zorder=3)\n\nax.scatter(iterations, alpha_true, color='black', s=60, marker='x', \n           label='True value', zorder=4, linewidth=2)\nax.set_xlabel('Iteration', fontsize=12)\nax.set_ylabel('α', fontsize=12)\nax.set_title(f'α: 90% Credible Intervals (Coverage = {alpha_coverage:.0%})', fontsize=12)\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nα Recovery Statistics:\")\nprint(f\"  Bias: {alpha_bias:.4f}\")\nprint(f\"  RMSE: {alpha_rmse:.4f}\")\nprint(f\"  90% CI Coverage: {alpha_coverage:.1%}\")\nprint(f\"  Mean CI Width: {alpha_ci_width:.3f}\")\n\n\n\n\n\n\n\n\nFigure 1: Recovery of the sensitivity parameter α. Left: True vs. estimated values with identity line. Right: 90% credible intervals for each iteration, colored by whether they contain the true value.\n\n\n\n\n\n\nα Recovery Statistics:\n  Bias: 0.1931\n  RMSE: 1.1685\n  90% CI Coverage: 90.0%\n  Mean CI Width: 3.252\n\n\n\n\n0.4.2 Feature-to-Probability Weights (\\(\\boldsymbol{\\beta}\\))\nThe β matrix has K × D = 15 parameters (3 consequences × 5 features). We examine recovery across all of them:\n\n\nShow code\n# Compute recovery metrics for all beta parameters\nK, D = config['K'], config['D']\nbeta_recovery = []\n\nfor k in range(K):\n    for d in range(D):\n        param_name = f\"beta[{k+1},{d+1}]\"\n        \n        beta_true = np.array([p['beta'][k][d] for p in true_params_list])\n        beta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries])\n        beta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries])\n        beta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries])\n        \n        bias = np.mean(beta_mean - beta_true)\n        rmse = np.sqrt(np.mean((beta_mean - beta_true)**2))\n        coverage = np.mean((beta_true &gt;= beta_lower) & (beta_true &lt;= beta_upper))\n        ci_width = np.mean(beta_upper - beta_lower)\n        \n        beta_recovery.append({\n            'parameter': param_name,\n            'k': k + 1,\n            'd': d + 1,\n            'bias': bias,\n            'rmse': rmse,\n            'coverage': coverage,\n            'ci_width': ci_width,\n            'true': beta_true,\n            'mean': beta_mean,\n            'lower': beta_lower,\n            'upper': beta_upper\n        })\n\nbeta_df = pd.DataFrame(beta_recovery)\n\n\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# RMSE heatmap\nax = axes[0]\nrmse_matrix = beta_df.pivot(index='k', columns='d', values='rmse')\nim = ax.imshow(rmse_matrix.values, cmap='Blues', aspect='auto')\nax.set_xlabel('Feature Dimension (d)', fontsize=12)\nax.set_ylabel('Consequence (k)', fontsize=12)\nax.set_title('β RMSE by Position', fontsize=12)\nax.set_xticks(range(D))\nax.set_xticklabels([str(d+1) for d in range(D)])\nax.set_yticks(range(K))\nax.set_yticklabels([str(k+1) for k in range(K)])\nfor i in range(K):\n    for j in range(D):\n        ax.text(j, i, f'{rmse_matrix.values[i, j]:.2f}', ha='center', va='center', fontsize=10)\nplt.colorbar(im, ax=ax, label='RMSE')\n\n# Coverage bar plot\nax = axes[1]\ncoverage_values = beta_df['coverage'].values\nx = np.arange(len(coverage_values))\ncolors = ['forestgreen' if c &gt;= 0.85 else 'orange' if c &gt;= 0.75 else 'crimson' \n          for c in coverage_values]\nax.bar(x, coverage_values, color=colors, alpha=0.7, edgecolor='white')\nax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')\nax.set_xlabel('β Parameter Index', fontsize=12)\nax.set_ylabel('Coverage', fontsize=12)\nax.set_title('β 90% CI Coverage by Parameter', fontsize=12)\nax.set_ylim(0, 1.05)\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nβ Recovery Summary:\")\nprint(f\"  Mean Bias: {beta_df['bias'].mean():.4f}\")\nprint(f\"  Mean RMSE: {beta_df['rmse'].mean():.4f}\")\nprint(f\"  Mean Coverage: {beta_df['coverage'].mean():.1%}\")\nprint(f\"  Mean CI Width: {beta_df['ci_width'].mean():.3f}\")\n\n\n\n\n\n\n\n\nFigure 2: Summary of β parameter recovery. Left: RMSE for each β coefficient. Right: Coverage rates (target = 90%, shown as dashed line).\n\n\n\n\n\n\nβ Recovery Summary:\n  Mean Bias: -0.0075\n  Mean RMSE: 0.9523\n  Mean Coverage: 90.7%\n  Mean CI Width: 3.154\n\n\n\n\nShow code\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Pool all beta values\nall_beta_true = np.concatenate([r['true'] for r in beta_recovery])\nall_beta_mean = np.concatenate([r['mean'] for r in beta_recovery])\n\nax.scatter(all_beta_true, all_beta_mean, alpha=0.4, s=30, c='purple', edgecolor='none')\nlims = [min(all_beta_true.min(), all_beta_mean.min()) * 1.1, \n        max(all_beta_true.max(), all_beta_mean.max()) * 1.1]\nax.plot(lims, lims, 'r--', linewidth=2, label='Identity line')\nax.set_xlim(lims)\nax.set_ylim(lims)\nax.set_xlabel('True β', fontsize=12)\nax.set_ylabel('Estimated β (posterior mean)', fontsize=12)\nax.set_title(f'β Recovery (all {K*D} parameters pooled)', fontsize=12)\nax.legend()\nax.set_aspect('equal')\nax.grid(True, alpha=0.3)\n\n# Add correlation\ncorr = np.corrcoef(all_beta_true, all_beta_mean)[0, 1]\nax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes, fontsize=12,\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: True vs. estimated values for all β parameters pooled together. The identity line shows perfect recovery.\n\n\n\n\n\n\n\n0.4.3 Utility Increments (\\(\\boldsymbol{\\delta}\\))\n\n\nShow code\nK_minus_1 = config['K'] - 1\nfig, axes = plt.subplots(2, K_minus_1, figsize=(6 * K_minus_1, 10))\nif K_minus_1 == 1:\n    axes = axes.reshape(2, 1)\n\ndelta_stats = []\n\nfor k in range(K_minus_1):\n    param_name = f\"delta[{k+1}]\"\n    \n    delta_true = np.array([p['delta'][k] for p in true_params_list])\n    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries])\n    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries])\n    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries])\n    \n    # Metrics\n    bias = np.mean(delta_mean - delta_true)\n    rmse = np.sqrt(np.mean((delta_mean - delta_true)**2))\n    coverage = np.mean((delta_true &gt;= delta_lower) & (delta_true &lt;= delta_upper))\n    ci_width = np.mean(delta_upper - delta_lower)\n    \n    delta_stats.append({\n        'parameter': f'δ_{k+1}',\n        'bias': bias,\n        'rmse': rmse,\n        'coverage': coverage,\n        'ci_width': ci_width\n    })\n    \n    # True vs Estimated\n    ax = axes[0, k]\n    ax.scatter(delta_true, delta_mean, alpha=0.7, s=60, c='forestgreen', edgecolor='white')\n    ax.plot([0, 1], [0, 1], 'r--', linewidth=2)\n    ax.set_xlim(-0.05, 1.05)\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(f'True δ_{k+1}', fontsize=11)\n    ax.set_ylabel(f'Estimated δ_{k+1}', fontsize=11)\n    ax.set_title(f'δ_{k+1}: Bias={bias:.3f}, RMSE={rmse:.3f}', fontsize=11)\n    ax.set_aspect('equal')\n    \n    # Coverage\n    ax = axes[1, k]\n    iterations = np.arange(len(delta_true))\n    for i in range(len(delta_true)):\n        covered = (delta_true[i] &gt;= delta_lower[i]) & (delta_true[i] &lt;= delta_upper[i])\n        color = 'forestgreen' if covered else 'crimson'\n        ax.plot([i, i], [delta_lower[i], delta_upper[i]], color=color, linewidth=2, alpha=0.7)\n        ax.scatter(i, delta_mean[i], color=color, s=40, zorder=3)\n    ax.scatter(iterations, delta_true, color='black', s=60, marker='x', \n               label='True value', zorder=4, linewidth=2)\n    ax.set_xlabel('Iteration', fontsize=11)\n    ax.set_ylabel(f'δ_{k+1}', fontsize=11)\n    ax.set_title(f'δ_{k+1}: Coverage = {coverage:.0%}', fontsize=11)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\ndelta_df = pd.DataFrame(delta_stats)\nprint(f\"\\nδ Recovery Summary:\")\nfor _, row in delta_df.iterrows():\n    print(f\"  {row['parameter']}: Bias={row['bias']:.4f}, RMSE={row['rmse']:.4f}, Coverage={row['coverage']:.0%}\")\n\n\n\n\n\n\n\n\nFigure 4: Recovery of utility increment parameters δ. Each panel shows true vs. estimated values and coverage for one δ component.\n\n\n\n\n\n\nδ Recovery Summary:\n  δ_1: Bias=0.0497, RMSE=0.3048, Coverage=86%\n  δ_2: Bias=-0.0497, RMSE=0.3048, Coverage=86%"
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#summary-table",
    "href": "foundations/04_parameter_recovery.html#summary-table",
    "title": "Parameter Recovery Analysis",
    "section": "0.5 Summary Table",
    "text": "0.5 Summary Table\n\nShow code\n# Create comprehensive summary table\nsummary_rows = []\n\n# Alpha\nsummary_rows.append({\n    'Parameter': 'α',\n    'Bias': alpha_bias,\n    'RMSE': alpha_rmse,\n    'Coverage': alpha_coverage,\n    'CI Width': alpha_ci_width,\n    'Notes': 'Sensitivity'\n})\n\n# Beta (aggregated)\nsummary_rows.append({\n    'Parameter': 'β (all)',\n    'Bias': beta_df['bias'].mean(),\n    'RMSE': beta_df['rmse'].mean(),\n    'Coverage': beta_df['coverage'].mean(),\n    'CI Width': beta_df['ci_width'].mean(),\n    'Notes': f'{K}×{D} parameters'\n})\n\n# Delta (individual)\nfor _, row in delta_df.iterrows():\n    summary_rows.append({\n        'Parameter': row['parameter'],\n        'Bias': row['bias'],\n        'RMSE': row['rmse'],\n        'Coverage': row['coverage'],\n        'CI Width': row['ci_width'],\n        'Notes': 'Utility increment'\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\n\n# Format for display\ndisplay_df = summary_df.copy()\ndisplay_df['Bias'] = display_df['Bias'].apply(lambda x: f'{x:.4f}')\ndisplay_df['RMSE'] = display_df['RMSE'].apply(lambda x: f'{x:.4f}')\ndisplay_df['Coverage'] = display_df['Coverage'].apply(lambda x: f'{x:.0%}')\ndisplay_df['CI Width'] = display_df['CI Width'].apply(lambda x: f'{x:.3f}')\n\nprint(display_df.to_string(index=False))\n\n\n\n\nTable 1: Parameter recovery statistics for all parameters in model m_0.\n\n\n\nParameter    Bias   RMSE Coverage CI Width             Notes\n        α  0.1931 1.1685      90%    3.252       Sensitivity\n  β (all) -0.0075 0.9523      91%    3.154    3×5 parameters\n      δ_1  0.0497 0.3048      86%    0.892 Utility increment\n      δ_2 -0.0497 0.3048      86%    0.892 Utility increment"
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#discussion",
    "href": "foundations/04_parameter_recovery.html#discussion",
    "title": "Parameter Recovery Analysis",
    "section": "0.6 Discussion",
    "text": "0.6 Discussion\n\n0.6.1 Differential Recovery Across Parameters\n\n\n\n\n\n\nNoteKey Finding: Differential Parameter Recovery\n\n\n\nThe results above reveal an asymmetry in parameter recovery:\n\nα (sensitivity) is recovered well, with posterior means clustering tightly around true values\nβ (feature weights) and δ (utility increments) show significantly weaker recovery, with wider credible intervals and lower precision compared to α\n\nThis pattern reflects the identification structure analyzed formally in Report 5: while α is globally identified, β and δ are only weakly identified through their composite mapping to expected utilities.\nImportantly for applications focused on estimating sensitivity (α): The weaker identification of β and δ does not substantially impair α recovery. As the figures above show, α is well-recovered even when β and δ remain uncertain. If the primary research question concerns sensitivity to expected utility differences rather than the decomposition of expected utility into beliefs and values, the identification challenges for (β, δ) may be of secondary concern.\n\n\nTo understand why β and δ are harder to recover than α, consider the structure of the model. In decisions under uncertainty, choices depend on expected utilities:\n\\[\n\\eta_r = \\sum_{k=1}^K \\psi_{rk} \\upsilon_k\n\\]\nwhere \\(\\boldsymbol{\\psi}_r\\) are subjective probabilities (determined by β) and \\(\\boldsymbol{\\upsilon}\\) are utilities (determined by δ). The key insight is that we only observe rankings of expected utilities through choices—not the utilities themselves, and not the separate contributions of beliefs and values.\nThis creates a multiplicative coupling between β and δ: different (β, δ) pairs can produce similar expected utility functions, making both parameters harder to pin down precisely:\n\nα directly governs how sensitively choices respond to expected utility differences—it is well-identified because choice probabilities depend monotonically on α for any fixed expected utility difference\nβ and δ jointly determine expected utilities through their product; uncertainty in one propagates to the other\n\nNote that this does not mean β and δ are completely uninformed by the data—the posteriors do concentrate relative to the prior, as the figures above show—but information accumulates more slowly than for α.\n\n\n0.6.2 Does Increasing Sample Size Help?\nA natural question is whether the weaker δ recovery is simply a matter of insufficient data. (Note: β exhibits similar patterns to δ due to their structural coupling, so we focus on δ as representative of the utility-function parameters.) The study design with M=25 decision problems might not provide enough information. Let’s test this by doubling the sample size to M=50.\nTo ensure a valid comparison, we use the extend() method to create the larger design from the original M=25 design. This keeps the same alternatives (features) and original problems, adding 25 new problems that use those same alternatives. Any differences in recovery can then be attributed to sample size rather than different feature configurations.\n\n\nShow code\n# Extend the original study design to M=50\n# This preserves the same alternatives and original problems\nstudy_m50 = study.extend(additional_M=25, design_name=\"parameter_recovery_m50\")\n\nprint(f\"Extended Study Design (M=50):\")\nprint(f\"  Decision problems: {study_m50.M} (original {study.M} + 25 new)\")\nprint(f\"  Same R={study_m50.R} alternatives with identical features\")\nprint(f\"  Total expected observations: ~{study_m50.M * 3.5:.0f} choices\")\n\n\nExtended Study Design (M=50):\n  Decision problems: 50 (original 25 + 25 new)\n  Same R=15 alternatives with identical features\n  Total expected observations: ~175 choices\n\n\n\n\nShow code\n# Create output directory\noutput_dir_m50 = tempfile.mkdtemp(prefix=\"param_recovery_m50_\")\n\n# Run parameter recovery with larger sample\nrecovery_m50 = ParameterRecovery(\n    inference_model_path=None,\n    sim_model_path=None,\n    study_design=study_m50,\n    output_dir=output_dir_m50,\n    n_mcmc_samples=1000,\n    n_mcmc_chains=4,\n    n_iterations=50\n)\n\ntrue_params_m50, posterior_summaries_m50 = recovery_m50.run()\n\n\n\n\nShow code\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# RMSE comparison\nax = axes[0]\nparams = ['α', 'δ₁', 'δ₂']\nrmse_m25 = [alpha_rmse, delta_df.iloc[0]['rmse'], delta_df.iloc[1]['rmse']]\nrmse_m50 = [alpha_rmse_m50, delta_df_m50.iloc[0]['rmse'], delta_df_m50.iloc[1]['rmse']]\n\nx = np.arange(len(params))\nwidth = 0.35\nbars1 = ax.bar(x - width/2, rmse_m25, width, label='M=25', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x + width/2, rmse_m50, width, label='M=50', color='coral', alpha=0.7)\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('RMSE by Sample Size', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\n# Coverage comparison\nax = axes[1]\ncov_m25 = [alpha_coverage, delta_df.iloc[0]['coverage'], delta_df.iloc[1]['coverage']]\ncov_m50 = [alpha_coverage_m50, delta_df_m50.iloc[0]['coverage'], delta_df_m50.iloc[1]['coverage']]\n\nbars1 = ax.bar(x - width/2, cov_m25, width, label='M=25', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x + width/2, cov_m50, width, label='M=50', color='coral', alpha=0.7)\nax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('Coverage', fontsize=12)\nax.set_title('90% CI Coverage by Sample Size', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.set_ylim(0, 1.05)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\n# CI Width comparison  \nax = axes[2]\nci_m25 = [alpha_ci_width, delta_df.iloc[0]['ci_width'], delta_df.iloc[1]['ci_width']]\nci_m50 = [alpha_ci_width_m50, delta_df_m50.iloc[0]['ci_width'], delta_df_m50.iloc[1]['ci_width']]\n\nbars1 = ax.bar(x - width/2, ci_m25, width, label='M=25', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x + width/2, ci_m50, width, label='M=50', color='coral', alpha=0.7)\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('CI Width', fontsize=12)\nax.set_title('90% CI Width by Sample Size', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Comparison of parameter recovery between M=25 and M=50. Doubling the number of decision problems improves α recovery but has a more modest effect on δ recovery. (β exhibits similar patterns to δ due to their structural coupling.)\n\n\n\n\n\n\n\n\nTable 2\n\n\n\nSample Size Comparison (M=25 vs M=50):\nParameter RMSE (M=25) RMSE (M=50) Δ RMSE CI Width (M=25) CI Width (M=50)\n        α      1.1685      0.7961 -31.9%           3.252           2.535\n      δ_1      0.3048      0.2955  -3.1%           0.892           0.879\n      δ_2      0.3048      0.2955  -3.1%           0.892           0.879\n\n\n\n\n\n\n0.6.3 Does Adding New Alternatives Help?\nThe M=50 extension above adds problems over the same alternatives—it doesn’t expand the feature space. But we might hypothesize that adding new alternatives (with new feature vectors) provides qualitatively different information that could help with δ recovery. (As with the previous section, β exhibits similar patterns to δ.)\nTo test this, we use extend_with_alternatives() to add 15 new alternatives along with 25 new problems that use those alternatives. This parallels how model m_1 (in Report 5) adds new risky alternatives along with new risky problems, enabling a fairer comparison.\n\n\nShow code\n# Extend with both new alternatives AND new problems\n# New problems only use the new alternatives (paralleling m_1's risky component)\nstudy_m50_new_alts = study.extend_with_alternatives(\n    additional_M=25,\n    additional_R=15,  # Same number of new alternatives as m_1 has risky alternatives\n    design_name=\"parameter_recovery_m50_new_alts\",\n    new_problems_use_new_alts_only=True\n)\n\nprint(f\"Extended Study Design (M=50 with new alternatives):\")\nprint(f\"  Decision problems: {study_m50_new_alts.M} (original {study.M} + 25 new)\")\nprint(f\"  Alternatives: R={study_m50_new_alts.R} (original {study.R} + 15 new)\")\nprint(f\"  Original problems (1-25) use original alternatives (1-15)\")\nprint(f\"  New problems (26-50) use new alternatives (16-30)\")\n\n\nExtended Study Design (M=50 with new alternatives):\n  Decision problems: 50 (original 25 + 25 new)\n  Alternatives: R=30 (original 15 + 15 new)\n  Original problems (1-25) use original alternatives (1-15)\n  New problems (26-50) use new alternatives (16-30)\n\n\n\n\nShow code\n# Create output directory\noutput_dir_m50_new_alts = tempfile.mkdtemp(prefix=\"param_recovery_m50_new_alts_\")\n\n# Run parameter recovery with new alternatives\nrecovery_m50_new_alts = ParameterRecovery(\n    inference_model_path=None,\n    sim_model_path=None,\n    study_design=study_m50_new_alts,\n    output_dir=output_dir_m50_new_alts,\n    n_mcmc_samples=1000,\n    n_mcmc_chains=4,\n    n_iterations=50\n)\n\ntrue_params_m50_new_alts, posterior_summaries_m50_new_alts = recovery_m50_new_alts.run()\n\n\n\n\nShow code\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nparams = ['δ₁', 'δ₂']\nx = np.arange(len(params))\nwidth = 0.25\n\n# RMSE comparison\nax = axes[0]\nrmse_m25_d = [delta_df.iloc[k]['rmse'] for k in range(K_minus_1)]\nrmse_m50_d = [delta_df_m50.iloc[k]['rmse'] for k in range(K_minus_1)]\nrmse_m50_new_d = [delta_stats_m50_new[k]['rmse'] for k in range(K_minus_1)]\n\nbars1 = ax.bar(x - width, rmse_m25_d, width, label='M=25 (base)', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x, rmse_m50_d, width, label='M=50 (same alts)', color='coral', alpha=0.7)\nbars3 = ax.bar(x + width, rmse_m50_new_d, width, label='M=50 (new alts)', color='mediumseagreen', alpha=0.7)\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('δ RMSE by Extension Type', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.legend(loc='upper right')\nax.grid(True, alpha=0.3, axis='y')\n\n# Coverage comparison\nax = axes[1]\ncov_m25_d = [delta_df.iloc[k]['coverage'] for k in range(K_minus_1)]\ncov_m50_d = [delta_df_m50.iloc[k]['coverage'] for k in range(K_minus_1)]\ncov_m50_new_d = [delta_stats_m50_new[k]['coverage'] for k in range(K_minus_1)]\n\nbars1 = ax.bar(x - width, cov_m25_d, width, label='M=25 (base)', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x, cov_m50_d, width, label='M=50 (same alts)', color='coral', alpha=0.7)\nbars3 = ax.bar(x + width, cov_m50_new_d, width, label='M=50 (new alts)', color='mediumseagreen', alpha=0.7)\nax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('Coverage', fontsize=12)\nax.set_title('δ 90% CI Coverage by Extension Type', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.set_ylim(0, 1.05)\nax.legend(loc='lower right')\nax.grid(True, alpha=0.3, axis='y')\n\n# CI Width comparison  \nax = axes[2]\nci_m25_d = [delta_df.iloc[k]['ci_width'] for k in range(K_minus_1)]\nci_m50_d = [delta_df_m50.iloc[k]['ci_width'] for k in range(K_minus_1)]\nci_m50_new_d = [delta_stats_m50_new[k]['ci_width'] for k in range(K_minus_1)]\n\nbars1 = ax.bar(x - width, ci_m25_d, width, label='M=25 (base)', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x, ci_m50_d, width, label='M=50 (same alts)', color='coral', alpha=0.7)\nbars3 = ax.bar(x + width, ci_m50_new_d, width, label='M=50 (new alts)', color='mediumseagreen', alpha=0.7)\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('CI Width', fontsize=12)\nax.set_title('δ 90% CI Width by Extension Type', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.legend(loc='upper right')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Three-way comparison of δ recovery: M=25 (base), M=50 (same alternatives), and M=50 (new alternatives). Adding new alternatives may provide different information than simply adding more problems over existing alternatives. (β exhibits similar patterns to δ due to their structural coupling.)\n\n\n\n\n\n\n\n\nTable 3\n\n\n\nThree-Way Extension Comparison:\n  Param    RMSE           RMSE          RMSE    CI Width       CI Width      CI Width\n           M=25    M=50 (same)    M=50 (new)        M=25    M=50 (same)    M=50 (new)\n-------  ------  -------------  ------------  ----------  -------------  ------------\n      α  1.1685         0.7961        0.7062       3.252          2.535         2.374\n    δ_1  0.3048         0.2955        0.287        0.892          0.879         0.877\n    δ_2  0.3048         0.2955        0.287        0.892          0.879         0.877\n\n\n\n\n\n\n0.6.4 Interpretation: Sample Size and Alternative Effects\nThe three-way comparison reveals important patterns about what helps δ recovery (β exhibits similar patterns due to the structural coupling between these parameters):\n\n\n\n\n\n\nNoteExtension Type Matters\n\n\n\nComparing the three conditions:\n\nM=25 (base): Baseline recovery with modest δ precision\nM=50 (same alternatives): Adding 25 problems over the same R=15 alternatives\nM=50 (new alternatives): Adding 25 problems using 15 new alternatives (R increases to 30)\n\nKey findings: - α recovery improves with more data regardless of extension type - δ recovery may benefit differently from new alternatives vs. more problems over existing alternatives - Adding new feature vectors expands the feature space, potentially providing different information about how features map to subjective probabilities\nThe comparison between conditions 2 and 3 is particularly important for understanding what helps δ recovery. If new alternatives (condition 3) outperform same alternatives (condition 2), it suggests that expanding the feature space provides qualitatively different information.\n\n\nThis three-way comparison sets up a crucial comparison for Report 5, where we add risky alternatives (lotteries with known probabilities) instead of additional uncertain alternatives. By comparing:\n\nM=50 with new uncertain alternatives (this report): Expands uncertain feature space\nM=25 + N=25 with new risky alternatives (Report 5): Adds known-probability lotteries\n\n…we can isolate whether improvements come from the type of data (risk vs. uncertainty) or simply from adding more diverse choice problems.\nThe asymmetry in learning rates for (β, δ) versus α reflects the model structure: in decisions under uncertainty, utilities and subjective probabilities enter the likelihood through their product (expected utilities). This creates a multiplicative coupling that makes (β, δ) harder to pin down than α, which more directly governs choice sensitivity.\n\n\n0.6.5 Study Design Parameters in m_0\nThe m_0.stan data block defines several parameters that characterize study size:\n\n\n\nParameter\nDescription\nCurrent Value\n\n\n\n\nM\nNumber of decision problems\n25 → 50\n\n\nK\nNumber of consequences\n3\n\n\nD\nFeature dimensions\n5\n\n\nR\nDistinct alternatives\n15\n\n\n\nWe might consider varying other parameters:\n\nIncreasing K (more consequences): Would require estimating more δ parameters, potentially making recovery harder\nIncreasing R (more alternatives): Provides more information about β, and may indirectly help δ through better-constrained expected utilities\nIncreasing D (more features): Similar to increasing R—primarily helps with β\n\nThe effectiveness of these changes for (β, δ) recovery is an empirical question worth exploring in future work.\n\n\n0.6.6 An Alternative Approach: Adding Risky Choice Data\nDecision theory offers a principled alternative to simply increasing sample size: adding risky choice data where probabilities are known to the decision-maker. This approach, motivated by the Anscombe-Aumann framework, provides a different type of information that may help constrain utility parameters more directly.\nIn risky decisions, the probabilities are fixed by the experimenter, so choices directly reveal information about utilities without the partial confounding with subjective probabilities that occurs in decisions under uncertainty.\n\n\n\n\n\n\nNotePreview of Model m_1\n\n\n\nModel m_1 extends m_0 by adding N risky choice problems where:\n\nAlternatives are lotteries with experimenter-specified probabilities\nThe same utility function υ (and hence δ) governs both choice types\nRisky choices provide more direct information about utility structure\n\nThis provides an alternative route to improving (β, δ) recovery that may be more efficient than simply increasing M. See Report 5 for the full development and comparison."
  },
  {
    "objectID": "foundations/04_parameter_recovery.html#conclusion",
    "href": "foundations/04_parameter_recovery.html#conclusion",
    "title": "Parameter Recovery Analysis",
    "section": "0.7 Conclusion",
    "text": "0.7 Conclusion\nParameter recovery analysis for model m_0 reveals differential recovery across parameters:\n\nWell-recovered parameter: α (sensitivity) can be recovered with good precision\nSlower-learning parameters: β (feature weights) and δ (utility increments) show wider uncertainty, though they do respond to data—posteriors concentrate relative to priors, but more slowly than for α\nSample size helps, but differentially: Doubling M improves α recovery more than (β, δ) recovery\n\nThis pattern reflects the identification structure of the SEU model analyzed formally in Report 5: while α is globally identified, β and δ interact multiplicatively in the expected utility, causing uncertainty in one to propagate to the other.\nImplications for applications:\n\nIf the primary research question concerns sensitivity (α)—how strongly choices respond to expected utility differences—model m_0 performs well even with modest sample sizes\nIf precise decomposition of expected utility into beliefs (β) and values (δ) is needed, either larger samples or the addition of risky choice data (model m_1) will be required\n\nTwo paths forward for improving (β, δ) recovery:\n\nMore data: Larger M (and perhaps R) may eventually yield precise (β, δ) estimates, though the rate of improvement is slower than for α\nDifferent data: Adding risky choices (model m_1) provides a complementary data source where probabilities are known, breaking the multiplicative coupling and constraining utilities more directly\n\nThe choice between these approaches—or their combination—depends on practical considerations like study feasibility and the relative importance of precise utility estimation for the research question at hand."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html",
    "href": "foundations/02_concrete_implementation.html",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "",
    "text": "The previous report established the theoretical foundations of the softmax choice model with sensitivity parameter \\(\\alpha\\). We proved three fundamental properties and showed how they apply when values are subjective expected utilities.\nThis report bridges theory and practice by specifying an initial concrete implementation in Stan. We show precisely how the abstract formulation is instantiated, highlighting the specific choices made for:\n\nSubjective probability formation: How features of alternatives map to beliefs about consequences\nUtility parameterization: How we construct ordered utilities on the unit scale\nPrior distributions: The default priors and their rationale\n\n\n\n\n\n\n\nNoteReading This Report\n\n\n\nThis report is intended for readers who want to understand, modify, or extend the model. We present the full Stan code with detailed commentary on each block."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#introduction",
    "href": "foundations/02_concrete_implementation.html#introduction",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "",
    "text": "The previous report established the theoretical foundations of the softmax choice model with sensitivity parameter \\(\\alpha\\). We proved three fundamental properties and showed how they apply when values are subjective expected utilities.\nThis report bridges theory and practice by specifying an initial concrete implementation in Stan. We show precisely how the abstract formulation is instantiated, highlighting the specific choices made for:\n\nSubjective probability formation: How features of alternatives map to beliefs about consequences\nUtility parameterization: How we construct ordered utilities on the unit scale\nPrior distributions: The default priors and their rationale\n\n\n\n\n\n\n\nNoteReading This Report\n\n\n\nThis report is intended for readers who want to understand, modify, or extend the model. We present the full Stan code with detailed commentary on each block."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#from-abstract-to-concrete",
    "href": "foundations/02_concrete_implementation.html#from-abstract-to-concrete",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.2 From Abstract to Concrete",
    "text": "0.2 From Abstract to Concrete\nThe abstract formulation in Report 1 established notation for alternatives (\\(\\mathcal{R}\\)), consequences (\\(K\\)), subjective probabilities (\\(\\boldsymbol{\\psi}_r\\)), utilities (\\(\\boldsymbol{\\upsilon}\\)), expected utilities (\\(\\eta_r\\)), and the sensitivity parameter (\\(\\alpha\\)). That formulation left several things unspecified:\n\nWhere do subjective probabilities come from? The abstract model takes \\(\\boldsymbol{\\psi}_r\\) as given\nHow are utilities parameterized? We required \\(0 = \\upsilon_1 \\leq \\cdots \\leq \\upsilon_K = 1\\), but not how to achieve this\nWhat priors should we use? The theoretical results are prior-free\n\nThe m_0 model provides a concrete answer to each of these questions."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#data-structure",
    "href": "foundations/02_concrete_implementation.html#data-structure",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.3 Data Structure",
    "text": "0.3 Data Structure\nThe model operates on a specific data structure that supports flexible experimental designs.\n\n0.3.1 Conceptual Overview\nWe assume:\n\nA set of \\(R\\) distinct alternatives, each described by a \\(D\\)-dimensional feature vector\nA collection of \\(M\\) decision problems, each presenting a subset of the alternatives\nFor each problem, an observed choice from among the available alternatives\n\nThis structure supports a wide range of experimental designs—from simple paired comparisons to complex multi-alternative choice sets.\n\n\n0.3.2 Stan Data Block\n\n\nm_0.stan (data block)\n\ndata {\n  int&lt;lower=1&gt; M;                        // Number of decision problems\n  int&lt;lower=2&gt; K;                        // Number of possible consequences\n  int&lt;lower=1&gt; D;                        // Feature dimensions\n  int&lt;lower=2&gt; R;                        // Number of distinct alternatives\n  array[R] vector[D] w;                  // Feature vectors for alternatives\n  array[M,R] int&lt;lower=0,upper=1&gt; I;     // Indicator: I[m,r]=1 if r in problem m\n  array[M] int&lt;lower=1&gt; y;               // Observed choices\n}\n\n\n\n\n\n\n\nNoteNotation: Data Structure\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nStan Variable\nDescription\n\n\n\n\n\\(M\\)\nM\nNumber of decision problems\n\n\n\\(D\\)\nD\nFeature dimensions\n\n\n\\(\\mathbf{w}_r \\in \\mathbb{R}^D\\)\nw[r]\nFeature vector for alternative \\(r\\)\n\n\n\\(I_{m,r} \\in \\{0,1\\}\\)\nI[m,r]\nIndicator: 1 if alternative \\(r\\) available in problem \\(m\\)\n\n\n\\(y_m\\)\ny[m]\nObserved choice in problem \\(m\\)\n\n\n\nThe variables \\(R\\), \\(K\\), and \\(\\alpha\\) retain their meanings from Report 1.\n\n\n\n\n\n\n\n\nNoteOn “Distinct” Alternatives\n\n\n\nThe variable R represents the number of distinct alternatives in the study design. “Distinct” here reflects a judgment by the study designer about how to individuate alternatives. Within the model, two alternatives with identical feature vectors \\(\\mathbf{w}_r\\) will have the same subjective probability distribution over consequences, and thus the same expected utility. The model itself has no notion of alternative identity beyond the feature representation.\n\n\n\n\n0.3.3 Transformed Data\nThe model preprocesses the data for computational efficiency:\n\n\nm_0.stan (transformed data)\n\ntransformed data {\n  array[M] int&lt;lower=2&gt; N;           // Alternatives per problem\n  int total_alternatives = 0;\n  \n  for (m in 1:M) {\n    N[m] = sum(I[m]);                // Count alternatives in problem m\n    total_alternatives += N[m];\n  }\n  \n  // Construct flattened feature array from w and I\n  array[total_alternatives] vector[D] x;\n  {\n    int pos = 1;\n    for (m in 1:M) {\n      for (r in 1:R) {\n        if (I[m, r] == 1) {\n          x[pos] = w[r];\n          pos += 1;\n        }\n      }\n    }\n  }\n}\n\nThe key transformation is constructing x—a flattened array that lists the feature vectors for all alternatives across all problems, in order. This allows efficient vectorized computation in the transformed parameters block."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#model-parameters",
    "href": "foundations/02_concrete_implementation.html#model-parameters",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.4 Model Parameters",
    "text": "0.4 Model Parameters\nThe m_0 model estimates three parameter groups:\n\n\nm_0.stan (parameters)\n\nparameters {\n  real&lt;lower=0&gt; alpha;           // Sensitivity parameter\n  matrix[K,D] beta;              // Feature-to-probability mapping\n  simplex[K-1] delta;            // Utility increments\n}\n\n\n0.4.1 The Sensitivity Parameter (\\(\\alpha\\))\nThe parameter alpha is constrained to be non-negative, matching the theoretical requirement \\(\\alpha \\geq 0\\). Its interpretation was established in Report 1: with utilities standardized to \\([0,1]\\), \\(\\alpha\\) represents the log-odds change per unit of expected utility difference.\n\n\n0.4.2 The Feature-to-Probability Mapping (\\(\\boldsymbol{\\beta}\\))\nThe \\(K \\times D\\) matrix beta parameterizes how alternative features determine subjective probabilities over consequences. This is the key modeling assumption that connects observable features to latent beliefs.\n\n\n0.4.3 Utility Increments (\\(\\boldsymbol{\\delta}\\))\nThe \\((K-1)\\)-simplex delta parameterizes the spacing of utilities on the unit interval. We explain this construction in detail below."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#subjective-probability-formation",
    "href": "foundations/02_concrete_implementation.html#subjective-probability-formation",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.5 Subjective Probability Formation",
    "text": "0.5 Subjective Probability Formation\nA central modeling decision in m_0 is how subjective probabilities arise from alternative features.\n\n0.5.1 The Mapping\nFor each alternative with feature vector \\(\\mathbf{w}_r \\in \\mathbb{R}^D\\), we compute subjective probabilities via:\n\\[\n\\boldsymbol{\\psi}_r = \\text{softmax}(\\boldsymbol{\\beta} \\mathbf{w}_r)\n\\tag{1}\\]\nwhere \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{K \\times D}\\) is a coefficient matrix.\nExpanding the softmax:\n\\[\n\\psi_{r,k} = \\frac{\\exp\\left(\\sum_{d=1}^D \\beta_{k,d} \\cdot w_{r,d}\\right)}{\\sum_{j=1}^K \\exp\\left(\\sum_{d=1}^D \\beta_{j,d} \\cdot w_{r,d}\\right)}\n\\tag{2}\\]\n\n\n0.5.2 Stan Implementation\n\n\nm_0.stan (subjective probabilities)\n\ntransformed parameters {\n  array[sum(N)] simplex[K] psi;    // Subjective probabilities\n  \n  // Calculate subjective probabilities via softmax\n  for (i in 1:sum(N)) {\n    psi[i] = softmax(beta * x[i]);\n  }\n  // ...\n}\n\n\n\n0.5.3 Interpretation\nThis parameterization embeds several assumptions:\n\nLinear-in-features: The log-odds of each consequence are linear functions of features\nShared coefficient matrix: The same \\(\\boldsymbol{\\beta}\\) applies to all alternatives\nSoftmax normalization: Probabilities sum to 1 and are strictly positive\n\n\n\nShow code\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Example feature vector\nw = np.array([0.8, -0.3])\n\n# Example beta matrix (K=3, D=2)\nbeta = np.array([\n    [1.2, -0.5],   # Consequence 1 coefficients\n    [0.0,  0.8],   # Consequence 2 coefficients\n    [-0.8, 0.3]    # Consequence 3 coefficients\n])\n\n# Compute log-odds and probabilities\nlog_odds = beta @ w\npsi = softmax(log_odds)\n\n# Plot 1: Feature vector\nax1 = axes[0]\nax1.bar(['w₁', 'w₂'], w, color=['steelblue', 'coral'])\nax1.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\nax1.set_ylabel('Feature Value')\nax1.set_title('Feature Vector w')\nax1.set_ylim(-1, 1)\n\n# Plot 2: Log-odds (beta * w)\nax2 = axes[1]\nax2.bar(['k=1', 'k=2', 'k=3'], log_odds, color=['forestgreen', 'orange', 'purple'])\nax2.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)\nax2.set_ylabel('Log-odds')\nax2.set_title('Linear Mapping βw')\n\n# Plot 3: Probabilities (softmax)\nax3 = axes[2]\nbars = ax3.bar(['ψ₁', 'ψ₂', 'ψ₃'], psi, color=['forestgreen', 'orange', 'purple'])\nax3.set_ylabel('Probability')\nax3.set_title('Subjective Probabilities ψ = softmax(βw)')\nax3.set_ylim(0, 1)\n\n# Add probability labels\nfor bar, p in zip(bars, psi):\n    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n             f'{p:.2f}', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Illustration of subjective probability formation. A 2D feature vector is mapped through the β matrix to produce a probability distribution over 3 consequences.\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative Specifications\n\n\n\nThe linear-softmax mapping is a reasonable starting point, but alternatives exist:\n\nNeural network: Replace \\(\\boldsymbol{\\beta} \\mathbf{w}_r\\) with a nonlinear function\nHierarchical: Allow \\(\\boldsymbol{\\beta}\\) to vary across individuals or problem types\nConstrained: Impose structure on \\(\\boldsymbol{\\beta}\\) based on domain knowledge\n\nThese extensions would require modifying the Stan model but preserve the overall framework.\n\n\n\n\n\n\n\n\nNoteOn the β Prior Scale\n\n\n\nThe \\(\\mathcal{N}(0, 1)\\) prior places 95% of coefficients within \\(\\pm 2\\). In the softmax context, this corresponds to log-odds differences up to \\(\\pm 2\\), or probability ratios up to \\(e^2 \\approx 7.4\\) per unit feature change. This is moderately informative—it allows substantial feature effects while regularizing against extreme values. For applications where features are expected to have very strong effects on beliefs, a wider prior (e.g., \\(\\mathcal{N}(0, 2)\\)) may be appropriate."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#utility-parameterization",
    "href": "foundations/02_concrete_implementation.html#utility-parameterization",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.6 Utility Parameterization",
    "text": "0.6 Utility Parameterization\nThe abstract formulation required utilities to satisfy: \\[\n0 = \\upsilon_1 \\leq \\upsilon_2 \\leq \\cdots \\leq \\upsilon_K = 1\n\\]\nThe m_0 model achieves this through an incremental parameterization.\n\n0.6.1 The Incremental Construction\nDefine \\(\\boldsymbol{\\delta} = (\\delta_1, \\ldots, \\delta_{K-1})\\) as a \\((K-1)\\)-simplex, so \\(\\delta_k \\geq 0\\) and \\(\\sum_{k=1}^{K-1} \\delta_k = 1\\).\nThen construct utilities as cumulative sums: \\[\n\\upsilon_k = \\sum_{j=1}^{k-1} \\delta_j\n\\tag{3}\\]\nwith the convention that \\(\\upsilon_1 = 0\\) (empty sum).\nThis construction guarantees:\n\n\\(\\upsilon_1 = 0\\) (the sum of zero terms)\n\\(\\upsilon_K = \\sum_{j=1}^{K-1} \\delta_j = 1\\) (the simplex sums to 1)\n\\(\\upsilon_k \\leq \\upsilon_{k+1}\\) (each \\(\\delta_k \\geq 0\\))\n\n\n\n0.6.2 Stan Implementation\n\n\nm_0.stan (utility construction)\n\ntransformed parameters {\n  // ...\n  ordered[K] upsilon;              // Ordered utilities\n  \n  // Construct ordered utilities from increments\n  upsilon = cumulative_sum(append_row(0, delta));\n  // ...\n}\n\nThe append_row(0, delta) prepends a zero to the \\(\\delta\\) vector, and cumulative_sum computes the running totals.\n\n\n0.6.3 Visualizing the Construction\n\n\nShow code\nnp.random.seed(42)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Generate sample delta vectors from Dirichlet(1,1)\nn_samples = 3\ncolors = ['steelblue', 'coral', 'forestgreen']\n\nfor i in range(n_samples):\n    delta = np.random.dirichlet([1, 1])\n    upsilon = np.cumsum(np.concatenate([[0], delta]))\n    \n    # Plot delta\n    x_positions = [i * 2.5, i * 2.5 + 1]\n    axes[0].bar(x_positions, delta, \n                alpha=0.7, color=colors[i], label=f'Sample {i+1}', width=0.8)\n    \n    # Plot upsilon\n    axes[1].plot([1, 2, 3], upsilon, 'o-', color=colors[i], \n                 linewidth=2, markersize=8, label=f'Sample {i+1}')\n\n# Set x-axis labels for delta plot\naxes[0].set_xticks([0.5, 3, 5.5])\naxes[0].set_xticklabels(['Sample 1\\n(δ₁, δ₂)', 'Sample 2\\n(δ₁, δ₂)', 'Sample 3\\n(δ₁, δ₂)'])\n\naxes[0].set_ylabel('Increment Value')\naxes[0].set_title('Utility Increments δ ~ Dirichlet(1,1)')\naxes[0].legend()\n\naxes[1].set_xlabel('Consequence k')\naxes[1].set_ylabel('Utility υₖ')\naxes[1].set_title('Resulting Utility Vectors')\naxes[1].set_xticks([1, 2, 3])\naxes[1].set_ylim(-0.05, 1.05)\naxes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[1].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: The incremental utility construction. Left: Three sample δ vectors from a Dirichlet(1,1) prior. Right: The corresponding utility vectors, all satisfying υ₁=0 and υ₃=1.\n\n\n\n\n\n\n\n0.6.4 Prior on Utilities\nThe default prior delta ~ dirichlet(rep_vector(1, K-1)) is the symmetric Dirichlet with concentration parameter 1. This induces a uniform distribution over the simplex—all valid \\(\\boldsymbol{\\delta}\\) configurations are equally likely a priori.\nFor \\(K=3\\) consequences, this means:\n\nThe middle utility \\(\\upsilon_2\\) has prior mean 0.5 and prior standard deviation \\(\\approx 0.29\\)\nThe utilities are uniformly distributed subject to the ordering constraint\n\n\n\n\n\n\n\nNoteOn the Choice of Utility Prior\n\n\n\nThe uniform Dirichlet prior is relatively uninformative—it expresses no preference for how utilities should be spaced. In some applications, a more informative prior might be appropriate:\n\nDirichlet with larger concentration (e.g., all 5’s): Concentrates utilities more evenly\nDomain-specific: If consequences have natural ordering with known approximate spacing\n\nThe prior predictive analysis in Report 3 examines the implications of this default choice."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#expected-utility-and-choice-probabilities",
    "href": "foundations/02_concrete_implementation.html#expected-utility-and-choice-probabilities",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.7 Expected Utility and Choice Probabilities",
    "text": "0.7 Expected Utility and Choice Probabilities\nWith subjective probabilities \\(\\boldsymbol{\\psi}\\) and utilities \\(\\boldsymbol{\\upsilon}\\) in hand, we compute expected utilities and choice probabilities exactly as in the abstract formulation.\n\n0.7.1 Expected Utility Calculation\n\\[\n\\eta_r = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon} = \\sum_{k=1}^K \\psi_{r,k} \\cdot \\upsilon_k\n\\]\n\n\nm_0.stan (expected utilities)\n\ntransformed parameters {\n  // ...\n  vector[sum(N)] eta;              // Expected utilities\n  \n  // Calculate expected utility for each alternative\n  for (i in 1:sum(N)) {\n    eta[i] = dot_product(psi[i], upsilon);\n  }\n  // ...\n}\n\n\n\n0.7.2 Choice Probabilities\nFor each decision problem \\(m\\) with \\(N_m\\) available alternatives:\n\\[\n\\chi_{m,r} = \\frac{\\exp(\\alpha \\cdot \\eta_r)}{\\sum_{j \\in \\text{problem } m} \\exp(\\alpha \\cdot \\eta_j)}\n\\]\n\n\nm_0.stan (choice probabilities)\n\ntransformed parameters {\n  // ...\n  array[M] simplex[max(N)] chi;    // Choice probabilities (with padding)\n  \n  {\n    int pos = 1;\n    for (i in 1:M) {\n      vector[N[i]] problem_eta = segment(eta, pos, N[i]);\n      \n      chi[i] = append_row(\n        softmax(alpha * problem_eta),\n        rep_vector(0, max(N) - N[i])  // Padding for ragged array\n      );\n      \n      pos += N[i];\n    }\n  }\n}\n\nThe padding with zeros handles the ragged array structure—problems can have different numbers of alternatives, but Stan requires fixed array dimensions."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#prior-distributions",
    "href": "foundations/02_concrete_implementation.html#prior-distributions",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.8 Prior Distributions",
    "text": "0.8 Prior Distributions\nThe model block specifies prior distributions for all parameters:\n\n\nm_0.stan (model block)\n\nmodel {\n  // Priors\n  alpha ~ lognormal(0, 1);               // Sensitivity\n  to_vector(beta) ~ std_normal();        // Feature coefficients\n  delta ~ dirichlet(rep_vector(1, K-1)); // Utility increments\n  \n  // Likelihood\n  for (i in 1:M) {\n    y[i] ~ categorical(chi[i]);\n  }\n}\n\n\n0.8.1 Prior on \\(\\alpha\\): Lognormal(0, 1)\nThe lognormal prior ensures \\(\\alpha &gt; 0\\) and places substantial mass on moderate sensitivity values.\nKey properties of Lognormal(0, 1):\n\n\n\nQuantity\nValue\n\n\n\n\nMedian\n1.00\n\n\nMean\n1.65\n\n\n25th percentile\n0.51\n\n\n75th percentile\n1.96\n\n\n95th percentile\n5.18\n\n\n\n\n\n0.8.2 Prior on \\(\\boldsymbol{\\beta}\\): Standard Normal\nEach element of the \\(K \\times D\\) matrix \\(\\boldsymbol{\\beta}\\) receives an independent \\(\\mathcal{N}(0, 1)\\) prior. This is a weakly informative default that:\n\nCenters coefficients at zero (no a priori direction of effect)\nAllows moderately large coefficients (95% within \\(\\pm 2\\))\nIs computationally convenient\n\n\n\n0.8.3 Prior on \\(\\boldsymbol{\\delta}\\): Symmetric Dirichlet\nAs discussed above, delta ~ dirichlet(rep_vector(1, K-1)) induces a uniform distribution over valid utility configurations."
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#summary",
    "href": "foundations/02_concrete_implementation.html#summary",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.9 Summary",
    "text": "0.9 Summary\nThe m_0 model provides a complete Bayesian implementation of the SEU sensitivity framework:\n\n\n\n\n\n\n\n\nComponent\nAbstract\nConcrete (m_0)\n\n\n\n\nBeliefs\n\\(\\boldsymbol{\\psi}_r \\in \\Delta^{K-1}\\)\n\\(\\boldsymbol{\\psi}_r = \\text{softmax}(\\boldsymbol{\\beta} \\mathbf{w}_r)\\)\n\n\nUtilities\n\\(0 = \\upsilon_1 \\leq \\cdots \\leq \\upsilon_K = 1\\)\n\\(\\boldsymbol{\\upsilon} = \\text{cumsum}([0, \\boldsymbol{\\delta}])\\)\n\n\nSensitivity\n\\(\\alpha \\geq 0\\)\n\\(\\alpha \\sim \\text{Lognormal}(0, 1)\\)\n\n\nChoice\n\\(\\chi_m = \\text{softmax}(\\alpha \\cdot \\boldsymbol{\\eta}_m)\\)\nSame, with categorical likelihood\n\n\n\nThe model makes specific choices—linear feature mapping, symmetric Dirichlet prior on utilities—that could be modified for particular applications. The theoretical properties from Report 1 (monotonicity, perfect rationality limit, random choice limit) hold for any such modifications, provided the basic softmax choice structure is preserved.\nThe next report examines the prior predictive distribution implied by these choices, asking: what range of behaviors does the prior permit?"
  },
  {
    "objectID": "foundations/02_concrete_implementation.html#references",
    "href": "foundations/02_concrete_implementation.html#references",
    "title": "Concrete Implementation: The m_0 Stan Model",
    "section": "0.10 References",
    "text": "0.10 References"
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html",
    "href": "applications/temperature_study/01_initial_study.html",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "",
    "text": "The foundational reports established the SEU sensitivity model’s theoretical properties (Report 1), concrete implementation (Report 2), and computational validity (Reports 3–6). This report presents the first empirical application: investigating how the sampling temperature of a large language model affects its estimated sensitivity to expected utility maximization.\nThe motivation is straightforward. In the softmax choice model, the sensitivity parameter \\(\\alpha\\) governs how sharply choices track expected utility differences. Language models employ their own softmax temperature during token sampling, which controls the entropy of the next-token distribution. If the LLM’s choice behavior is well-described by our model, then increasing the external sampling temperature should decrease the estimated \\(\\alpha\\)—the additional stochasticity in token generation should manifest as noisier, less EU-aligned choices.\n\n\n\n\n\n\nNoteWhat This Report Covers\n\n\n\nThis report presents initial results from a single data collection. The study design, prior calibration, model fitting, and primary analysis are documented here. Subsequent reports will address robustness checks and extensions."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#introduction",
    "href": "applications/temperature_study/01_initial_study.html#introduction",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "",
    "text": "The foundational reports established the SEU sensitivity model’s theoretical properties (Report 1), concrete implementation (Report 2), and computational validity (Reports 3–6). This report presents the first empirical application: investigating how the sampling temperature of a large language model affects its estimated sensitivity to expected utility maximization.\nThe motivation is straightforward. In the softmax choice model, the sensitivity parameter \\(\\alpha\\) governs how sharply choices track expected utility differences. Language models employ their own softmax temperature during token sampling, which controls the entropy of the next-token distribution. If the LLM’s choice behavior is well-described by our model, then increasing the external sampling temperature should decrease the estimated \\(\\alpha\\)—the additional stochasticity in token generation should manifest as noisier, less EU-aligned choices.\n\n\n\n\n\n\nNoteWhat This Report Covers\n\n\n\nThis report presents initial results from a single data collection. The study design, prior calibration, model fitting, and primary analysis are documented here. Subsequent reports will address robustness checks and extensions."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#sec-design",
    "href": "applications/temperature_study/01_initial_study.html#sec-design",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.2 Experimental Design",
    "text": "0.2 Experimental Design\n\n0.2.1 Task and Conditions\nWe use an insurance claims triage task in which GPT-4o selects which claim to forward for investigation from a set of \\(K = 3\\) alternatives. The LLM first assesses each claim individually (producing text that is then embedded), and subsequently makes a choice among the alternatives in a given problem.\nFive temperature levels define the between-condition factor:\n\n\nShow code\nimport pandas as pd\n\nconditions = pd.DataFrame({\n    'Level': [1, 2, 3, 4, 5],\n    'Temperature': [0.0, 0.3, 0.7, 1.0, 1.5],\n    'Description': [\n        'Deterministic (greedy decoding)',\n        'Low variance',\n        'Moderate variance (typical default)',\n        'High variance',\n        'Very high variance'\n    ]\n})\nconditions\n\n\n\n\nTable 1: Experimental conditions. Each temperature level constitutes a separate model fit.\n\n\n\n\n\n\n\n\n\n\nLevel\nTemperature\nDescription\n\n\n\n\n0\n1\n0.0\nDeterministic (greedy decoding)\n\n\n1\n2\n0.3\nLow variance\n\n\n2\n3\n0.7\nModerate variance (typical default)\n\n\n3\n4\n1.0\nHigh variance\n\n\n4\n5\n1.5\nVery high variance\n\n\n\n\n\n\n\n\n\n\n\n\n0.2.2 Design Parameters\n\n\nShow code\n# Load frozen study configuration\nimport yaml\n\nwith open(data_dir / \"study_config.yaml\") as f:\n    config = yaml.safe_load(f)\n\n# Load run summary for pipeline details\nwith open(data_dir / \"run_summary.json\") as f:\n    run_summary = json.load(f)\n\nprint(f\"Study Design:\")\nprint(f\"  Decision problems (M):      {config['num_problems']} base × {config['num_presentations']} presentations = {config['num_problems'] * config['num_presentations']}\")\nprint(f\"  Alternatives per problem:    {config['min_alternatives']}–{config['max_alternatives']}\")\nprint(f\"  Consequences (K):            {config['K']}\")\nprint(f\"  Embedding dimensions (D):    {config['target_dim']}\")\nprint(f\"  Distinct alternatives (R):   {run_summary['phases']['phase3_data_prep']['per_temperature']['0.0']['R']}\")\nprint(f\"  LLM model:                   {config['llm_model']}\")\nprint(f\"  Embedding model:             {config['embedding_model']}\")\n\n\nStudy Design:\n  Decision problems (M):      100 base × 3 presentations = 300\n  Alternatives per problem:    2–4\n  Consequences (K):            3\n  Embedding dimensions (D):    32\n  Distinct alternatives (R):   30\n  LLM model:                   gpt-4o\n  Embedding model:             text-embedding-3-small\n\n\nEach of the 100 base problems is presented \\(P = 3\\) times with claims shuffled to different positions, yielding \\(M = 300\\) observations per temperature condition. This position counterbalancing design addresses the systematic position bias discovered in an earlier pilot study, where unparseable responses were silently mapped to position 0. Here, each claim appears in multiple positions across presentations, and any unparseable response is recorded as NA rather than assigned a default.\n\n\n0.2.3 Feature Construction\nAlternative features are constructed through a two-stage process. First, the LLM assesses each claim on its own merits at the relevant temperature, producing a natural-language evaluation. These assessments are embedded using text-embedding-3-small, yielding high-dimensional vectors. Second, all embeddings across temperature conditions are pooled and projected via PCA to \\(D = 32\\) dimensions. Crucially, PCA is fit once on the pooled set and then applied to each condition separately, ensuring a shared coordinate system without privileging any single temperature.\n\n\nPCA Summary:\n  Components retained: 32\n  Total variance explained: 82.2%\n  First 5 components: 35.4%\n  First 10 components: 53.0%\n\n\n\n\n0.2.4 Data Quality\n\n\nNA Summary:\n  Overall: 0 / 1500 (0.0%)\n  T=0.0: 0 / 300 (0.0%)\n  T=0.3: 0 / 300 (0.0%)\n  T=0.7: 0 / 300 (0.0%)\n  T=1.0: 0 / 300 (0.0%)\n  T=1.5: 0 / 300 (0.0%)\n\n\nNo observations were lost to parsing failures at any temperature level—a substantial improvement over the earlier pilot, which required ad hoc imputation."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#sec-model",
    "href": "applications/temperature_study/01_initial_study.html#sec-model",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.3 Model and Prior Calibration",
    "text": "0.3 Model and Prior Calibration\n\n0.3.1 The m_01 Model Variant\nWe fit the m_01 model, which is structurally identical to the foundational m_0 model described in Report 2. The only difference is the prior on the sensitivity parameter \\(\\alpha\\):\n\n\n\n\n\n\n\n\n\nm_0 (foundational)\nm_01 (this study)\n\n\n\n\n\\(\\alpha\\) prior\n\\(\\text{Lognormal}(0, 1)\\)\n\\(\\text{Lognormal}(3.0, 0.75)\\)\n\n\nPrior median\n\\(\\approx 1\\)\n\\(\\approx 20\\)\n\n\nPrior 90% CI\n\\([0.19, 5.0]\\)\n\\([5.5, 67]\\)\n\n\nAll other priors\n—\nIdentical to m_0\n\n\n\nThe foundational prior \\(\\text{Lognormal}(0, 1)\\) was chosen for generality: it spans a wide range of sensitivity levels appropriate for exploring the model’s behavior in simulation. However, when applied to LLM choice data—where we expect substantial sensitivity to EU differences—this prior places most of its mass well below the region where the data concentrate. The prior predictive analysis below motivates a more informative alternative.\n\n\n0.3.2 Prior Predictive Grid Search\nTo select the m_01 prior, we conducted a grid search over 12 candidate lognormal hyperparameter pairs \\((\\mu, \\sigma)\\), evaluating each via prior predictive simulation against the actual study design.\nFor each candidate prior, we:\n\nDrew \\(N = 200\\) values of \\(\\alpha\\) from \\(\\text{Lognormal}(\\mu, \\sigma)\\)\nSimulated choice data from m_0_sim.stan using the actual study design (\\(M = 300\\), \\(K = 3\\), \\(D = 32\\), \\(R = 30\\))\nComputed the SEU-maximizer selection rate: the fraction of problems in which the simulated agent chose the alternative with the highest expected utility\n\n\n\nShow code\nwith open(data_dir / \"grid_results.json\") as f:\n    grid = json.load(f)\n\nresults = grid['results']\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: SEU rate by prior\nlabels = [r['prior_label'] for r in results]\nmeans = [r['seu_rate_mean'] for r in results]\nq05s = [r['seu_rate_q05'] for r in results]\nq95s = [r['seu_rate_q95'] for r in results]\n\ny_pos = np.arange(len(labels))\nerrors = np.array([[m - q05, q95 - m] for m, q05, q95 in zip(means, q05s, q95s)]).T\n\ncolors = [SEU_COLORS['accent'] if 'lognormal(3.0, 0.75)' in l else SEU_COLORS['primary'] \n          for l in labels]\n\naxes[0].barh(y_pos, means, xerr=errors, color=colors, alpha=0.8, \n             edgecolor='white', capsize=3)\naxes[0].set_yticks(y_pos)\naxes[0].set_yticklabels(labels, fontsize=9)\naxes[0].set_xlabel('SEU-Maximizer Selection Rate')\naxes[0].set_title('Prior-Implied SEU-Max Rate')\naxes[0].axvline(x=1/3, color='gray', linestyle='--', alpha=0.5, label='Random (1/K)')\naxes[0].legend(fontsize=9)\naxes[0].set_xlim(0, 1)\n\n# Right: prior density comparison\nselected = [r for r in results if 'lognormal(3.0, 0.75)' in r['prior_label']][0]\nfoundational = [r for r in results if 'lognormal(0.0, 1.0)' in r['prior_label']][0]\n\nfrom scipy.stats import lognorm\nx = np.linspace(0.1, 150, 500)\n\n# m_0 prior: lognormal(0, 1) → scipy shape=1, scale=exp(0)=1\naxes[1].plot(x, lognorm.pdf(x, s=1.0, scale=np.exp(0)), \n             color=SEU_COLORS['secondary'], linewidth=2, label='m_0: Lognormal(0, 1)')\n# m_01 prior: lognormal(3.0, 0.75)\naxes[1].plot(x, lognorm.pdf(x, s=0.75, scale=np.exp(3.0)),\n             color=SEU_COLORS['accent'], linewidth=2, label='m_01: Lognormal(3.0, 0.75)')\naxes[1].set_xlabel('α')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Prior Comparison')\naxes[1].legend(fontsize=9)\naxes[1].set_xlim(0, 150)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Prior predictive grid search results. Each point represents a candidate lognormal prior for α, evaluated by the SEU-maximizer selection rate it implies. The selected prior lognormal(3.0, 0.75) balances informativeness with sufficient coverage of the plausible parameter range.\n\n\n\n\n\nThe selected prior \\(\\text{Lognormal}(3.0, 0.75)\\) has the following properties:\n\nMedian \\(\\approx 20\\), placing prior mass in the region where we expect LLM sensitivity to lie\n90% CI \\(\\approx [5.5, 67]\\), wide enough to accommodate substantial uncertainty\nPrior-implied SEU-max rate \\(\\approx 78\\%\\), consistent with the expectation that LLMs make reasonably EU-aligned choices\nNo numerical issues: the prior avoids the extreme \\(\\alpha\\) values that caused softmax overflow with wider priors\n\n\n\n\n\n\n\nNoteWhy Not the Foundational Prior?\n\n\n\nThe m_0 prior \\(\\text{Lognormal}(0, 1)\\) was designed for generality in simulation studies. Its median of \\(\\approx 1\\) means most prior mass sits in a regime where choices are nearly random—appropriate for exploring model behavior, but a poor match for LLM data where we observe strong EU-alignment. With the actual study design (\\(M = 300\\), \\(D = 32\\)), the foundational prior also places non-negligible mass on very large \\(\\alpha\\) values that cause numerical overflow in the softmax computation. The m_01 prior resolves both issues."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#sec-results",
    "href": "applications/temperature_study/01_initial_study.html#sec-results",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.4 Results",
    "text": "0.4 Results\n\n0.4.1 Loading Posterior Draws\n\n\nShow code\ntemperatures = [0.0, 0.3, 0.7, 1.0, 1.5]\ntemp_labels = {t: f\"T={t}\" for t in temperatures}\n\n# Load alpha draws for each temperature\nalpha_draws = {}\nfor t in temperatures:\n    key = f\"T{str(t).replace('.', '_')}\"\n    data = np.load(data_dir / f\"alpha_draws_{key}.npz\")\n    alpha_draws[t] = data['alpha']\n\n# Load pre-computed analysis results\nwith open(data_dir / \"primary_analysis.json\") as f:\n    analysis = json.load(f)\n\n# Load fit summary\nwith open(data_dir / \"fit_summary.json\") as f:\n    fit_summary = json.load(f)\n\n\n\n\n  T=0.0: 4,000 posterior draws loaded\n  T=0.3: 4,000 posterior draws loaded\n  T=0.7: 4,000 posterior draws loaded\n  T=1.0: 4,000 posterior draws loaded\n  T=1.5: 4,000 posterior draws loaded\n\n\n\n\n0.4.2 MCMC Diagnostics\n\n\nShow code\ndiag_rows = []\nfor t in temperatures:\n    key = f\"T{str(t).replace('.', '_')}\"\n    with open(data_dir / f\"diagnostics_{key}.txt\") as f:\n        diag_text = f.read()\n    \n    # Parse divergences\n    if \"No divergent transitions\" in diag_text:\n        n_div = 0\n    else:\n        import re\n        match = re.search(r'(\\d+) of (\\d+)', diag_text)\n        n_div = int(match.group(1)) if match else 0\n    \n    rhat_ok = \"R-hat values satisfactory\" in diag_text\n    ess_ok = \"effective sample size satisfactory\" in diag_text\n    ebfmi_ok = \"E-BFMI satisfactory\" in diag_text\n    \n    diag_rows.append({\n        'Temperature': t,\n        'Divergences': f\"{n_div}/4000\",\n        'R̂': '✓' if rhat_ok else '✗',\n        'ESS': '✓' if ess_ok else '✗',\n        'E-BFMI': '✓' if ebfmi_ok else '✗',\n    })\n\npd.DataFrame(diag_rows)\n\n\n\n\nTable 2: MCMC diagnostics for all five temperature conditions. All fits used 4 chains with 1,000 warmup and 1,000 sampling iterations each (4,000 post-warmup draws total).\n\n\n\n\n\n\n\n\n\n\nTemperature\nDivergences\nR̂\nESS\nE-BFMI\n\n\n\n\n0\n0.0\n0/4000\n✓\n✓\n✓\n\n\n1\n0.3\n0/4000\n✓\n✓\n✓\n\n\n2\n0.7\n0/4000\n✓\n✓\n✓\n\n\n3\n1.0\n1/4000\n✓\n✓\n✓\n\n\n4\n1.5\n2/4000\n✓\n✓\n✓\n\n\n\n\n\n\n\n\n\n\nAll conditions show clean diagnostics. The 1–2 divergent transitions at \\(T = 1.0\\) and \\(T = 1.5\\) (&lt; 0.05% of transitions) are well within acceptable bounds and do not indicate systematic sampling difficulties.\n\n\n0.4.3 Posterior Summaries\n\n\nShow code\nsummary = analysis['summary_table']\n\nrows = []\nfor s in summary:\n    rows.append({\n        'Temperature': s['temperature'],\n        'Median': f\"{s['median']:.1f}\",\n        'Mean': f\"{s['mean']:.1f}\",\n        'SD': f\"{s['sd']:.1f}\",\n        '90% CI': f\"[{s['ci_low']:.1f}, {s['ci_high']:.1f}]\",\n    })\n\npd.DataFrame(rows)\n\n\n\n\nTable 3: Posterior summaries for the sensitivity parameter α at each temperature level. Intervals are 90% credible intervals.\n\n\n\n\n\n\n\n\n\n\nTemperature\nMedian\nMean\nSD\n90% CI\n\n\n\n\n0\n0.0\n74.1\n78.0\n23.6\n[47.3, 121.1]\n\n\n1\n0.3\n54.9\n58.2\n18.7\n[36.1, 88.3]\n\n\n2\n0.7\n56.3\n58.3\n14.6\n[38.6, 84.9]\n\n\n3\n1.0\n39.1\n40.3\n9.8\n[26.6, 58.3]\n\n\n4\n1.5\n36.0\n37.0\n8.0\n[25.5, 51.8]\n\n\n\n\n\n\n\n\n\n\nThe estimates reveal a clear pattern: \\(\\alpha\\) is highest at \\(T = 0.0\\) (greedy decoding) and lowest at \\(T = 1.5\\) (high temperature), with intermediate conditions falling between these extremes. Note, however, the near-equality of the \\(T = 0.3\\) and \\(T = 0.7\\) estimates—a point we return to below.\n\n\n0.4.4 Forest Plot\n\n\nShow code\nfig, ax = plt.subplots(figsize=(8, 5))\n\ny_positions = np.arange(len(temperatures))[::-1]\n\nfor i, t in enumerate(temperatures):\n    draws = alpha_draws[t]\n    median = np.median(draws)\n    q05, q25, q75, q95 = np.percentile(draws, [5, 25, 75, 95])\n    \n    y = y_positions[i]\n    \n    # 90% CI (thin line)\n    ax.plot([q05, q95], [y, y], color=SEU_PALETTE[i], linewidth=1.5, alpha=0.7)\n    # 50% CI (thick line)\n    ax.plot([q25, q75], [y, y], color=SEU_PALETTE[i], linewidth=4, alpha=0.9)\n    # Median (point)\n    ax.plot(median, y, 'o', color=SEU_PALETTE[i], markersize=8, \n            markeredgecolor='white', markeredgewidth=1.5, zorder=5)\n\nax.set_yticks(y_positions)\nax.set_yticklabels([f'T = {t}' for t in temperatures])\nax.set_xlabel('Sensitivity (α)')\nax.set_title('Posterior Distributions of α by Temperature')\nax.grid(axis='x', alpha=0.3)\nax.grid(axis='y', alpha=0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Forest plot of posterior α distributions across temperature conditions. Points show posterior medians; thick bars span the 50% credible interval; thin bars span the 90% credible interval. Higher temperature is associated with lower estimated sensitivity.\n\n\n\n\n\n\n\n0.4.5 Posterior Densities\n\n\nShow code\nfrom scipy.stats import gaussian_kde\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nfor i, t in enumerate(temperatures):\n    draws = alpha_draws[t]\n    kde = gaussian_kde(draws)\n    x_grid = np.linspace(draws.min() * 0.8, draws.max() * 1.1, 300)\n    ax.fill_between(x_grid, kde(x_grid), alpha=0.2, color=SEU_PALETTE[i])\n    ax.plot(x_grid, kde(x_grid), color=SEU_PALETTE[i], linewidth=2, \n            label=f'T = {t} (median = {np.median(draws):.0f})')\n\nax.set_xlabel('Sensitivity (α)')\nax.set_ylabel('Density')\nax.set_title('Posterior Density of α')\nax.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Kernel density estimates of the posterior α distributions. The separation between T = 0.0 and T ≥ 1.0 is clear, while T = 0.3 and T = 0.7 overlap substantially.\n\n\n\n\n\n\n\n0.4.6 Posterior Predictive Checks\n\n\nShow code\nppc_rows = []\nfor t in temperatures:\n    key = f\"T{str(t).replace('.', '_')}\"\n    with open(data_dir / f\"ppc_{key}.json\") as f:\n        ppc = json.load(f)\n    \n    pvals = ppc['p_values']\n    ppc_rows.append({\n        'Temperature': t,\n        'Log-likelihood': f\"{pvals['ll']:.3f}\",\n        'Modal frequency': f\"{pvals['modal']:.3f}\",\n        'Mean probability': f\"{pvals['prob']:.3f}\",\n    })\n\npd.DataFrame(ppc_rows)\n\n\n\n\nTable 4: Posterior predictive check p-values for each temperature condition. Values near 0.5 indicate good calibration; values near 0 or 1 indicate model misfit. Three test statistics are used: log-likelihood (ll), modal choice frequency (modal), and mean choice probability (prob).\n\n\n\n\n\n\n\n\n\n\nTemperature\nLog-likelihood\nModal frequency\nMean probability\n\n\n\n\n0\n0.0\n0.404\n0.366\n0.338\n\n\n1\n0.3\n0.432\n0.490\n0.476\n\n\n2\n0.7\n0.434\n0.503\n0.385\n\n\n3\n1.0\n0.472\n0.624\n0.529\n\n\n4\n1.5\n0.479\n0.540\n0.515\n\n\n\n\n\n\n\n\n\n\nAll posterior predictive p-values fall comfortably within \\([0.3, 0.65]\\), indicating that the model provides an adequate description of the choice data at every temperature level. There is no evidence of systematic misfit."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#sec-monotonicity",
    "href": "applications/temperature_study/01_initial_study.html#sec-monotonicity",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.5 Monotonicity Analysis",
    "text": "0.5 Monotonicity Analysis\nThe central question of this study is whether estimated sensitivity \\(\\alpha\\) decreases monotonically with temperature. We assess this at three levels of granularity.\n\n0.5.1 Global Slope\nWe fit a simple linear relationship \\(\\alpha = a + b \\cdot T\\) to each posterior draw, yielding a posterior distribution over the slope \\(b\\):\n\n\nShow code\nslopes = analysis['slope']\n\n# Recompute the slope draws for the density plot\ntemp_array = np.array(temperatures)\n\nslope_draws = []\nfor draw_idx in range(len(alpha_draws[temperatures[0]])):\n    alphas_at_draw = np.array([alpha_draws[t][draw_idx] for t in temperatures])\n    # OLS slope: cov(T, α) / var(T)\n    b = np.cov(temp_array, alphas_at_draw)[0, 1] / np.var(temp_array)\n    slope_draws.append(b)\nslope_draws = np.array(slope_draws)\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nkde = gaussian_kde(slope_draws)\nx_grid = np.linspace(np.percentile(slope_draws, 0.5), np.percentile(slope_draws, 99.5), 300)\nax.fill_between(x_grid, kde(x_grid), alpha=0.3, color=SEU_COLORS['primary'])\nax.plot(x_grid, kde(x_grid), color=SEU_COLORS['primary'], linewidth=2)\n\nmedian_slope = np.median(slope_draws)\nax.axvline(x=median_slope, color=SEU_COLORS['accent'], linestyle='-', linewidth=2,\n           label=f'Median = {median_slope:.1f}')\nax.axvline(x=0, color='gray', linestyle='--', alpha=0.5, label='No effect')\n\n# Shade 90% CI\nq05, q95 = np.percentile(slope_draws, [5, 95])\nmask = (x_grid &gt;= q05) & (x_grid &lt;= q95)\nax.fill_between(x_grid[mask], kde(x_grid[mask]), alpha=0.15, color=SEU_COLORS['accent'])\nax.axvline(x=q05, color=SEU_COLORS['accent'], linestyle=':', alpha=0.6)\nax.axvline(x=q95, color=SEU_COLORS['accent'], linestyle=':', alpha=0.6)\n\nax.set_xlabel('Slope (Δα / ΔT)')\nax.set_ylabel('Density')\nax.set_title('Posterior Distribution of Temperature–Sensitivity Slope')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Slope summary:\")\nprint(f\"  Median:  {median_slope:.1f}\")\nprint(f\"  90% CI:  [{q05:.1f}, {q95:.1f}]\")\nprint(f\"  P(slope &lt; 0): {np.mean(slope_draws &lt; 0):.4f}\")\n\n\n\n\n\n\n\n\nFigure 4: Posterior distribution of the slope Δα/ΔT from a draw-wise linear fit. The entire 90% credible interval lies below zero, providing strong evidence that α decreases with temperature.\n\n\n\n\n\nSlope summary:\n  Median:  -30.8\n  90% CI:  [-65.5, -8.3]\n  P(slope &lt; 0): 0.9905\n\n\nThe posterior probability that the slope is negative exceeds 0.99, providing strong evidence for the directional hypothesis: higher temperature is associated with lower sensitivity to expected utility.\n\n\n0.5.2 Pairwise Comparisons\n\n\nShow code\npairs = analysis['pairwise_comparisons']\n\npair_rows = []\nfor key, prob in pairs.items():\n    t1, t2 = key.split('_vs_')\n    strength = '●●●' if prob &gt; 0.95 else ('●●' if prob &gt; 0.8 else ('●' if prob &gt; 0.65 else '○'))\n    pair_rows.append({\n        'Comparison': f'α(T={t1}) &gt; α(T={t2})',\n        'P': f'{prob:.3f}',\n        'Evidence': strength,\n    })\n\npd.DataFrame(pair_rows)\n\n\n\n\nTable 5: Posterior probability that α is higher at the lower temperature in each pair. Values above 0.95 indicate strong separation; values near 0.5 indicate indistinguishability.\n\n\n\n\n\n\n\n\n\n\nComparison\nP\nEvidence\n\n\n\n\n0\nα(T=0.0) &gt; α(T=0.3)\n0.773\n●\n\n\n1\nα(T=0.0) &gt; α(T=0.7)\n0.767\n●\n\n\n2\nα(T=0.0) &gt; α(T=1.0)\n0.963\n●●●\n\n\n3\nα(T=0.0) &gt; α(T=1.5)\n0.983\n●●●\n\n\n4\nα(T=0.3) &gt; α(T=0.7)\n0.480\n○\n\n\n5\nα(T=0.3) &gt; α(T=1.0)\n0.827\n●●\n\n\n6\nα(T=0.3) &gt; α(T=1.5)\n0.895\n●●\n\n\n7\nα(T=0.7) &gt; α(T=1.0)\n0.863\n●●\n\n\n8\nα(T=0.7) &gt; α(T=1.5)\n0.921\n●●\n\n\n9\nα(T=1.0) &gt; α(T=1.5)\n0.603\n○\n\n\n\n\n\n\n\n\n\n\nThe pairwise analysis reveals a structured pattern:\n\nStrong separation (\\(P &gt; 0.95\\)): \\(T = 0.0\\) vs \\(T = 1.0\\) and \\(T = 1.5\\)\nModerate separation (\\(P &gt; 0.8\\)): \\(T = 0.3\\) vs \\(T \\geq 1.0\\); \\(T = 0.7\\) vs \\(T \\geq 1.0\\)\nWeak/no separation (\\(P \\approx 0.5\\)): \\(T = 0.3\\) vs \\(T = 0.7\\)\n\n\n\n0.5.3 Strict Monotonicity\n\n\nShow code\n# P(α(T=0.0) &gt; α(T=0.3) &gt; α(T=0.7) &gt; α(T=1.0) &gt; α(T=1.5)) — joint probability\nn_draws = len(alpha_draws[0.0])\nstrictly_decreasing = 0\n\nfor i in range(n_draws):\n    vals = [alpha_draws[t][i] for t in temperatures]\n    if all(vals[j] &gt; vals[j+1] for j in range(len(vals)-1)):\n        strictly_decreasing += 1\n\np_mono = strictly_decreasing / n_draws\nprint(f\"P(α strictly decreasing across all T): {p_mono:.4f}\")\n\n\nP(α strictly decreasing across all T): 0.1247\n\n\nThe probability of strict monotonicity across all five levels is modest, at approximately 0.12. This is driven entirely by the near-equality of \\(\\alpha\\) at \\(T = 0.3\\) and \\(T = 0.7\\), where the pairwise probability is essentially a coin flip (\\(P = 0.48\\)). Collapsing these two levels yields a coarser ordering that holds with much higher confidence:\n\n\nShow code\n# Coarser test: α(T=0.0) &gt; mean(α(T=0.3), α(T=0.7)) &gt; α(T=1.0) &gt; α(T=1.5)\ncoarse_decreasing = 0\n\nfor i in range(n_draws):\n    a00 = alpha_draws[0.0][i]\n    a_mid = (alpha_draws[0.3][i] + alpha_draws[0.7][i]) / 2\n    a10 = alpha_draws[1.0][i]\n    a15 = alpha_draws[1.5][i]\n    if a00 &gt; a_mid &gt; a10 &gt; a15:\n        coarse_decreasing += 1\n\np_coarse = coarse_decreasing / n_draws\nprint(f\"P(α(0.0) &gt; mean(α(0.3), α(0.7)) &gt; α(1.0) &gt; α(1.5)): {p_coarse:.4f}\")\n\n\nP(α(0.0) &gt; mean(α(0.3), α(0.7)) &gt; α(1.0) &gt; α(1.5)): 0.3815"
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#sec-discussion",
    "href": "applications/temperature_study/01_initial_study.html#sec-discussion",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.6 Discussion",
    "text": "0.6 Discussion\n\n0.6.1 Summary of Findings\nThe results support the directional hypothesis: increasing LLM temperature decreases estimated sensitivity to expected utility maximization. The evidence is strongest at the global level—the posterior slope \\(\\Delta\\alpha / \\Delta T\\) is negative with high confidence—and weakest at fine-grained adjacent comparisons.\n\n\nShow code\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: point estimates with CIs\nmedians = [np.median(alpha_draws[t]) for t in temperatures]\nq05s = [np.percentile(alpha_draws[t], 5) for t in temperatures]\nq95s = [np.percentile(alpha_draws[t], 95) for t in temperatures]\n\naxes[0].errorbar(temperatures, medians, \n                 yerr=[np.array(medians) - np.array(q05s), np.array(q95s) - np.array(medians)],\n                 fmt='o-', color=SEU_COLORS['primary'], linewidth=2, markersize=8,\n                 capsize=5, capthick=1.5)\naxes[0].set_xlabel('Temperature')\naxes[0].set_ylabel('Sensitivity (α)')\naxes[0].set_title('α vs. Temperature')\naxes[0].set_xticks(temperatures)\n\n# Right: pairwise heatmap\nn_temps = len(temperatures)\nheatmap = np.full((n_temps, n_temps), np.nan)\n\nfor key, prob in pairs.items():\n    t1, t2 = key.split('_vs_')\n    i = temperatures.index(float(t1))\n    j = temperatures.index(float(t2))\n    heatmap[i, j] = prob\n    heatmap[j, i] = 1 - prob\n\nnp.fill_diagonal(heatmap, 0.5)\n\nim = axes[1].imshow(heatmap, cmap='RdYlGn', vmin=0, vmax=1, aspect='equal')\naxes[1].set_xticks(range(n_temps))\naxes[1].set_xticklabels([f'{t}' for t in temperatures])\naxes[1].set_yticks(range(n_temps))\naxes[1].set_yticklabels([f'{t}' for t in temperatures])\naxes[1].set_xlabel('Temperature (column)')\naxes[1].set_ylabel('Temperature (row)')\naxes[1].set_title('P(α_row &gt; α_col)')\n\n# Annotate cells\nfor i in range(n_temps):\n    for j in range(n_temps):\n        if not np.isnan(heatmap[i, j]):\n            color = 'white' if heatmap[i, j] &gt; 0.8 or heatmap[i, j] &lt; 0.2 else 'black'\n            axes[1].text(j, i, f'{heatmap[i, j]:.2f}', ha='center', va='center', \n                        fontsize=9, color=color)\n\nplt.colorbar(im, ax=axes[1], shrink=0.8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Summary of the temperature–sensitivity relationship. Left: Posterior medians with 90% credible intervals. Right: Pairwise posterior probabilities P(α_i &gt; α_j) as a heatmap.\n\n\n\n\n\n\n\n0.6.2 Interpreting the Non-Separation at \\(T = 0.3\\) and \\(T = 0.7\\)\nThe inability to distinguish \\(\\alpha\\) at these two temperatures admits several interpretations:\n\nThreshold effect. The relationship between temperature and sensitivity may not be linear. There may be a regime (\\(T \\lesssim 0.7\\)) where the LLM’s choice behavior is relatively stable, with sensitivity declining more sharply only at higher temperatures.\nInsufficient statistical power. With \\(M = 300\\) observations per condition, the study may lack power to resolve small differences in \\(\\alpha\\). The posteriors at \\(T = 0.3\\) and \\(T = 0.7\\) have standard deviations of \\(\\sim 15\\)–\\(19\\), comparable to the difference between their medians (\\(\\sim 1.5\\)).\nGenuine non-monotonicity. It is possible that the true relationship is not strictly monotonic at fine scales, even if the overall trend is negative.\n\nThese interpretations are not mutually exclusive, and the current data do not distinguish among them.\n\n\n0.6.3 Model Adequacy\nThe posterior predictive checks provide no evidence of misfit at any temperature level. This is reassuring but not conclusive—PPC p-values assess whether the model can reproduce summary statistics of the data, not whether the model captures the mechanism by which temperature affects choices. The m_01 model treats each temperature condition as an independent estimation problem; it does not model how temperature enters the choice process.\n\n\n0.6.4 Relation to the Foundational Framework\nThese results can be interpreted through the lens of Report 1:\n\nThe monotonicity property (Theorem 1) tells us that for any fixed value function, higher \\(\\alpha\\) implies higher probability of choosing value-maximizing alternatives. Our finding that \\(\\alpha\\) decreases with temperature thus means the LLM’s choices become less concentrated on EU-maximizing alternatives as temperature increases.\nThe rate of convergence (Theorem 5) tells us that the effect of \\(\\alpha\\) on choice probabilities depends on the gap \\(\\Delta\\) between the best and second-best expected utilities. Problems with larger utility gaps will show temperature effects more readily.\nThe commitment–performance distinction discussed in Report 1 provides a natural framing: if we take EU maximization as the LLM’s “commitment” (the normative standard its training approximates), then temperature controls the performance noise around that commitment."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#sec-reproducibility",
    "href": "applications/temperature_study/01_initial_study.html#sec-reproducibility",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.7 Reproducibility",
    "text": "0.7 Reproducibility\n\n0.7.1 Data Snapshot\nAll results in this report are loaded from a frozen data snapshot in the data/ subdirectory, which is version-controlled. This protects the report from changes if the temperature study pipeline is re-run with different parameters.\nThe snapshot contains:\n\n\n\n\n\n\n\nFile\nDescription\n\n\n\n\nalpha_draws_T*.npz\nPosterior draws of α (4,000 per condition)\n\n\nppc_T*.json\nPosterior predictive check results\n\n\ndiagnostics_T*.txt\nCmdStan diagnostic output\n\n\nstan_data_T*.json\nStan-ready data (for refitting)\n\n\nfit_summary.json\nSummary statistics across conditions\n\n\nprimary_analysis.json\nPre-computed monotonicity and slope statistics\n\n\nrun_summary.json\nPipeline metadata and configuration\n\n\ngrid_results.json\nPrior predictive grid search results\n\n\nstudy_config.yaml\nFrozen copy of the study configuration\n\n\n\n\n\n0.7.2 Refitting from Source\nTo reproduce the Stan fits from the included stan_data_T*.json files:\n\n\nShow code\n# Uncomment to refit from source data (requires CmdStanPy; ~30 min per condition)\n#\n# import cmdstanpy\n# model = cmdstanpy.CmdStanModel(stan_file=\"models/m_01.stan\")\n#\n# for t in [0.0, 0.3, 0.7, 1.0, 1.5]:\n#     key = f\"T{str(t).replace('.', '_')}\"\n#     fit = model.sample(\n#         data=f\"data/stan_data_{key}.json\",\n#         chains=4,\n#         iter_warmup=1000,\n#         iter_sampling=1000,\n#         seed=42,\n#     )\n#     print(f\"T={t}: alpha median = {np.median(fit.stan_variable('alpha')):.1f}\")"
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#next-steps",
    "href": "applications/temperature_study/01_initial_study.html#next-steps",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.8 Next Steps",
    "text": "0.8 Next Steps\nThis initial study establishes that temperature affects estimated SEU sensitivity. Several extensions are planned:\n\nPosition bias and consistency analysis. The data support analyses described in DESIGN.md §6.2–6.4, examining whether position bias or choice inconsistency varies with temperature. These diagnostics may reveal whether temperature affects how the LLM chooses, beyond the summary captured by \\(\\alpha\\).\nAlternative model specifications. The current analysis fits m_01 (structurally identical to m_0) at each temperature independently. A hierarchical extension could model \\(\\alpha(T)\\) as a function of temperature, borrowing strength across conditions.\nReplication with different LLMs. The temperature parameter has model-specific effects. Repeating this study with different architectures would test the generality of the finding.\nRobustness to prior choice. While the prior predictive analysis motivates \\(\\text{Lognormal}(3.0, 0.75)\\), a sensitivity analysis across nearby priors would characterize how much the conclusions depend on this specific choice."
  },
  {
    "objectID": "applications/temperature_study/01_initial_study.html#references",
    "href": "applications/temperature_study/01_initial_study.html#references",
    "title": "Temperature and SEU Sensitivity: Initial Results",
    "section": "0.9 References",
    "text": "0.9 References"
  },
  {
    "objectID": "applications/prompt_framing_study/01_background.html",
    "href": "applications/prompt_framing_study/01_background.html",
    "title": "Prompt Framing and LLM Rationality",
    "section": "",
    "text": "This report series applies the SEU sensitivity framework developed in the Foundations reports to a novel research question: Does prompting an LLM with explicit rationality cues change its decision-making behavior?\nLarge Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks, but their decision-making behavior remains poorly understood. When faced with choices involving uncertain outcomes, do LLMs behave rationally in the sense of maximizing expected utility? And crucially, can we induce more rational behavior through careful prompt engineering?\n\n\n\n\n\n\nNoteCore Research Question\n\n\n\nWhen prompted with explicit rationality cues (e.g., “maximize expected utility”), do LLMs exhibit higher sensitivity (α) to SEU-optimal choices compared to minimal prompts?"
  },
  {
    "objectID": "applications/prompt_framing_study/01_background.html#introduction",
    "href": "applications/prompt_framing_study/01_background.html#introduction",
    "title": "Prompt Framing and LLM Rationality",
    "section": "",
    "text": "This report series applies the SEU sensitivity framework developed in the Foundations reports to a novel research question: Does prompting an LLM with explicit rationality cues change its decision-making behavior?\nLarge Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks, but their decision-making behavior remains poorly understood. When faced with choices involving uncertain outcomes, do LLMs behave rationally in the sense of maximizing expected utility? And crucially, can we induce more rational behavior through careful prompt engineering?\n\n\n\n\n\n\nNoteCore Research Question\n\n\n\nWhen prompted with explicit rationality cues (e.g., “maximize expected utility”), do LLMs exhibit higher sensitivity (α) to SEU-optimal choices compared to minimal prompts?"
  },
  {
    "objectID": "applications/prompt_framing_study/01_background.html#theoretical-framework",
    "href": "applications/prompt_framing_study/01_background.html#theoretical-framework",
    "title": "Prompt Framing and LLM Rationality",
    "section": "0.2 Theoretical Framework",
    "text": "0.2 Theoretical Framework\n\n0.2.1 The SEU Sensitivity Model\nOur foundational model (m_0) characterizes a decision maker’s choice behavior through a softmax rule:\n\\[\nP(\\text{choose } r \\mid \\alpha, \\boldsymbol{\\eta}) = \\frac{\\exp(\\alpha \\cdot \\eta_r)}{\\sum_{j} \\exp(\\alpha \\cdot \\eta_j)}\n\\]\nwhere:\n\n\\(\\eta_r\\) is the expected utility of alternative \\(r\\)\n\\(\\alpha \\geq 0\\) is the sensitivity parameter\n\nThe sensitivity parameter α has a clear interpretation:\n\n\n\nα Value\nInterpretation\n\n\n\n\nα = 0\nRandom choice (uniform distribution)\n\n\nα = 1\nModerate sensitivity to utility differences\n\n\nα → ∞\nDeterministic utility maximization\n\n\n\nBy fitting this model to LLM choice data under different prompt conditions, we can quantify whether rationality framing affects the sensitivity parameter.\n\n\n0.2.2 Prompt Framing Hypothesis\nOur hypothesis is that prompt framing affects α through two potential mechanisms:\n\nAttention mechanism: Explicit rationality cues may focus the model’s attention on decision-relevant features\nRepresentation shift: The same alternatives may be represented differently when embedded in rationality-framed contexts\n\nTo capture the second mechanism, we introduce contextualized embeddings: rather than embedding claims in isolation, we embed them within the full prompt context. This allows the same claim to have different representations under different framings."
  },
  {
    "objectID": "applications/prompt_framing_study/01_background.html#experimental-design",
    "href": "applications/prompt_framing_study/01_background.html#experimental-design",
    "title": "Prompt Framing and LLM Rationality",
    "section": "0.3 Experimental Design",
    "text": "0.3 Experimental Design\n\n0.3.1 Prompt Variants\nWe designed four prompt variants with increasing levels of rationality emphasis:\n\n\n\n\n\n\n\n\nFigure 1: Four levels of rationality emphasis in prompt framing, from minimal task description to explicit decision-theoretic reasoning.\n\n\n\n\n\n\n\n0.3.2 Domain: Insurance Claims Triage\nWe use insurance claims triage as our decision domain:\n\nTask: Select which flagged claim to send for human investigation\nAlternatives: 2-4 claims per decision problem\nOutcomes (K=3):\n\nBoth expert judges agree the selection warrants investigation (utility = 1.0)\nOne judge agrees, one disagrees (utility = 0.5)\nNeither judge agrees (utility = 0.0)\n\n\nThis domain was chosen because:\n\nIt involves genuine uncertainty about outcomes\nThe claims vary in “suspiciousness” (providing natural variation in expected utility)\nThe task is interpretable to LLMs trained on diverse text\n\n\n\n0.3.3 Contextualized Embeddings\nA key methodological innovation is our use of contextualized embeddings. Rather than computing a single embedding for each claim, we compute embeddings within the context of each prompt variant:\nEmbedding(claim | minimal_prompt) ≠ Embedding(claim | maximal_prompt)\nThis captures the hypothesis that the same claim is represented differently when framed with explicit rationality cues. The embedding serves as the feature vector (ψ) in our SEU model."
  },
  {
    "objectID": "applications/prompt_framing_study/01_background.html#study-overview",
    "href": "applications/prompt_framing_study/01_background.html#study-overview",
    "title": "Prompt Framing and LLM Rationality",
    "section": "0.4 Study Overview",
    "text": "0.4 Study Overview\nThe study pipeline consists of:\n\nProblem Generation: Create decision problems by sampling claims\nEmbedding Generation: Compute contextualized embeddings for all variant-claim pairs\nChoice Collection: Present problems to LLM under each framing, collect choices\nModel Fitting: Fit m_0 model separately for each variant\nAnalysis: Compare α estimates across variants\n\n\n\n\n\n\n\nTipKey Insight\n\n\n\nIf prompt framing affects rationality, we expect:\n\nHigher α for maximal/enhanced prompts (more sensitivity to EU-optimal choices)\nLower α for minimal prompts (choices less tied to EU ranking)\n\nA null result (similar α across variants) would suggest LLM decision-making is relatively robust to framing."
  },
  {
    "objectID": "applications/prompt_framing_study/01_background.html#whats-next",
    "href": "applications/prompt_framing_study/01_background.html#whats-next",
    "title": "Prompt Framing and LLM Rationality",
    "section": "0.5 What’s Next",
    "text": "0.5 What’s Next\nIn Report 2: Pilot Study, we present results from our first pilot study, including:\n\nCost estimation and API usage\nModel fitting results for each variant\nDiscovery of methodological challenges (position bias)\nLessons learned for study design refinement"
  },
  {
    "objectID": "legacy/seu_sensitivity_from_theory_to_implementation.html",
    "href": "legacy/seu_sensitivity_from_theory_to_implementation.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "legacy/seu_sensitivity_from_theory_to_implementation.html#abstract-formulation",
    "href": "legacy/seu_sensitivity_from_theory_to_implementation.html#abstract-formulation",
    "title": "",
    "section": "1.1 Abstract Formulation",
    "text": "1.1 Abstract Formulation\n\n1.1.1 Data Structure\nAssume we have descriptions of \\(R\\) alternatives, that these descriptions can be encoded as \\(D\\)-dimensional vectors, and that we have created \\(M\\) decision problems involving various subsets of these descriptions. Data collection consists of presenting the subject with each decision problem \\(m \\in \\{1, \\ldots, M\\}\\) and recording their choice \\(y_m\\) from the \\(N_m\\) available alternatives.\nAfter data collection, we have the following data structure:\n\n\\(M\\): number of decision problems\n\\(K\\): number of possible consequences for each alternative\n\\(D\\): dimensionality of alternative feature vectors\n\\(R\\): number of distinct alternatives\n\\(\\mathbf{W} = \\{\\mathbf{w}_1, \\ldots, \\mathbf{w}_R\\}\\): feature vectors for each alternative, where \\(\\mathbf{w}_r \\in \\mathbb{R}^D\\)\n\\(\\mathbf{I} = (I_{m,r})_{m=1:M,r=1:R}\\): indicator matrix specifying which alternatives appear in which problems\n\\(\\mathbf{y} = (y_1, \\ldots, y_M)\\): observed choices, where \\(y_m \\in \\{1, \\ldots, N_m\\}\\)\n\nThis data structure supports a wide range of experiments. For now, however, we assume that the order of presentation—both across problems and within problems—is irrelevant to our analysis. This is a strong simplification that is surely false in realistic settings, but it serves as a starting point for building intuition about the core model.\nOur use of the term “description” is more suggestive than formal. Formally speaking, the only requirement is that these descriptions can be represented in a given finite-dimensional feature space. On the other hand, it is intended to be suggestive, as we want to consider experiments where subjects are presented with descriptions of various alternatives (e.g., monetary gambles described by probabilities and outcomes, consumer products described by attributes, etc.) and then are asked to choose among them.\n\n\n1.1.2 Model Type\nAssume that the observed choices \\(\\mathbf{y}\\) are generated by a decision maker with utilities \\(\\boldsymbol{\\upsilon}\\) for each of the \\(K\\) possible consequences, a functional procedure for assigning subjective probabilities \\(\\boldsymbol{\\psi}\\) to the consequences of alternatives based on their descriptions, and a positive-valued \\(\\alpha\\) that measures the decision maker’s sensitivity to subjective expected utility (SEU) maximization.\nThe subjective expected utility (SEU) of alternative \\(r\\) is:\n\\[\\eta_r = \\sum_{k=1}^K \\psi_{r,k} \\cdot \\upsilon_k = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon}\\]\nThe probability that the decision maker chooses alternative \\(r\\) from the set of alternatives in problem \\(m\\) is given by a softmax:\n\\[P(\\text{choose } r \\mid m, \\alpha, \\boldsymbol{\\psi}, \\boldsymbol{\\upsilon}) = \\frac{\\exp(\\alpha \\cdot \\eta_r)}{\\sum_{j: I_{m,j}=1} \\exp(\\alpha \\cdot \\eta_j)}\\]\nConditional on the problem \\(m\\) and utilities \\(\\upsilon\\), the following properties of this model type are central to interpreting \\(\\alpha\\) as a measure of sensitivity to SEU maximization:\n\nMonotonicity: The probability of choosing SEU-maximizing alternatives increases as sensitivity increases.\nPerfect Rationality Limit: As \\(\\alpha \\to \\infty\\), choice probabilities concentrate on alternatives that maximize subjective expected utility.\nRandom Choice Limit: As \\(\\alpha \\to 0\\), choice probabilities converge to a uniform distribution over available alternatives.\n\nThese properties follow from the softmax structure and its behavior under scaling. We review the details in the appendix.\nNote that we will need to assume standardized utilities \\(\\boldsymbol{\\upsilon}\\) both for theoretical reasons (in SEU theory, utility is unique only up to positive affine transformations) and for the purpose of identification when fitting a model the this type.\n\n\n1.1.3 Concrete Implementation\nWe now turn to a model of the type described above, as formulated in the Stan program (models/m_0.stan).\nData Block\nThis is just the data structure described above in Stan syntax:\n// filepath: models/m_0.stan (excerpt)\ndata {\n  int&lt;lower=1&gt; M;                    // number of decision problems\n  int&lt;lower=2&gt; K;                    // number of possible consequences\n  int&lt;lower=1&gt; D;                    // number of dimensions to describe an alternative\n  int&lt;lower=2&gt; R;                    // number of distinct alternatives\n  array[R] vector[D] w;              // feature vectors for alternatives\n  array[M,R] int&lt;lower=0,upper=1&gt; I; // indicator matrix\n  array[M] int&lt;lower=1&gt; y;           // observed choices\n}\nParameters Block\nThe Stan program estimates three sets of parameters:\n// filepath: models/m_0.stan (excerpt)\nparameters {\n  real&lt;lower=0&gt; alpha;           // sensitivity parameter\n  matrix[K,D] beta;              // feature-to-probability mapping\n  simplex[K-1] delta;            // utility increments\n}\nbeta parameterizes the mapping from features to subjective probabilities, and delta parameterizes the 0–1 scaling of utilities.\nTransformed Parameters Block\nThe subjective probabilities, utilities, and expected utilities are defined as transformed parameters:\n// filepath: models/m_0.stan (excerpt)\ntransformed parameters {\n  array[sum(N)] simplex[K] psi;  // subjective probabilities\n  ordered[K] upsilon;            // utilities (ordered, on unit scale)\n  vector[sum(N)] eta;            // expected utilities\n  array[M] simplex[max(N)] chi;  // choice probabilities\n  \n  // Calculate subjective probabilities via softmax\n  for (i in 1:sum(N)) {\n    psi[i] = softmax(beta * x[i]);\n  }\n  \n  // Construct ordered utilities from increments\n  upsilon = cumulative_sum(append_row(0, delta));\n  \n  // Calculate expected utility for each alternative\n  for (i in 1:sum(N)) {\n    eta[i] = dot_product(psi[i], upsilon);\n  }\n  \n  // Calculate choice probabilities with sensitivity scaling\n  // ...existing code...\n  chi[i] = append_row(\n    softmax(alpha * problem_eta),\n    rep_vector(0, max(N) - N[i])\n  );\n  // ...existing code...\n}\nRemarks\n\nSubjective Probabilities from Descriptions: We assume that the procedure for assigning subjective probabilities to an alternative’s possible consequences, based on its description, can be represented as a softmax applied to a linear mapping from descriptions to log-odds. This seems like a reasonable starting point, though more complex mappings could be considered in future work.\nUtilities: We parameterize the 0–1-scaled utilities via cumulative sums:\n\n\\(\\boldsymbol{\\upsilon} = \\text{cumsum}([0, \\delta_1, \\ldots, \\delta_{K-1}])\\)\n\\(\\boldsymbol{\\delta} \\sim \\text{Dirichlet}(1, \\ldots, 1)\\) ensures \\(\\sum_i \\delta_i = 1\\), implying \\(\\upsilon_K = 1\\).\n\n\nNote that the Dirichlet prior on \\(\\delta\\) induces a uniform prior over the space of possible utility vectors on the unit scale. Given that the utilities are assumed to represent the decision maker’s preferences, it might make more sense to increase the concentration parameter to get more separation between the alternatives, e.g., for psychological plausibility. Increasing the concentration should also reduce the chance of non-fatal numerical issues when two utilities are very close together, causing Stan’s ordered vector type to complain about the resulting \\(\\upsilon\\) not being strictly ordered.\n\nChoice Probabilities: The softmax choice rule from the abstract model is implemented directly:\n\n\\(P(\\text{choose } r \\mid m, \\alpha, \\psi, \\upsilon) = \\exp(\\alpha \\cdot \\eta_r) / \\sum_j \\exp(\\alpha \\cdot \\eta_j)\\)\n\n\nModel Block\nPrior distributions and the likelihood function complete the Bayesian specification:\n// filepath: models/m_0.stan (excerpt)\nmodel {\n  // Priors\n  alpha ~ lognormal(0, 1);               // sensitivity\n  to_vector(beta) ~ std_normal();        // feature coefficients\n  delta ~ dirichlet(rep_vector(1,K-1));  // utility increments\n  \n  // Likelihood\n  for (i in 1:M) {\n    y[i] ~ categorical(chi[i]);\n  }\n}\nThere was no particular reason for selecting these priors. Of course, the same could be said of other parts of the model, e.g., linearity assumptions, which, as with the choice of priors, were made for mathematical and computational simplicity."
  },
  {
    "objectID": "legacy/seu_sensitivity_from_theory_to_implementation.html#prior-analysis",
    "href": "legacy/seu_sensitivity_from_theory_to_implementation.html#prior-analysis",
    "title": "",
    "section": "1.2 Prior Analysis",
    "text": "1.2 Prior Analysis\nWe now turn to a prior analysis of the m_0.stan program described above (note: we actually run the analysis on the program models/m_0_sim.stan, but this is a purely technical matter related to Stan). The prior predictive analysis samples parameter configurations from the prior, simulates choices under those parameters, and examines the resulting distributions.\n\n1.2.1 Study Design\nFor the present analysis, we will use a study design with the following characteristics (see study_design.json in this directory):\n\nM = 10 decision problems\nK = 3 possible consequences per alternative\nD = 2 feature dimensions\nR = 5 distinct alternatives\nAlternatives per problem: 2 to 4 (mean = 3.2)\nAlternative appearance frequency: 3 to 8 appearances (mean = 6.4)\n\nFeature Vectors (w): The 5 distinct alternatives are characterized by 2-dimensional feature vectors:\n\"w\": [\n  [0.901, -0.404],   // Alternative 1\n  [0.658, -0.658],   // Alternative 2\n  [0.516,  0.396],   // Alternative 3\n  [-0.854, -0.648],  // Alternative 4\n  [-0.341, -0.006]   // Alternative 5\n]\nIndicator Matrix (I): The 10×5 indicator matrix specifies which alternatives appear in which problems. For instance:\n\"I\": [\n  [0, 1, 0, 1, 1],  // Problem 1: alternatives {2, 4, 5}\n  [0, 1, 0, 1, 1],  // Problem 2: alternatives {2, 4, 5}\n  [1, 1, 1, 0, 1],  // Problem 3: alternatives {1, 2, 3, 5}\n  [0, 1, 1, 1, 1],  // Problem 4: alternatives {2, 3, 4, 5}\n  // ... (6 more problems)\n]\n Figure 1: Frequency with which each of the 5 distinct alternatives appears across the 10 decision problems. Alternatives appear between 3-8 times, with alternative 2 appearing most frequently (8 times) and alternatives 1 and 3 appearing least frequently (3 times each).\n Figure 2: Distribution of the number of alternatives in each decision problem.\n\n\n1.2.2 Prior Distribution of Sensitivity Parameter\nThe sensitivity parameter α, which governs how strongly choices reflect SEU maximization, follows a lognormal(0, 1) prior. This prior produces a wide range of sensitivity values:\n Figure 3: Prior distribution of α (sensitivity parameter).\n\n\n1.2.3 Expected Utilities Under the Prior\n Figure 4: Distribution of expected utilities for alternatives across the 10 decision problems. Each box represents the distribution of expected utility values across 1000 parameter samples from the prior. The variation both within and across problems reflects uncertainty in subjective probabilities (determined by the 3×2 β matrix) and utilities (determined by δ, a 2-simplex for the 3 consequences).\n\n\n1.2.4 Simulated Choice Behavior\n Figure 5: Empirical choice frequencies for the 10 decision problems.\n\n\n1.2.5 SEU Maximizer Selection\nWe now turn to an estimate of the probability of selecting an seu maximizer under the prior.\nComputational Implementation: For each simulated decision problem, we track whether the chosen alternative maximizes subjective expected utility. The implementation in m_0_sim.stan performs this check as follows:\n// For each decision problem i with N[i] alternatives:\n// 1. Identify the maximum expected utility\nreal max_eta = max(problem_eta);\n\n// 2. Check if the selected alternative has the maximum expected utility\n//    (within numerical tolerance to handle floating-point comparison)\nif (abs(problem_eta[y[i]] - max_eta) &lt; 1e-10) {\n  selected_seu_max[i] = 1;\n} else {\n  selected_seu_max[i] = 0;\n}\nThis produces two key outputs: - selected_seu_max[i]: Binary indicator for each problem (1 = SEU maximizer selected, 0 = otherwise) - total_seu_max_selected: Sum across all M problems, counting how many times the SEU maximizer was chosen\nThe numerical tolerance (1e-10) accounts for floating-point arithmetic issues when multiple alternatives have nearly identical expected utilities.\n Figure 6: Probability of selecting the SEU-maximizing alternative for each of the 10 decision problems.\n Figure 7: Distribution of the total number of problems (out of 10) where the SEU maximizer was selected across 1000 simulations. The distribution shape and central tendency reveal the range of behaviors implied by the prior on α and the other parameters."
  },
  {
    "objectID": "legacy/seu_sensitivity_from_theory_to_implementation.html#parameter-recovery-analysis",
    "href": "legacy/seu_sensitivity_from_theory_to_implementation.html#parameter-recovery-analysis",
    "title": "",
    "section": "1.3 Parameter Recovery Analysis",
    "text": "1.3 Parameter Recovery Analysis\nHaving examined the prior predictive distribution, we now turn to parameter recovery analysis: can the model reliably recover known parameter values from simulated data? This is crucial for assessing whether our proposed experimental designs can support accurate inference about the sensitivity parameter α and the other model parameters.\n\n1.3.1 Recovery Methodology\nThe parameter recovery analysis proceeds as follows:\n\nData Generation: For each iteration, we sample parameter values from the prior distributions:\n\nα ~ lognormal(0, 1)\nβ[k,d] ~ N(0, 1) for each consequence k and dimension d\nδ ~ Dirichlet(1, …, 1) for the (K-1)-simplex\n\nSimulation: Using these “true” parameters, we simulate choice data according to the model.\nInference: We fit the model to the simulated data using MCMC, obtaining posterior distributions for all parameters.\nEvaluation: We assess recovery quality through:\n\nBias: Mean difference between posterior means and true values\nRMSE: Root mean squared error between estimates and true values\n\nCoverage: Proportion of 90% credible intervals containing the true value\nCI Width: Average width of 90% credible intervals\n\n\n\n\n1.3.2 Recovery Summary Statistics\nThe following table summarizes recovery performance across 20 iterations using our 10-problem study design:\n\n\n\nParameter\nBias\nRMSE\nCoverage\nCI Width\n\n\n\n\nalpha\n0.045\n0.512\n95.0%\n1.891\n\n\nbeta (avg)\n-0.001\n0.571\n91.7%\n2.189\n\n\ndelta (avg)\n0.000\n0.104\n92.5%\n0.401\n\n\n\n\n\n1.3.3 Coverage Diagnostics\nTo understand parameter recovery in detail, we examine the 90% credible intervals for each parameter across all iterations. Green intervals indicate successful coverage (the true value falls within the interval), while red intervals indicate coverage failures.\nAlpha (Sensitivity Parameter)\n Figure 8: 90% credible intervals for α across 20 recovery iterations.\nBeta Parameters (Feature-to-Probability Mapping)\nThe β matrix maps the D=2 dimensional feature vectors to subjective probabilities over K=3 consequences, requiring estimation of 6 parameters.\n Figure 9: Coverage intervals for β[1,1]. This parameter maps the first feature dimension to the log-odds of the first consequence.\n Figure 10: Coverage intervals for β[1,2]. This parameter maps the second feature dimension to the log-odds of the first consequence.\n Figure 11: Coverage intervals for β[2,1]. This parameter maps the first feature dimension to the log-odds of the second consequence.\n Figure 12: Coverage intervals for β[2,2]. This parameter maps the second feature dimension to the log-odds of the second consequence.\n Figure 13: Coverage intervals for β[3,1]. This parameter maps the first feature dimension to the log-odds of the third consequence.\n Figure 14: Coverage intervals for β[3,2]. This parameter maps the second feature dimension to the log-odds of the third consequence.\nDelta Parameters (Utility Increments)\nThe δ parameters represent increments on the unit utility scale, constrained to sum to 1 (as a 2-simplex for K=3 consequences).\n Figure 15: Coverage intervals for δ[1], the utility increment from consequence 1 to consequence 2.\n Figure 16: Coverage intervals for δ[2], the utility increment from consequence 2 to consequence 3.\n\n\n1.3.4 Observations\nIt looks like the \\(\\alpha\\) parameter, as well as the first and third sets of \\(\\beta\\) parameters are showing some evidence of recoverability, while the second set of \\(\\beta\\) parameters and both \\(\\delta\\) parameters look as though are not being informed by the data at all. Perhaps they are being informed slightly, and we just need to consider a larger study design. We can enlarge in several different directions. In the next section, we will consider increasing the number of problems (M) while holding everything else constant to see if that helps."
  },
  {
    "objectID": "legacy/seu_sensitivity_from_theory_to_implementation.html#sample-size-analysis-recovery-vs.-number-of-problems",
    "href": "legacy/seu_sensitivity_from_theory_to_implementation.html#sample-size-analysis-recovery-vs.-number-of-problems",
    "title": "",
    "section": "1.4 Sample Size Analysis: Recovery vs. Number of Problems",
    "text": "1.4 Sample Size Analysis: Recovery vs. Number of Problems\nTo investigate whether the identification issues observed at M=10 are due to insufficient sample size or represent deeper structural problems, we conducted a systematic sample size analysis. Using the same set of alternatives (w) and varying only M and the indicator matrix I, we examined parameter recovery across M ∈ {10, 15, 20, 25, 30}.\n\n1.4.1 Methodology\nFor each value of M:\n\nFixed Design Elements: Used the identical 5 alternatives (w) from the base study design\nVarying Elements: Generated new indicator matrices (I) for M problems, with 2-4 alternatives per problem\nRecovery Iterations: Conducted 50 parameter recovery iterations per M value\nMetrics Tracked:\n\n90% credible interval width\nRoot mean squared error (RMSE)\nMean absolute error (MAE)\nCoverage rate\n\n\nThis design ensures that observed changes in recovery are attributable to M rather than differences in the alternative space.\n\n\n1.4.2 Results: Credible Interval Width vs. M\n Figure 17: Mean 90% credible interval width across parameters as a function of M. Left: \\(\\alpha\\) shows consistent narrowing. Middle: \\(\\beta[1,]\\) and \\(\\beta[3,]\\) narrow with M, while \\(\\beta[2,]\\) stays relatively constant. Right: The \\(\\delta\\) parameters maintain wide intervals across all M values, indicating persistent identification issues.\n\n\n1.4.3 Results: Estimation Error vs. M\n Figure 18: Root mean squared error (RMSE) as a function of M. With the possible exception of \\(\\alpha\\), and possibly \\(\\beta[1,1]\\) and \\(\\beta[3,1]\\), these plots don’t seem to show the error trending downward with increasing M.\n\n\n1.4.4 Alternative Space: Increasing R from 5 to 15\nThe initial sample size analysis used R=5 distinct alternatives. To investigate whether the identification issues stem from an insufficiently rich alternative space rather than sample size alone, we conducted a second analysis with R=15 alternatives while maintaining the same range of M values.\nMethodology: Using a new set of 15 alternatives drawn from the same feature distribution (2-dimensional, standard normal), we repeated the sample size analysis for M ∈ {10, 15, 20, 25, 30}, with 20 recovery iterations per M value. All other aspects of the methodology remained identical to the R=5 analysis.\n Figure 19: Mean 90% credible interval width with R=15 alternatives. The pattern closely mirrors the R=5 results: α and selected β parameters narrow with M, while β[2,·] and δ parameters remain wide.\n Figure 20: RMSE with R=15 alternatives. Again, the patterns are remarkably similar to R=5: no clear downward trend for most parameters except possibly α and β[1,1] and β[3,1].\n\n\n1.4.5 Observations\nThe increased alternative space (R=15) did not substantially improve parameter recovery compared to R=5. The persistent identification issues for certain parameters suggest that the model structure or the information content of the data may be limiting factors, rather than sample size or alternative diversity alone. We consider a slight modification to the model structure in the next section to address these issues."
  },
  {
    "objectID": "legacy/seu_sensitivity_from_theory_to_implementation.html#alternative-approach-informative-prior-on-utilities",
    "href": "legacy/seu_sensitivity_from_theory_to_implementation.html#alternative-approach-informative-prior-on-utilities",
    "title": "",
    "section": "1.5 Alternative Approach: Informative Prior on Utilities",
    "text": "1.5 Alternative Approach: Informative Prior on Utilities\nGiven the persistent identification issues with the δ parameters (utility increments), we explored whether a more informative prior might help. The original m_0 model uses a symmetric Dirichlet(1,1) prior on the (K-1)-simplex for δ, which induces a uniform distribution over the space of possible utility configurations on [0,1].\n\n1.5.1 Model m_01: Strengthened Dirichlet Prior\nWe created model m_01, identical to m_0 except for using delta ~ dirichlet(rep_vector(5, K-1)) instead of dirichlet(rep_vector(1, K-1)). This stronger prior:\n\nConcentrates probability mass around utilities that are more evenly spaced\nFor K=3, encourages the middle utility (upsilon[2]) to be closer to 0.5\nReduces prior uncertainty about utility configurations\n\nPrior Predictive Comparison: Under the Dirichlet(5,5) prior for K=3: - Mean of middle utility: 0.506 (vs 0.500 under Dirichlet(1,1)) - Std of middle utility: 0.150 (vs 0.289 under Dirichlet(1,1))\nThe stronger prior substantially reduces the variance of the middle utility from 0.289 to 0.150—approximately a 48% reduction in standard deviation.\n\n\n1.5.2 Parameter Recovery Results\nWe ran the same 20-iteration parameter recovery analysis on the same 10-problem study design used for m_0.\n Figure 21: 90% credible intervals for δ[1] under model m_01 with Dirichlet(5,5) prior. Coverage = 85.0%. The intervals are noticeably narrower than under m_0, reflecting the stronger prior.\n Figure 22: 90% credible intervals for δ[2] under model m_01 with Dirichlet(5,5) prior. Coverage = 85.0%. Again, intervals are much narrower but the posterior means show no clear relationship to true values.\n\n\n1.5.3 Interpretation\nThe informative prior approach successfully reduced posterior uncertainty but failed to solve the identification problem:\n\nNarrower Intervals: The credible intervals for δ parameters are approximately half as wide as under m_0 (width ~0.50 vs ~0.90), reflecting the informative prior.\nPoor Calibration: Coverage dropped from 92-94% under m_0 to 85%, suggesting the narrower intervals don’t appropriately reflect the data’s information content.\nUnresponsive Posteriors: Most critically, the posterior means show essentially no relationship to the true parameter values. The posteriors are dominated by the prior—they cluster around δ ≈ 0.5 regardless of the true values.\nRegularization vs. Identification: The informative prior acts as a regularizer that reduces variance, but it cannot create information that isn’t in the data. The fundamental issue remains: choice data under this experimental design do not strongly constrain the utility parameters.\n\nThis negative result is important because it rules out “weak priors” as the primary cause of poor δ recovery. Even with a prior that concentrates 68% of its mass in the range [0.35, 0.65] for the middle utility, the data fail to pull the posterior away from this prior concentration.\nThe persistent identification issues across different values of M, R, and prior specifications point toward a more fundamental problem: the confounding between subjective probabilities (ψ, determined by β) and utilities (υ, determined by δ) in the expected utility calculation η = ψ’υ. The model may achieve similar choice probabilities with different (β, δ) combinations, making these parameters empirically indistinguishable from choice data alone."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Informal discussion of ideas, findings, and methodological reflections from the SEU Sensitivity project. For detailed technical content, see the Foundations and Applications reports.\n\n\n\n\n\n\n\n\n\n\nNo matching items\nReuseCC BY-SA 4.0CitationBibTeX citation:@online{helzner2026,\n  author = {Helzner, Jeff},\n  title = {Blog},\n  date = {2026-02-16},\n  url = {https://jeffhelzner.github.io/seu-sensitivity/blog/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHelzner, Jeff. 2026. “Blog.” SEU Sensitivity Project.\nFebruary 16, 2026. https://jeffhelzner.github.io/seu-sensitivity/blog/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SEU Sensitivity",
    "section": "",
    "text": "This project provides a Bayesian framework for modeling and analyzing decision-making behavior through the lens of Subjective Expected Utility (SEU) theory. We develop computational tools for measuring an agent’s sensitivity to SEU maximization—captured by a parameter α that governs how consistently agents maximize expected utility."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "SEU Sensitivity",
    "section": "",
    "text": "This project provides a Bayesian framework for modeling and analyzing decision-making behavior through the lens of Subjective Expected Utility (SEU) theory. We develop computational tools for measuring an agent’s sensitivity to SEU maximization—captured by a parameter α that governs how consistently agents maximize expected utility."
  },
  {
    "objectID": "index.html#report-series",
    "href": "index.html#report-series",
    "title": "SEU Sensitivity",
    "section": "0.2 Report Series",
    "text": "0.2 Report Series\n\n0.2.1 Foundational Reports\nThese reports establish the theoretical and methodological foundations:\n\nAbstract Formulation — Mathematical specification and key theoretical properties\nConcrete Implementation — Stan model implementation details\nPrior Analysis — Prior predictive analysis and prior selection\nParameter Recovery — Validation that parameters can be recovered from data\nAdding Risky Choices — Extension to model m_1 for utility identification\nSBC Validation — Simulation-based calibration results\n\n\n\n0.2.2 Application Reports\n\n0.2.2.1 Temperature Study\n\nInitial Results — How LLM temperature affects estimated SEU sensitivity"
  },
  {
    "objectID": "index.html#key-insights",
    "href": "index.html#key-insights",
    "title": "SEU Sensitivity",
    "section": "0.3 Key Insights",
    "text": "0.3 Key Insights\n\n\n\n\n\n\nNoteThe Sensitivity Parameter α\n\n\n\nThe parameter α has a natural interpretation:\n\nα → 0: Random choice (uniform over alternatives)\nα → ∞: Perfect SEU maximization (deterministic optimal choice)\nIntermediate α: Probabilistic choice with tendency toward higher-SEU alternatives\n\nWith utilities normalized to [0,1], α represents the log-odds change per unit of standardized SEU difference.\n\n\n\n\n\n\n\n\nImportantA Central Finding\n\n\n\nDecisions under uncertainty alone cannot fully identify the utility function—utilities and subjective probabilities are confounded. The foundational reports demonstrate that incorporating risky choices (with known probabilities) resolves this identification problem, following the Anscombe-Aumann approach from classical decision theory."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "SEU Sensitivity",
    "section": "0.4 Getting Started",
    "text": "0.4 Getting Started\nSee the GitHub repository for installation instructions and code."
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "SEU Sensitivity",
    "section": "0.5 Citation",
    "text": "0.5 Citation\nIf you use this work, please cite:\nHelzner, J. (2025). SEU Sensitivity: A Bayesian Framework for Modeling \nDecision-Making Sensitivity. https://github.com/jeffhelzner/seu-sensitivity"
  },
  {
    "objectID": "legacy/theory/m_0_theory.html",
    "href": "legacy/theory/m_0_theory.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#introduction",
    "href": "legacy/theory/m_0_theory.html#introduction",
    "title": "",
    "section": "1.1 1. Introduction",
    "text": "1.1 1. Introduction\nThis document establishes the theoretical foundations for our computational model of epistemic agents. We first prove three fundamental properties of the softmax choice model with respect to an arbitrary value function, then show how these properties apply to our specific case where values are subjective expected utilities (SEU). This approach clarifies that the core choice-theoretic results are independent of how values are constructed, while the SEU interpretation provides the substantive behavioral content."
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#general-softmax-choice-model",
    "href": "legacy/theory/m_0_theory.html#general-softmax-choice-model",
    "title": "",
    "section": "1.2 2. General Softmax Choice Model",
    "text": "1.2 2. General Softmax Choice Model\n\n1.2.1 2.1 Notation and Definitions\nLet: - A = {1, 2, …, K} be a finite set of alternatives - V: A → ℝ be an arbitrary value function assigning real-valued utilities to alternatives - V(j) ∈ ℝ denote the value of alternative j - α ∈ ℝ₊ denote the sensitivity parameter\n\n\n1.2.2 2.2 Softmax Choice Rule\nThe probability that a decision maker selects alternative k ∈ A is given by:\nP(choose k | α, V) = exp(α · V(k)) / Σⱼ∈A exp(α · V(j))\nThis is the Luce choice rule or softmax function.\n\n\n1.2.3 2.3 Optimal Alternatives\nDefine the set of value-maximizing alternatives:\nA* = {j ∈ A : V(j) ≥ V(k) for all k ∈ A}\nLet V* = max{V(j) : j ∈ A} denote the maximum value, and A⁻ = A  A* denote the set of suboptimal alternatives."
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#fundamental-properties-of-softmax-choice",
    "href": "legacy/theory/m_0_theory.html#fundamental-properties-of-softmax-choice",
    "title": "",
    "section": "1.3 3. Fundamental Properties of Softmax Choice",
    "text": "1.3 3. Fundamental Properties of Softmax Choice\nThese properties hold for ANY value function V: A → ℝ.\n\n1.3.1 Property 1: Monotonicity in Sensitivity\nStatement: For any value function V: A → ℝ, holding V fixed: - For k ∈ A* (value-maximizing), P(choose k | α, V) is strictly increasing in α - For j ∉ A* (suboptimal), P(choose j | α, V) is strictly decreasing in α\nProof:\nPart A: Value-maximizing alternatives (k ∈ A)*\nLet k ∈ A* such that V(k) = V*. Taking the derivative with respect to α:\n∂P(k)/∂α = ∂/∂α [exp(α·V(k)) / Z(α)]\nwhere Z(α) = Σⱼ∈A exp(α·V(j)) is the partition function.\nUsing the quotient rule:\n∂P(k)/∂α = [Z(α)·V(k)·exp(α·V(k)) - exp(α·V(k))·Z'(α)] / Z(α)²\n         = P(k)·[V(k) - Z'(α)/Z(α)]\nComputing Z’(α):\nZ'(α) = Σⱼ∈A V(j)·exp(α·V(j))\nTherefore:\nZ'(α)/Z(α) = Σⱼ∈A V(j)·P(j) = 𝔼[V]\nwhere 𝔼[V] is the expected value under the current choice distribution.\nThus:\n∂P(k)/∂α = P(k)·[V(k) - 𝔼[V]] = P(k)·[V* - 𝔼[V]]\nSince V* = max{V(j)} and 𝔼[V] is a weighted average:\n𝔼[V] = Σⱼ∈A P(j)·V(j) ≤ V*\nwith equality only when P(k) = 1 for some k ∈ A* (which occurs only as α → ∞).\nFor any finite α, we have 𝔼[V] &lt; V*, so:\n∂P(k)/∂α = P(k)·[V* - 𝔼[V]] &gt; 0\nPart B: Suboptimal alternatives (j ∉ A)*\nFor j ∉ A, we have V(j) &lt; V. Following the same derivation:\n∂P(j)/∂α = P(j)·[V(j) - 𝔼[V]]\nSince j is suboptimal and A* is non-empty (P(A*) &gt; 0 for all finite α):\n𝔼[V] ≥ P(A*)·V* + P(j)·V(j)\n     &gt; P(A*)·V(j) + P(j)·V(j)    [since V* &gt; V(j)]\n     = V(j)\nTherefore, V(j) - 𝔼[V] &lt; 0, and:\n∂P(j)/∂α = P(j)·[V(j) - 𝔼[V]] &lt; 0\n□\n\n\n1.3.2 Property 2: Perfect Optimization in the Limit (α → ∞)\nStatement: For any value function V: A → ℝ, as α → ∞:\nlim_{α→∞} P(choose k | α, V) = {\n    1/|A*|  if k ∈ A*\n    0       if k ∉ A*\n}\nProof:\nCase 1: k ∈ A (value-maximizing)*\nP(k) = exp(α·V*) / [|A*|·exp(α·V*) + Σⱼ∈A⁻ exp(α·V(j))]\nDividing by exp(α·V*):\nP(k) = 1 / [|A*| + Σⱼ∈A⁻ exp(α·[V(j) - V*])]\nFor j ∈ A⁻, we have V(j) &lt; V, so V(j) - V &lt; 0.\nAs α → ∞:\nexp(α·[V(j) - V*]) → 0  for all j ∈ A⁻\nThus:\nlim_{α→∞} P(k) = 1/|A*|\nCase 2: j ∉ A (suboptimal)*\nP(j) = exp(α·V(j)) / [Σₘ∈A* exp(α·V*) + Σₙ∈A⁻ exp(α·V(n))]\nDividing by exp(α·V*):\nP(j) = exp(α·[V(j) - V*]) / [|A*| + Σₙ∈A⁻ exp(α·[V(n) - V*])]\nSince V(j) - V* &lt; 0: - Numerator → 0 - Denominator ≥ |A*| &gt; 0\nTherefore:\nlim_{α→∞} P(j) = 0\n□\n\n\n1.3.3 Property 3: Uniform Choice in the Limit (α → 0)\nStatement: For any value function V: A → ℝ, as α → 0:\nlim_{α→0} P(choose k | α, V) = 1/|A|  for all k ∈ A\nProof:\nUsing Taylor expansion exp(x) = 1 + x + O(x²):\nP(k) = [1 + α·V(k) + O(α²)] / [Σⱼ∈A (1 + α·V(j) + O(α²))]\n     = [1 + α·V(k) + O(α²)] / [|A| + α·Σⱼ V(j) + O(α²)]\nAs α → 0:\nlim_{α→0} P(k) = 1/|A|\nAlternative proof via logarithms:\nlog P(k) = α·V(k) - log[Σⱼ∈A exp(α·V(j))]\nExpanding the log-sum-exp:\nlog[Σⱼ∈A exp(α·V(j))] = log[|A| + α·Σⱼ V(j) + O(α²)]\n                        = log|A| + (α·Σⱼ V(j))/|A| + O(α²)\nTherefore:\nlog P(k) = α·V(k) - log|A| - (α·Σⱼ V(j))/|A| + O(α²)\n         = -log|A| + α·[V(k) - (Σⱼ V(j))/|A|] + O(α²)\n         → -log|A|  as α → 0\nThus:\nlim_{α→0} P(k) = 1/|A|\n□"
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#application-to-subjective-expected-utility",
    "href": "legacy/theory/m_0_theory.html#application-to-subjective-expected-utility",
    "title": "",
    "section": "1.4 4. Application to Subjective Expected Utility",
    "text": "1.4 4. Application to Subjective Expected Utility\n\n1.4.1 4.1 SEU as a Value Function\nWe now specialize to the case where the value function V is constructed as subjective expected utility:\nLet: - Ω = {ω₁, ω₂, …, ωₙ} be a finite outcome space - υⱼ(ωᵢ) ∈ ℝ denote the utility of outcome ωᵢ under alternative j - ψⱼ(ωᵢ) ∈ [0,1] denote the subjective probability of outcome ωᵢ given alternative j, where Σᵢ ψⱼ(ωᵢ) = 1\nDefine the subjective expected utility function:\nSEU: A → ℝ\nSEU(j) = Σᵢ ψⱼ(ωᵢ)·υⱼ(ωᵢ)\nKey observation: SEU is simply a particular choice of value function V = SEU. Therefore, all three properties proved above apply immediately when we set V(j) = SEU(j).\n\n\n1.4.2 4.2 SEU Maximization Properties\nBy substituting V = SEU into Properties 1-3, we obtain:\nCorollary 1 (Monotonicity for SEU): Holding υ and ψ fixed, higher sensitivity α increases the probability of choosing alternatives that maximize SEU.\nCorollary 2 (Perfect Rationality): As α → ∞, the decision maker chooses SEU-maximizing alternatives with probability 1.\nCorollary 3 (Random Choice): As α → 0, the decision maker chooses uniformly at random, independent of SEU values.\n\n\n1.4.3 4.3 What SEU Adds\nWhile the mathematical properties of softmax choice hold for any value function, the SEU construction provides:\n\nInterpretability: Values decompose into beliefs (ψ) and utilities (υ), allowing separate analysis of epistemic and preference components\nNormative content: SEU maximization is a rationality criterion - Properties 1-3 characterize adherence to this normative standard\nEmpirical predictions: The model predicts that choices will track SEU, not other potential value functions, providing testable restrictions\nParameter identification: With sufficient choice data and variation in alternatives, we can potentially identify ψ and υ separately (not just their product)\n\n\n\n1.4.4 4.4 Scale Invariance and Identification of Sensitivity\nA fundamental property of utility functions in decision theory is that they are unique only up to positive affine transformations. This raises a critical question: how can we meaningfully identify and interpret the sensitivity parameter α?\nTheorem (Scale Invariance): Let υ be a utility function and define a rescaled utility function:\nυ̃(ω) = a·υ(ω) + b  where a &gt; 0\nThen for any alternative j:\nSEU_υ̃(j) = a·SEU_υ(j) + b\nProof:\nSEU_υ̃(j) = Σᵢ ψⱼ(ωᵢ)·υ̃(ωᵢ)\n          = Σᵢ ψⱼ(ωᵢ)·[a·υ(ωᵢ) + b]\n          = a·Σᵢ ψⱼ(ωᵢ)·υ(ωᵢ) + b·Σᵢ ψⱼ(ωᵢ)\n          = a·SEU_υ(j) + b\nInvariance of Choice Probabilities: Under softmax choice, this transformation leaves probabilities unchanged:\nP(j | α, υ̃) = exp(α·SEU_υ̃(j)) / Σₖ exp(α·SEU_υ̃(k))\n             = exp(α·[a·SEU_υ(j) + b]) / Σₖ exp(α·[a·SEU_υ(k) + b])\n             = exp(α·a·SEU_υ(j))·exp(α·b) / [Σₖ exp(α·a·SEU_υ(k))·exp(α·b)]\n             = exp(α·a·SEU_υ(j)) / Σₖ exp(α·a·SEU_υ(k))\n             = P(j | α·a, υ)\nKey Implication: The pair (α, υ) and (α·a, υ̃) generate identical choice probabilities for any a &gt; 0. This means α and the scale of utility are not separately identified from choice data alone.\n\n\n1.4.5 4.5 Resolving the Identification Problem\nTo make α interpretable as “sensitivity to subjective expected utility,” we must fix the scale of utility. Model m_0 achieves this through normalization:\nNormalization Constraint: We constrain utilities to lie in [0,1]:\nυ₁ = 0  and  υₖ = 1\nThis is implemented in m_0 via:\nυ = cumulative_sum([0, δ])  where δ ~ Dirichlet(1,...,1)\nensuring 0 = υ₁ ≤ υ₂ ≤ … ≤ υₖ = 1.\nIdentification Result: Given this normalization, α is identified from choice data as the unique parameter governing sensitivity to differences in subjective expected utility measured on the [0,1] scale.\nFormal Statement: Fix the utility scale by setting min(υ) = 0 and max(υ) = 1. Then:\n\nThe likelihood function P(y | α, ψ, υ) uniquely determines α\nDifferent values of α yield different choice distributions\nα has a clear interpretation: it measures sensitivity to expected utility differences on the unit scale\n\nProof of Identification: Under the normalization υ ∈ [0,1]:\n\nThe range of possible SEU values is bounded: SEU(j) ∈ [0,1] for all j\nThe maximum difference in SEU between any two alternatives is bounded: |SEU(j) - SEU(k)| ≤ 1\nTherefore, α directly controls the log-odds ratio between alternatives:\n\nlog[P(j)/P(k)] = α·[SEU(j) - SEU(k)]\nwhere SEU differences are measured in standardized units.\nSince log-odds ratios are directly observable in choice data (via choice frequencies), and SEU differences are determined by (ψ, υ), the parameter α is identified.\n\n\n1.4.6 4.6 Interpretation of α Under Normalization\nWith utilities normalized to [0,1], α has a precise interpretation:\nα = 1: A one-unit difference in SEU (the maximum possible difference) produces a log-odds ratio of 1, corresponding to:\nP(better)/P(worse) = e ≈ 2.72\nThe better alternative is chosen with probability ≈ 73%.\nα = 2: A one-unit SEU difference produces log-odds of 2:\nP(better)/P(worse) = e² ≈ 7.39\nThe better alternative is chosen with probability ≈ 88%.\nα = 5: A one-unit SEU difference produces log-odds of 5:\nP(better)/P(worse) = e⁵ ≈ 148\nThe better alternative is chosen with probability ≈ 99%.\nGeneral interpretation: α measures the log-odds change per unit of standardized SEU difference. Higher α means choices become more deterministically aligned with SEU rankings.\n\n\n1.4.7 4.7 Why This Matters for Model m_0\nThe normalization and identification results ensure that:\n\nPosterior inferences about α are meaningful: When we infer α ≈ 3 from data, this means the decision maker’s log-odds of choosing between alternatives changes by approximately 3 for each unit difference in normalized SEU.\nCross-study comparability: Two studies using the same normalization can meaningfully compare estimated α values - they measure sensitivity on the same scale.\nPrior specification is interpretable: When we set alpha ~ lognormal(0, 1), we’re placing prior mass on interpretable sensitivity levels relative to the unit scale.\nModel predictions are identifiable: The model makes sharp predictions about choice probabilities given (ψ, υ, α), and these parameters can be separately estimated from sufficiently rich choice data.\n\nWithout normalization: We could only identify the product α·a where a is the unknown utility scale. We couldn’t separately interpret “sensitivity” from “utility scale.”\nWith normalization: We fix a = 1/(max υ - min υ), making α interpretable as sensitivity per unit of standardized SEU difference."
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#model-m_0-specification",
    "href": "legacy/theory/m_0_theory.html#model-m_0-specification",
    "title": "",
    "section": "1.5 5. Model m_0 Specification",
    "text": "1.5 5. Model m_0 Specification\n\n1.5.1 5.1 Constructing SEU from Features\nIn model m_0, we parameterize the components of SEU:\nSubjective probabilities are determined by alternative features x through:\nψⱼ = softmax(β · xⱼ)\nwhere β ∈ ℝ^(K×D) maps D-dimensional features to K outcome probabilities.\nUtilities are ordered with incremental differences:\nυ = cumulative_sum([0, δ])\nwhere δ is a (K-1)-simplex ensuring utilities lie in [0,1] and are strictly ordered.\nSubjective expected utility is then:\nSEU(j) = Σₖ ψⱼₖ · υₖ = ψⱼᵀυ\nChoice probabilities follow:\nP(choose j | α, β, δ, x) = exp(α · SEU(j)) / Σₖ exp(α · SEU(k))\n\n\n1.5.2 5.2 Theoretical Guarantees\nProperties 1-3 ensure that:\n\nPosterior inference on α has a clear interpretation: higher inferred α means choices are more consistent with SEU maximization\nThe model nests both deterministic SEU maximization (α → ∞) and random choice (α → 0) as limiting cases\nIntermediate values of α capture bounded rationality where decision makers are sensitive to SEU differences but make probabilistic choices\n\n\n\n1.5.3 5.3 SEU Maximizer Selection\nAn important diagnostic for understanding model behavior is tracking whether agents select SEU-maximizing alternatives. For each decision problem m, we can define:\nSEU Maximizer Indicator:\nI_m = 1 if chosen alternative j* satisfies η(j*) = max_j η(j)\n     0 otherwise\nwhere η(j) is the expected utility of alternative j.\nExpected SEU Maximizer Selection: Under the softmax choice model with sensitivity α, the probability of selecting an SEU maximizer for problem m is:\nP(select SEU max | m, α) = Σ_{j ∈ A*_m} exp(α·η(j)) / Σ_{k=1}^{N_m} exp(α·η(k))\nwhere A*_m is the set of SEU-maximizing alternatives in problem m.\nTheoretical Properties:\n\nAs α → ∞: P(select SEU max | m, α) → 1 for all m\nAs α → 0: P(select SEU max | m, α) → |A*_m|/N_m (probability under random choice)\nMonotonicity: P(select SEU max | m, α) is strictly increasing in α\n\nAggregate Analysis: The total number of SEU maximizers selected across M problems follows:\nT = Σ_{m=1}^M I_m\nUnder prior predictive analysis, T provides a summary measure of how often the model generates “rational” choices given the prior distributions on parameters."
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#implications-for-rational-choice-theory",
    "href": "legacy/theory/m_0_theory.html#implications-for-rational-choice-theory",
    "title": "",
    "section": "1.6 6. Implications for Rational Choice Theory",
    "text": "1.6 6. Implications for Rational Choice Theory\n\n1.6.1 6.1 Generality of Results\nThe fact that Properties 1-3 hold for any value function V reveals an important insight: these properties characterize the softmax choice rule itself, not the specific theory of value.\nThis means: - The monotonicity, limiting behavior, and convergence properties are structural features of softmax choice - They would hold equally for risk-neutral expected value, prospect theory values, or any other value construction - The choice of SEU as our value function is a substantive theoretical commitment about what drives behavior\n\n\n1.6.2 6.2 SEU as a Rational Standard\nBy choosing V = SEU, we commit to SEU maximization as our rationality criterion. This commitment:\n\nAligns with classical Bayesian decision theory (Savage, 1954)\nProvides a normative benchmark for evaluating choice behavior\nMakes our parameter α interpretable as “degree of rationality” relative to this specific standard\n\n\n\n1.6.3 6.3 Alternative Value Functions\nOur framework could accommodate other value functions: - Expected value: V(j) = Σᵢ ψⱼ(ωᵢ)·ωᵢ (objective outcomes, no utilities) - Prospect theory: V(j) = Σᵢ w(ψⱼ(ωᵢ))·v(υⱼ(ωᵢ)) (probability weighting, reference dependence) - Regret theory: V(j) = f(υⱼ, max_k υₖ) (comparative evaluation)\nEach would satisfy Properties 1-3, but yield different substantive predictions about choice behavior."
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#technical-notes",
    "href": "legacy/theory/m_0_theory.html#technical-notes",
    "title": "",
    "section": "1.7 7. Technical Notes",
    "text": "1.7 7. Technical Notes\n\n1.7.1 7.1 Uniqueness of Maximum\nWhen |A*| = 1 (unique maximum), Property 2 shows deterministic optimal choice as α → ∞.\nWhen |A| &gt; 1 (multiple optima), the limiting distribution is uniform over A, representing rational indifference between equally valued alternatives.\n\n\n1.7.2 7.2 Rate of Convergence\n\nProperty 2 (α → ∞): Convergence is exponential with rate Δ = min{V* - V(j) : j ∉ A*}\nProperty 3 (α → 0): Convergence is polynomial (first-order in α)\n\n\n\n1.7.3 7.3 Numerical Implementation\nFor computational stability: - Large α: Use log-sum-exp trick: log(Σⱼ exp(xⱼ)) = max(x) + log(Σⱼ exp(xⱼ - max(x))) - Small α: Taylor expansion may provide better accuracy than direct evaluation\n\n\n1.7.4 7.4 Connection to Information Theory\nThe softmax choice model can be derived as the maximum entropy distribution subject to the constraint 𝔼[V] = c, revealing deep connections to information theory and statistical mechanics."
  },
  {
    "objectID": "legacy/theory/m_0_theory.html#references",
    "href": "legacy/theory/m_0_theory.html#references",
    "title": "",
    "section": "1.8 8. References",
    "text": "1.8 8. References\nSoftmax/Luce choice: - Luce, R. D. (1959). Individual Choice Behavior: A Theoretical Analysis - McFadden, D. (1973). Conditional logit analysis of qualitative choice behavior\nQuantal response: - McKelvey, R. D., & Palfrey, T. R. (1995). Quantal response equilibria for normal form games\nSubjective expected utility: - Savage, L. J. (1954). The Foundations of Statistics - Anscombe, F. J., & Aumann, R. J. (1963). A definition of subjective probability\nInformation theory connection: - Jaynes, E. T. (1957). Information theory and statistical mechanics - Cover, T. M., & Thomas, J. A. (2006). Elements of Information Theory"
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "",
    "text": "This report presents results from our first pilot study investigating whether prompt framing affects LLM rationality. Following the experimental design outlined in Report 1, we collected choice data from GPT-4 under four prompting conditions and fit the m_0 model to estimate the sensitivity parameter α for each condition.\n\n\n\n\n\n\nImportantKey Finding\n\n\n\nWhile initial results suggested the maximal prompting condition yields higher α (greater sensitivity to EU-optimal choices), deeper analysis revealed a position bias confound that complicates interpretation. This discovery highlights the importance of careful experimental design when studying LLM decision-making."
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#introduction",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#introduction",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "",
    "text": "This report presents results from our first pilot study investigating whether prompt framing affects LLM rationality. Following the experimental design outlined in Report 1, we collected choice data from GPT-4 under four prompting conditions and fit the m_0 model to estimate the sensitivity parameter α for each condition.\n\n\n\n\n\n\nImportantKey Finding\n\n\n\nWhile initial results suggested the maximal prompting condition yields higher α (greater sensitivity to EU-optimal choices), deeper analysis revealed a position bias confound that complicates interpretation. This discovery highlights the importance of careful experimental design when studying LLM decision-making."
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#study-configuration",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#study-configuration",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.2 Study Configuration",
    "text": "0.2 Study Configuration\n\n\nShow code\n# Load study metadata\nwith open(data_dir / \"run_metadata.json\", 'r') as f:\n    metadata = json.load(f)\n\nconfig = metadata[\"config\"]\ncost_estimate = metadata[\"cost_estimate\"]\n\nprint(\"Study Configuration\")\nprint(\"=\" * 50)\nprint(f\"  Decision problems: {config['num_problems']}\")\nprint(f\"  Claims (alternatives): 20\")\nprint(f\"  Alternatives per problem: {config['min_alternatives']}-{config['max_alternatives']}\")\nprint(f\"  Prompt variants: {metadata['num_variants']}\")\nprint(f\"  LLM model: {config['llm_model']}\")\nprint(f\"  Temperature: {config['temperature']}\")\nprint(f\"  Embedding model: {config['embedding_model']}\")\nprint(f\"  Embedding dimensions (reduced): {config['target_dim']}\")\n\n\nStudy Configuration\n==================================================\n  Decision problems: 100\n  Claims (alternatives): 20\n  Alternatives per problem: 2-4\n  Prompt variants: 4\n  LLM model: gpt-4\n  Temperature: 0.7\n  Embedding model: text-embedding-3-small\n  Embedding dimensions (reduced): 32\n\n\n\n0.2.1 Cost Summary\n\n\nShow code\nprint(\"\\nAPI Cost Summary\")\nprint(\"=\" * 50)\nprint(f\"  Embedding calls: {cost_estimate['embedding_costs']['num_embeddings']}\")\nprint(f\"  Choice collection calls: {cost_estimate['choice_collection_costs']['num_api_calls']}\")\nprint(f\"  Estimated cost: ${cost_estimate['total_estimated_cost_usd']:.2f}\")\nprint(f\"  (Actual cost was slightly lower due to conservative token estimates)\")\n\n\n\nAPI Cost Summary\n==================================================\n  Embedding calls: 80\n  Choice collection calls: 400\n  Estimated cost: $8.04\n  (Actual cost was slightly lower due to conservative token estimates)"
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#model-fitting-results",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#model-fitting-results",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.3 Model Fitting Results",
    "text": "0.3 Model Fitting Results\nWe fit the m_0 model separately to choice data from each prompt variant using CmdStan with 4 chains × 1000 samples after 1000 warmup iterations.\n\n\nShow code\nmodel_fits = metadata[\"model_fits\"]\n\n# Create results dataframe\nresults_df = pd.DataFrame([\n    {\n        \"Variant\": name.capitalize(),\n        \"Level\": [\"Minimal\", \"Baseline\", \"Enhanced\", \"Maximal\"].index(name.capitalize()),\n        \"α Mean\": fits[\"alpha_mean\"],\n        \"α Std\": fits[\"alpha_std\"],\n        \"α Median\": fits[\"alpha_median\"],\n        \"95% CI Lower\": fits[\"alpha_q05\"],\n        \"95% CI Upper\": fits[\"alpha_q95\"],\n    }\n    for name, fits in model_fits.items()\n]).sort_values(\"Level\")\n\nprint(\"\\nPosterior Estimates for α (Sensitivity Parameter)\")\nprint(\"=\" * 70)\nprint(results_df.to_string(index=False))\n\n\n\nPosterior Estimates for α (Sensitivity Parameter)\n======================================================================\n Variant  Level   α Mean    α Std  α Median  95% CI Lower  95% CI Upper\n Minimal      0 1.373356 1.081509  1.069134      0.202286      3.517987\nBaseline      1 0.933639 0.690754  0.760721      0.193103      2.247144\nEnhanced      2 1.166089 0.981266  0.880530      0.176981      3.153686\n Maximal      3 3.294146 2.168126  3.077448      0.397442      7.200530\n\n\n\n\nShow code\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Colors matching our variant scheme\ncolors = {\"Minimal\": \"#E69F00\", \"Baseline\": \"#56B4E9\", \n          \"Enhanced\": \"#009E73\", \"Maximal\": \"#CC79A7\"}\n\nvariants = results_df[\"Variant\"].tolist()\nx = np.arange(len(variants))\nmeans = results_df[\"α Mean\"].values\nci_lower = results_df[\"95% CI Lower\"].values\nci_upper = results_df[\"95% CI Upper\"].values\n\n# Error bars (90% CI)\nyerr = np.array([means - ci_lower, ci_upper - means])\n\nbars = ax.bar(x, means, yerr=yerr, capsize=8, \n              color=[colors[v] for v in variants],\n              alpha=0.8, edgecolor='black', linewidth=1.5)\n\n# Add value labels\nfor i, (bar, mean, upper) in enumerate(zip(bars, means, ci_upper)):\n    ax.text(bar.get_x() + bar.get_width()/2, upper + 0.2,\n            f'{mean:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\n# Reference line\nax.axhline(y=1.0, color='gray', linestyle='--', linewidth=1.5, \n           label='α=1 (moderate sensitivity)')\n\nax.set_xlabel('Prompt Variant (Rationality Emphasis →)', fontsize=12)\nax.set_ylabel('Estimated α (Sensitivity Parameter)', fontsize=12)\nax.set_title('SEU Sensitivity by Prompt Framing Condition', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(variants)\nax.legend(loc='upper left')\nax.set_ylim(0, max(ci_upper) * 1.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Estimated sensitivity parameter α across prompt variants. Error bars show 90% credible intervals. The dashed line at α=1 represents moderate sensitivity; higher values indicate stronger preference for EU-optimal choices.\n\n\n\n\n\n\n0.3.1 Posterior Predictive Checks\nBefore interpreting the α estimates, we assess model fit using posterior predictive checks (PPCs). These checks compare observed data to data simulated from the posterior predictive distribution using three test statistics:\n\nLog-likelihood: Overall model fit\nModal accuracy: Whether the model’s most likely prediction matches observed choices\n\nSum of chosen probabilities: Probability calibration\n\nA posterior p-value near 0.5 indicates good calibration; values near 0 or 1 suggest potential misspecification.\n\n\nShow code\n# Load PPC results\nppc_results = {}\nfor variant in [\"minimal\", \"baseline\", \"enhanced\", \"maximal\"]:\n    ppc_file = data_dir / \"ppc\" / variant / \"ppc_summary.json\"\n    if ppc_file.exists():\n        with open(ppc_file, 'r') as f:\n            ppc_results[variant] = json.load(f)\n\nif ppc_results:\n    # Create summary table\n    ppc_df = pd.DataFrame([\n        {\n            \"Variant\": name.capitalize(),\n            \"Log-lik p\": res[\"p_values\"][\"ll\"],\n            \"Modal p\": res[\"p_values\"][\"modal\"],\n            \"Prob p\": res[\"p_values\"][\"prob\"],\n        }\n        for name, res in ppc_results.items()\n    ])\n    \n    print(\"Posterior Predictive Check Summary\")\n    print(\"=\" * 60)\n    print(ppc_df.to_string(index=False))\n    print(\"\\nInterpretation: p ≈ 0.5 indicates good fit\")\n    print(\"               p &lt; 0.05 or p &gt; 0.95 indicates potential misfit\")\n\n\nPosterior Predictive Check Summary\n============================================================\n Variant  Log-lik p  Modal p  Prob p\n Minimal     0.4770   0.5075  0.4635\nBaseline     0.5925   0.5725  0.5680\nEnhanced     0.5205   0.5195  0.5050\n Maximal     0.3660   0.3920  0.3550\n\nInterpretation: p ≈ 0.5 indicates good fit\n               p &lt; 0.05 or p &gt; 0.95 indicates potential misfit\n\n\n\n\nShow code\nif ppc_results:\n    from matplotlib.colors import LinearSegmentedColormap\n    \n    # Create data matrix\n    variants_order = [\"minimal\", \"baseline\", \"enhanced\", \"maximal\"]\n    statistics = [\"ll\", \"modal\", \"prob\"]\n    stat_labels = [\"Log-likelihood\", \"Modal accuracy\", \"Chosen probability\"]\n    \n    data = np.array([\n        [ppc_results[v][\"p_values\"][s] for v in variants_order if v in ppc_results]\n        for s in statistics\n    ])\n    \n    fig, ax = plt.subplots(figsize=(10, 4))\n    \n    # Custom colormap: red near 0/1, green near 0.5\n    colors = ['#d73027', '#fee08b', '#1a9850', '#fee08b', '#d73027']\n    positions = [0, 0.25, 0.5, 0.75, 1.0]\n    cmap = LinearSegmentedColormap.from_list('ppc', list(zip(positions, colors)))\n    \n    im = ax.imshow(data, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n    cbar = fig.colorbar(im, ax=ax, label='Posterior p-value')\n    \n    # Add threshold lines to colorbar\n    cbar.ax.axhline(y=0.05, color='black', linewidth=1, linestyle='--')\n    cbar.ax.axhline(y=0.95, color='black', linewidth=1, linestyle='--')\n    \n    # Labels\n    ax.set_yticks(range(len(statistics)))\n    ax.set_yticklabels(stat_labels)\n    ax.set_xticks(range(len([v for v in variants_order if v in ppc_results])))\n    ax.set_xticklabels([v.capitalize() for v in variants_order if v in ppc_results])\n    \n    # Add value annotations\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            val = data[i, j]\n            text_color = 'white' if (val &lt; 0.15 or val &gt; 0.85) else 'black'\n            ax.text(j, i, f'{val:.2f}', ha='center', va='center', \n                   color=text_color, fontsize=11, fontweight='bold')\n    \n    ax.set_title('Posterior Predictive Check Summary\\n(p ≈ 0.5 indicates good fit)', \n                 fontsize=14, fontweight='bold')\n    ax.set_xlabel('Prompt Variant', fontsize=12)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\nFigure 2: Posterior predictive check p-values visualized as a heatmap. Green indicates good fit (p ≈ 0.5), red indicates potential misfit (p near 0 or 1).\n\n\n\n\n\n\n\n\n\n\n\nNoteModel Fit Assessment\n\n\n\nAll posterior p-values fall in the acceptable range (0.1–0.9), indicating that the m_0 model provides an adequate fit to the choice data across all prompt conditions. This supports interpreting the α estimates as meaningful measures of sensitivity—the model is not grossly misspecified in a way that would invalidate these interpretations.\n\n\n\n\n0.3.2 Initial Interpretation\nAt first glance, the results appear to support our hypothesis:\n\nMinimal, Baseline, Enhanced: α estimates around 0.9-1.4 (near random to moderate sensitivity)\nMaximal: α ≈ 3.3 with 90% CI [0.4, 7.2] — substantially higher\n\nThis would suggest that explicit decision-theoretic framing (the maximal condition) induces more rational choice behavior. However, examining the raw choice data revealed a potential confound."
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#position-bias-analysis",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#position-bias-analysis",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.4 Position Bias Analysis",
    "text": "0.4 Position Bias Analysis\n\n0.4.1 Observing the Pattern\nWhen we examined the distribution of choice positions, a striking pattern emerged:\n\n\nShow code\n# Load problems and choices\nwith open(data_dir / \"problems.json\", 'r') as f:\n    problems_data = json.load(f)\nproblems = problems_data[\"problems\"]\n\nwith open(data_dir / \"raw_choices.json\", 'r') as f:\n    choices = json.load(f)\n\n\n\n\nShow code\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nvariants_list = [\"minimal\", \"baseline\", \"enhanced\", \"maximal\"]\ncolors = {\"minimal\": \"#E69F00\", \"baseline\": \"#56B4E9\", \n          \"enhanced\": \"#009E73\", \"maximal\": \"#CC79A7\"}\n\nfor i, variant in enumerate(variants_list):\n    ax = axes[i]\n    \n    # Count positions\n    positions = [c[\"choice_1indexed\"] for c in choices[variant][\"choices\"]]\n    counts = Counter(positions)\n    total = len(positions)\n    \n    x = sorted(counts.keys())\n    y = [counts[k] for k in x]\n    pcts = [counts[k]/total*100 for k in x]\n    \n    bars = ax.bar(x, y, color=colors[variant], alpha=0.8, edgecolor='black')\n    \n    # Add percentage labels\n    for bar, pct in zip(bars, pcts):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                f'{pct:.0f}%', ha='center', va='bottom', fontsize=10)\n    \n    ax.set_xlabel('Choice Position', fontsize=11)\n    ax.set_ylabel('Count', fontsize=11)\n    ax.set_title(f'{variant.capitalize()} Variant', fontsize=12, fontweight='bold')\n    ax.set_xticks(x)\n\nplt.suptitle('Choice Position Distributions by Prompt Variant', \n             fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Distribution of choice positions by prompt variant. The maximal condition shows a strong preference for Position 1 (51%) compared to other conditions (21-27%).\n\n\n\n\n\n\n\n\n\n\n\nWarningKey Observation\n\n\n\nThe maximal condition shows a 51% preference for Position 1, compared to only 21-27% for other conditions. All conditions show roughly similar patterns for positions 2-4.\n\n\n\n\n0.4.2 Disentangling Position vs. Content\nThe critical question is whether this position preference reflects:\n\nTrue position bias: The maximal prompt induces a preference for “Claim 1” regardless of content\nContent-position correlation: Claims in position 1 happen to be more suspicious, and maximal is better at detecting this\nInteraction effect: Some combination of both\n\nTo investigate, we analyzed choice rates for specific claims when they appeared in Position 1 versus other positions:\n\n\nShow code\ndef analyze_claim_position_rates(claim_id, problems, choices, variants_list):\n    \"\"\"Analyze choice rates for a claim by position and variant.\"\"\"\n    results = []\n    \n    for variant in variants_list:\n        pos1_shown = pos1_chosen = 0\n        other_shown = other_chosen = 0\n        \n        for choice_data in choices[variant][\"choices\"]:\n            pid = choice_data[\"problem_id\"]\n            choice_idx = choice_data[\"choice\"]\n            problem = next(p for p in problems if p[\"id\"] == pid)\n            \n            if claim_id in problem[\"claim_ids\"]:\n                claim_pos = problem[\"claim_ids\"].index(claim_id)\n                if claim_pos == 0:\n                    pos1_shown += 1\n                    if choice_idx == 0:\n                        pos1_chosen += 1\n                else:\n                    other_shown += 1\n                    if choice_idx == claim_pos:\n                        other_chosen += 1\n        \n        pos1_rate = pos1_chosen/pos1_shown*100 if pos1_shown &gt; 0 else np.nan\n        other_rate = other_chosen/other_shown*100 if other_shown &gt; 0 else np.nan\n        \n        results.append({\n            \"Variant\": variant.capitalize(),\n            \"Pos1\": f\"{pos1_chosen}/{pos1_shown}\",\n            \"Pos1 Rate\": pos1_rate,\n            \"Other\": f\"{other_chosen}/{other_shown}\",\n            \"Other Rate\": other_rate,\n            \"Difference\": pos1_rate - other_rate if not np.isnan(pos1_rate) and not np.isnan(other_rate) else np.nan\n        })\n    \n    return pd.DataFrame(results)\n\n# Analyze key claims\nprint(\"Position Effect Analysis for Selected Claims\")\nprint(\"=\" * 70)\nprint(\"\\nC007 (Life insurance - undisclosed heart condition):\")\nprint(\"  This claim is highly suspicious and chosen ~100% regardless of position\")\ndf_c007 = analyze_claim_position_rates(\"C007\", problems, choices, variants_list)\nprint(df_c007.to_string(index=False))\n\n\nPosition Effect Analysis for Selected Claims\n======================================================================\n\nC007 (Life insurance - undisclosed heart condition):\n  This claim is highly suspicious and chosen ~100% regardless of position\n Variant Pos1  Pos1 Rate Other  Other Rate  Difference\n Minimal  4/4      100.0 13/13  100.000000    0.000000\nBaseline  4/4      100.0 13/13  100.000000    0.000000\nEnhanced  4/4      100.0 13/13  100.000000    0.000000\n Maximal  4/4      100.0 12/13   92.307692    7.692308\n\n\n\n\nShow code\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\nC003 (Business interruption - fire suppression issue):\")\nprint(\"  Shows strong position bias in maximal condition\")\ndf_c003 = analyze_claim_position_rates(\"C003\", problems, choices, variants_list)\nprint(df_c003.to_string(index=False))\n\n\n\n======================================================================\n\nC003 (Business interruption - fire suppression issue):\n  Shows strong position bias in maximal condition\n Variant Pos1  Pos1 Rate Other  Other Rate  Difference\n Minimal 1/12   8.333333  3/12   25.000000  -16.666667\nBaseline 1/12   8.333333  4/12   33.333333  -25.000000\nEnhanced 1/12   8.333333  4/12   33.333333  -25.000000\n Maximal 7/12  58.333333  3/12   25.000000   33.333333\n\n\n\n\nShow code\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\\nC009 (Hail damage - well-documented legitimate claim):\")\nprint(\"  Other variants never choose it; maximal chooses it frequently\")\ndf_c009 = analyze_claim_position_rates(\"C009\", problems, choices, variants_list)\nprint(df_c009.to_string(index=False))\n\n\n\n======================================================================\n\nC009 (Hail damage - well-documented legitimate claim):\n  Other variants never choose it; maximal chooses it frequently\n Variant Pos1  Pos1 Rate Other  Other Rate  Difference\n Minimal 0/12   0.000000   0/8         0.0    0.000000\nBaseline 0/12   0.000000   0/8         0.0    0.000000\nEnhanced 0/12   0.000000   0/8         0.0    0.000000\n Maximal 7/12  58.333333   4/8        50.0    8.333333\n\n\n\n\n0.4.3 Interpretation of Position Analysis\nThe analysis reveals a nuanced picture:\nC007 (Life Insurance Claim): All variants choose this claim ~100% of the time regardless of position. This is genuinely a suspicious claim (undisclosed pre-existing heart condition), and the consistent selection suggests all variants can identify clear fraud indicators.\nC003 (Business Interruption): The maximal condition shows a +33 percentage point boost when this claim is in Position 1 versus other positions. Other variants show negative or smaller effects. This suggests a position-content interaction in the maximal condition.\nC009 (Hail Damage): This is a well-documented, legitimate claim. Other variants appropriately never select it (0%). But maximal selects it 50-58% of the time—this appears to be position bias rather than content-based selection.\n\n\nShow code\n# Calculate average position effect for each variant\nposition_effects = []\n\nfor variant in variants_list:\n    effects = []\n    for claim_id in [f\"C{i:03d}\" for i in range(1, 21)]:  # All 20 claims\n        df = analyze_claim_position_rates(claim_id, problems, choices, [variant])\n        if not np.isnan(df[\"Difference\"].values[0]):\n            effects.append(df[\"Difference\"].values[0])\n    \n    position_effects.append({\n        \"Variant\": variant.capitalize(),\n        \"Mean Position Effect\": np.mean(effects),\n        \"Std\": np.std(effects),\n        \"N Claims\": len(effects)\n    })\n\neffects_df = pd.DataFrame(position_effects)\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nx = np.arange(len(effects_df))\ncolors_list = [colors[v.lower()] for v in effects_df[\"Variant\"]]\nbars = ax.bar(x, effects_df[\"Mean Position Effect\"], \n              yerr=effects_df[\"Std\"]/np.sqrt(effects_df[\"N Claims\"]),\n              color=colors_list, alpha=0.8, edgecolor='black', capsize=5)\n\nax.axhline(y=0, color='gray', linestyle='-', linewidth=1)\nax.set_xlabel('Prompt Variant', fontsize=12)\nax.set_ylabel('Position 1 Advantage (percentage points)', fontsize=12)\nax.set_title('Average Position Bias by Prompt Condition', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(effects_df[\"Variant\"])\n\n# Add value labels\nfor bar in bars:\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, height + 1,\n            f'{height:+.1f}%', ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Position bias effect by prompt variant. Shows the difference in choice rate when claims appear in Position 1 vs. other positions, averaged across claims. The maximal condition shows substantially larger position effects."
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#robustness-analysis",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#robustness-analysis",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.5 Robustness Analysis",
    "text": "0.5 Robustness Analysis\nWe also examined the robustness of our embedding methodology:\n\n\nShow code\nwith open(data_dir / \"robustness_analysis.json\", 'r') as f:\n    robustness = json.load(f)\n\npca_results = robustness[\"pca_sensitivity\"][\"analysis\"]\n\nprint(\"PCA Dimension Sensitivity\")\nprint(\"=\" * 50)\nprint(\"Explained variance by target dimension:\")\nfor dim_key, dim_data in pca_results.items():\n    dim = dim_key.replace(\"dim_\", \"\")\n    avg_var = np.mean([v.get(\"explained_variance\", 0) for v in dim_data.values() if v.get(\"explained_variance\")])\n    print(f\"  D={dim}: {avg_var*100:.1f}% variance explained (avg across variants)\")\n\n\nPCA Dimension Sensitivity\n==================================================\nExplained variance by target dimension:\n  D=16: 93.9% variance explained (avg across variants)\n  D=32: 100.0% variance explained (avg across variants)\n  D=64: 100.0% variance explained (avg across variants)\n  D=128: 100.0% variance explained (avg across variants)\n\n\n\n\n\n\n\n\nNoteRobustness Finding\n\n\n\nWith only 20 claims, PCA is limited to 20 dimensions maximum (achieving 100% variance). The 16-dimensional reduction captures ~94% of embedding variance, suggesting our 32-dimensional target is more than sufficient for this pilot scale."
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#discussion",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#discussion",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.6 Discussion",
    "text": "0.6 Discussion\n\n0.6.1 What We Learned\n\nInitial support for hypothesis: The maximal condition does show higher estimated α, consistent with rationality framing improving EU sensitivity\nPosition bias confound: However, the maximal condition also exhibits strong position bias (51% position 1 preference vs. ~25% for others), which may inflate the α estimate\nInteraction effects: The position effect varies by claim content, suggesting a complex interaction between framing, position, and claim characteristics\n\n\n\n0.6.2 Methodological Implications\nThe discovery of position bias has important implications for study design:\n\n\n\n\n\n\nImportantDesign Recommendation\n\n\n\nFuture studies should counterbalance claim positions across problems. Each claim should appear equally often in each position, allowing us to:\n\nEstimate position bias directly\nSeparate content-based sensitivity from position effects\n\nTest whether the framing × position interaction is significant\n\n\n\n\n\n0.6.3 Limitations of This Pilot\n\nSmall claim pool: Only 20 claims limits generalizability\nNo position counterbalancing: Claims are not balanced across positions\nSingle LLM: Only GPT-4 tested; effects may differ across models\nNo repetitions: Each problem presented once per condition (no within-condition reliability estimate)"
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#conclusion",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#conclusion",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.7 Conclusion",
    "text": "0.7 Conclusion\nThis pilot study provides preliminary evidence that prompt framing affects LLM decision-making behavior, with explicit rationality cues potentially increasing sensitivity to EU-optimal choices. However, the discovery of position bias in the maximal condition complicates interpretation and highlights the need for more careful experimental design.\nThe position bias finding is itself interesting: it suggests that explicit decision-theoretic framing may activate different processing modes in LLMs—modes that may include heuristics like “the first option is likely correct.” Understanding and controlling for such biases will be essential for rigorous study of LLM rationality.\n\n\n\n\n\n\nTipNext Steps\n\n\n\nIn Report 3, we will:\n\nImplement position-counterbalanced problem generation\nExpand the claim pool for better generalizability\n\nAdd within-condition repetitions to estimate reliability\nPotentially test multiple LLMs for cross-model comparison"
  },
  {
    "objectID": "applications/prompt_framing_study/02_pilot_study_1.html#appendix-claim-descriptions",
    "href": "applications/prompt_framing_study/02_pilot_study_1.html#appendix-claim-descriptions",
    "title": "Pilot Study 1: Initial Results and Methodological Discoveries",
    "section": "0.8 Appendix: Claim Descriptions",
    "text": "0.8 Appendix: Claim Descriptions\nFor reference, the 20 claims used in this pilot study:\n\n\nShow claims data\nwith open(data_dir / \"claims.json\", 'r') as f:\n    claims_data = json.load(f)\n\nfor claim in claims_data[\"claims\"][:10]:  # Show first 10\n    print(f\"\\n{claim['id']}:\")\n    print(f\"  {claim['description'][:200]}...\")\n    \nprint(\"\\n... (10 more claims omitted for brevity)\")\n\n\n\nC001:\n  A homeowner filed a claim for water damage to their basement, stating that a pipe burst during a cold snap. The claim includes $15,000 for flooring replacement and $8,000 for damaged furniture. Photos...\n\nC002:\n  An auto insurance claim for a rear-end collision in a shopping center parking lot. The claimant states they were stationary when hit, but parking lot surveillance is unavailable. Repair estimate is $4...\n\nC003:\n  A business interruption claim from a restaurant owner following a kitchen fire. The claim requests $50,000 for three weeks of lost revenue. Fire department report confirms grease fire but notes that f...\n\nC004:\n  A health insurance claim for an emergency room visit following a sports injury. The patient reports twisting their ankle during a recreational soccer game. Medical records show treatment for a spraine...\n\nC005:\n  A theft claim for a stolen laptop and jewelry from a home. The policyholder reports returning from vacation to find items missing. Police report filed but no signs of forced entry noted. Claimed items...\n\nC006:\n  A workers compensation claim from a warehouse employee reporting back pain from lifting heavy boxes. The injury allegedly occurred three weeks before reporting. Medical examination shows mild strain c...\n\nC007:\n  A life insurance claim following the death of a 45-year-old policyholder from cardiac arrest. The policy was purchased 18 months ago with a $500,000 benefit. Medical history provided during applicatio...\n\nC008:\n  A property damage claim for a tree falling on a detached garage during a storm. Weather records confirm high winds that day. The claim is for $25,000 to rebuild the garage. Inspection photos show sign...\n\nC009:\n  An auto claim for hail damage to a vehicle. The claimant reports their car was parked outside during a hailstorm. Damage assessment shows numerous small dents consistent with hail. Repair estimate is ...\n\nC010:\n  A disability insurance claim from a software developer claiming repetitive strain injury prevents them from working. Medical documentation shows carpal tunnel syndrome diagnosis. The claimant is seeki...\n\n... (10 more claims omitted for brevity)"
  },
  {
    "objectID": "foundations/01_abstract_formulation.html",
    "href": "foundations/01_abstract_formulation.html",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "",
    "text": "This report establishes the theoretical foundations for the intended interpretation of the models that are discussed in subsequent reports. We derive three properties of the softmax choice model with respect to an arbitrary value function, then show how these properties apply specifically when values are subjective expected utilities (SEU). While mathematically straightforward, these properties are essential to our intended interpretation: when value is taken to be subjective expected utility, \\(\\alpha\\) can be interpreted as measuring a decision maker’s alignment with the normative standard of SEU rationality—how consistently their choices track the SEU ranking.\nThis separation clarifies an important conceptual point: the core choice-theoretic results are independent of how values are constructed—they follow from the structure of softmax choice alone. The SEU interpretation provides the substantive behavioral content and connects our model to classical decision theory (Savage 1954; Neumann and Morgenstern 1947).\n\n\n\n\n\n\nNoteWhy This Matters\n\n\n\nUnderstanding these properties is essential for interpreting the sensitivity parameter α:\n\nFor practitioners: α has a precise meaning as log-odds change per unit of standardized utility difference\nFor theorists: The properties follow from any choice model of this functional form—a softmax transformation of utilities scaled by α—independent of the particular theory of value adopted"
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#introduction",
    "href": "foundations/01_abstract_formulation.html#introduction",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "",
    "text": "This report establishes the theoretical foundations for the intended interpretation of the models that are discussed in subsequent reports. We derive three properties of the softmax choice model with respect to an arbitrary value function, then show how these properties apply specifically when values are subjective expected utilities (SEU). While mathematically straightforward, these properties are essential to our intended interpretation: when value is taken to be subjective expected utility, \\(\\alpha\\) can be interpreted as measuring a decision maker’s alignment with the normative standard of SEU rationality—how consistently their choices track the SEU ranking.\nThis separation clarifies an important conceptual point: the core choice-theoretic results are independent of how values are constructed—they follow from the structure of softmax choice alone. The SEU interpretation provides the substantive behavioral content and connects our model to classical decision theory (Savage 1954; Neumann and Morgenstern 1947).\n\n\n\n\n\n\nNoteWhy This Matters\n\n\n\nUnderstanding these properties is essential for interpreting the sensitivity parameter α:\n\nFor practitioners: α has a precise meaning as log-odds change per unit of standardized utility difference\nFor theorists: The properties follow from any choice model of this functional form—a softmax transformation of utilities scaled by α—independent of the particular theory of value adopted"
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#general-softmax-choice-model",
    "href": "foundations/01_abstract_formulation.html#general-softmax-choice-model",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.2 General Softmax Choice Model",
    "text": "0.2 General Softmax Choice Model\n\n0.2.1 Notation and Definitions\nWe begin with abstract notation for the general softmax choice model, then show how it specializes to SEU. The notation is chosen to align with our Stan implementations.\n\n\n\n\n\n\nNoteNotation Summary (Abstract Model)\n\n\n\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(\\mathcal{R} = \\{1, 2, \\ldots, R\\}\\)\nSet of distinct alternatives\n\n\n\\(N_m\\)\nNumber of alternatives available in problem \\(m\\)\n\n\n\\(V: \\mathcal{R} \\to \\mathbb{R}\\)\nValue function assigning utilities to alternatives\n\n\n\\(V(r)\\)\nValue of alternative \\(r\\)\n\n\n\\(\\alpha \\in \\mathbb{R}_+\\)\nSensitivity parameter (non-negative)\n\n\n\nNotation used in proofs: We write \\(\\mathcal{R}^* = \\{r : V(r) = V^*\\}\\) for the set of value-maximizing alternatives, where \\(V^* = \\max_r V(r)\\), and \\(\\mathcal{R}^- = \\mathcal{R} \\setminus \\mathcal{R}^*\\) for suboptimal alternatives.\n\n\n\n\n0.2.2 The Softmax Choice Rule\nThe probability that a decision maker selects alternative \\(r\\) from a choice set is given by the softmax rule:\n\\[\nP(\\text{choose } r \\mid \\alpha, V) = \\frac{\\exp(\\alpha \\cdot V(r))}{\\sum_{j \\in \\mathcal{R}} \\exp(\\alpha \\cdot V(j))}\n\\tag{1}\\]\nThis functional form has historical precedents in several research traditions. Luce (Luce 1959) derived a ratio-scale choice rule from axiomatic foundations, working with abstract “scale values” rather than utilities per se. McFadden (McFadden 1974) arrived at the same functional form from random utility theory in econometrics. The rule also appears in statistical mechanics as the Boltzmann distribution, where the sensitivity parameter is called inverse temperature. We adopt the term “softmax” from machine learning, where this transformation is ubiquitous.\nThe sensitivity parameter \\(\\alpha\\) controls how deterministically choices track value differences.\nIn our Stan implementations, this corresponds to chi[m] = softmax(alpha * eta_m) where eta_m contains the expected utilities of alternatives available in problem \\(m\\).\nThis rule has several appealing properties:\n\nProbabilistic: Assigns positive probability to all alternatives\nMonotonic in value: Higher-value alternatives are more likely to be chosen\nParameterized sensitivity: α controls how sharply choices concentrate on high-value alternatives\n\n\n\nShow code\n# Demonstrate softmax with varying alpha\nvalues = np.array([0.2, 0.5, 0.8])\nalphas = np.linspace(0.01, 10, 100)\n\nprobs = np.array([softmax(a * values) for a in alphas])\n\nfig, ax = plt.subplots(figsize=(8, 5))\nfor i, label in enumerate(['η=0.2 (low)', 'η=0.5 (medium)', 'η=0.8 (high)']):\n    ax.plot(alphas, probs[:, i], label=label, linewidth=2)\n\nax.axhline(y=1/3, color='gray', linestyle='--', alpha=0.5, label='Uniform (α→0)')\nax.set_xlabel('Sensitivity (α)', fontsize=12)\nax.set_ylabel('Choice Probability χ', fontsize=12)\nax.set_title('Softmax Choice Probabilities', fontsize=14)\nax.legend()\nax.set_ylim(0, 1)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Softmax choice probabilities for three alternatives with values η = (0.2, 0.5, 0.8) as sensitivity α varies. In the m_0 model, these values represent expected utilities computed as η = ψᵀυ."
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#fundamental-properties-of-softmax-choice",
    "href": "foundations/01_abstract_formulation.html#fundamental-properties-of-softmax-choice",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.3 Fundamental Properties of Softmax Choice",
    "text": "0.3 Fundamental Properties of Softmax Choice\nThe following three properties hold for any value function \\(V: \\mathcal{R} \\to \\mathbb{R}\\). This generality is important—the properties follow from any choice model of this functional form, not any particular theory of value.\n\n0.3.1 Property 1: Monotonicity in Sensitivity\n\n\n\n\n\n\nNoteTheorem 1 (Monotonicity)\n\n\n\nFor any value function \\(V: \\mathcal{R} \\to \\mathbb{R}\\), holding \\(V\\) fixed:\n\nFor \\(r \\in \\mathcal{R}^*\\) (value-maximizing): \\(P(\\text{choose } r \\mid \\alpha, V)\\) is strictly increasing in \\(\\alpha\\)\nFor \\(r \\notin \\mathcal{R}^*\\) (suboptimal): \\(P(\\text{choose } r \\mid \\alpha, V)\\) is strictly decreasing in \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\nTipProof of Theorem 1\n\n\n\n\n\nPart A: Value-maximizing alternatives (\\(r \\in \\mathcal{R}^*\\))\nLet \\(r \\in \\mathcal{R}^*\\) such that \\(V(r) = V^*\\). Define the partition function: \\[\nZ(\\alpha) = \\sum_{j \\in \\mathcal{R}} \\exp(\\alpha \\cdot V(j))\n\\]\nTaking the derivative of \\(P(r)\\) with respect to \\(\\alpha\\): \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left[\\frac{\\exp(\\alpha \\cdot V(r))}{Z(\\alpha)}\\right]\n\\]\nUsing the quotient rule: \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = \\frac{Z(\\alpha) \\cdot V(r) \\cdot \\exp(\\alpha \\cdot V(r)) - \\exp(\\alpha \\cdot V(r)) \\cdot Z'(\\alpha)}{Z(\\alpha)^2}\n\\]\nSimplifying: \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = P(r) \\cdot \\left[V(r) - \\frac{Z'(\\alpha)}{Z(\\alpha)}\\right]\n\\]\nComputing \\(Z'(\\alpha)\\): \\[\nZ'(\\alpha) = \\sum_{j \\in \\mathcal{R}} V(j) \\cdot \\exp(\\alpha \\cdot V(j))\n\\]\nTherefore: \\[\n\\frac{Z'(\\alpha)}{Z(\\alpha)} = \\sum_{j \\in \\mathcal{R}} V(j) \\cdot P(j) = \\mathbb{E}[V]\n\\]\nwhere \\(\\mathbb{E}[V]\\) is the expected value under the current choice distribution.\nThus: \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = P(r) \\cdot [V^* - \\mathbb{E}[V]]\n\\]\nSince \\(V^* = \\max_j V(j)\\) and \\(\\mathbb{E}[V]\\) is a weighted average: \\[\n\\mathbb{E}[V] = \\sum_{j \\in \\mathcal{R}} P(j) \\cdot V(j) \\leq V^*\n\\]\nwith equality only when \\(P(r) = 1\\) for some \\(r \\in \\mathcal{R}^*\\) (which occurs only as \\(\\alpha \\to \\infty\\)).\nFor any finite \\(\\alpha\\), all alternatives receive positive probability under the softmax rule (since \\(\\exp(x) &gt; 0\\) for all real \\(x\\)), ensuring that both optimal and suboptimal alternatives contribute to the expectation. Hence \\(\\mathbb{E}[V] &lt; V^*\\) strictly, so: \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = P(r) \\cdot [V^* - \\mathbb{E}[V]] &gt; 0 \\quad \\blacksquare\n\\]\nPart B: Suboptimal alternatives (\\(r \\notin \\mathcal{R}^*\\))\nFor \\(r \\notin \\mathcal{R}^*\\), we have \\(V(r) &lt; V^*\\). Following the same derivation: \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = P(r) \\cdot [V(r) - \\mathbb{E}[V]]\n\\]\nSince \\(r\\) is suboptimal and \\(\\mathcal{R}^*\\) is non-empty (so \\(P(\\mathcal{R}^*) &gt; 0\\) for all finite \\(\\alpha\\)): \\[\n\\mathbb{E}[V] \\geq P(\\mathcal{R}^*) \\cdot V^* + P(r) \\cdot V(r) &gt; P(\\mathcal{R}^*) \\cdot V(r) + P(r) \\cdot V(r)\n\\]\nThe strict inequality follows because \\(V^* &gt; V(r)\\).\nTherefore, \\(V(r) - \\mathbb{E}[V] &lt; 0\\), and: \\[\n\\frac{\\partial P(r)}{\\partial \\alpha} = P(r) \\cdot [V(r) - \\mathbb{E}[V]] &lt; 0 \\quad \\blacksquare\n\\]\n\n\n\n\n\n0.3.2 Property 2: Perfect Optimization Limit\n\n\n\n\n\n\nNoteTheorem 2 (Convergence to Value Maximization)\n\n\n\nFor any value function \\(V: \\mathcal{R} \\to \\mathbb{R}\\), as \\(\\alpha \\to \\infty\\):\n\\[\n\\lim_{\\alpha \\to \\infty} P(\\text{choose } r \\mid \\alpha, V) =\n\\begin{cases}\n1/|\\mathcal{R}^*| & \\text{if } r \\in \\mathcal{R}^* \\\\\n0 & \\text{if } r \\notin \\mathcal{R}^*\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\nTipProof of Theorem 2\n\n\n\n\n\nCase 1: \\(r \\in \\mathcal{R}^*\\) (value-maximizing)\n\\[\nP(r) = \\frac{\\exp(\\alpha \\cdot V^*)}{|\\mathcal{R}^*| \\cdot \\exp(\\alpha \\cdot V^*) + \\sum_{j \\in \\mathcal{R}^-} \\exp(\\alpha \\cdot V(j))}\n\\]\nDividing numerator and denominator by \\(\\exp(\\alpha \\cdot V^*)\\): \\[\nP(r) = \\frac{1}{|\\mathcal{R}^*| + \\sum_{j \\in \\mathcal{R}^-} \\exp(\\alpha \\cdot [V(j) - V^*])}\n\\]\nFor \\(j \\in \\mathcal{R}^-\\), we have \\(V(j) &lt; V^*\\), so \\(V(j) - V^* &lt; 0\\).\nAs \\(\\alpha \\to \\infty\\): \\[\n\\exp(\\alpha \\cdot [V(j) - V^*]) \\to 0 \\quad \\text{for all } j \\in \\mathcal{R}^-\n\\]\nThus: \\[\n\\lim_{\\alpha \\to \\infty} P(r) = \\frac{1}{|\\mathcal{R}^*|} \\quad \\blacksquare\n\\]\nCase 2: \\(r \\notin \\mathcal{R}^*\\) (suboptimal)\n\\[\nP(r) = \\frac{\\exp(\\alpha \\cdot V(r))}{\\sum_{s \\in \\mathcal{R}^*} \\exp(\\alpha \\cdot V^*) + \\sum_{j \\in \\mathcal{R}^-} \\exp(\\alpha \\cdot V(j))}\n\\]\nDividing by \\(\\exp(\\alpha \\cdot V^*)\\): \\[\nP(r) = \\frac{\\exp(\\alpha \\cdot [V(r) - V^*])}{|\\mathcal{R}^*| + \\sum_{j \\in \\mathcal{R}^-} \\exp(\\alpha \\cdot [V(j) - V^*])}\n\\]\nSince \\(V(r) - V^* &lt; 0\\):\n\nNumerator \\(\\to 0\\)\nDenominator \\(\\geq |\\mathcal{R}^*| &gt; 0\\)\n\nTherefore: \\[\n\\lim_{\\alpha \\to \\infty} P(r) = 0 \\quad \\blacksquare\n\\]\n\n\n\n\n\n0.3.3 Property 3: Random Choice Limit\n\n\n\n\n\n\nNoteTheorem 3 (Convergence to Uniform Choice)\n\n\n\nFor any value function \\(V: \\mathcal{R} \\to \\mathbb{R}\\), as \\(\\alpha \\to 0\\):\n\\[\n\\lim_{\\alpha \\to 0} P(\\text{choose } r \\mid \\alpha, V) = \\frac{1}{|\\mathcal{R}|} \\quad \\text{for all } r \\in \\mathcal{R}\n\\]\n\n\n\n\n\n\n\n\nTipProof of Theorem 3\n\n\n\n\n\nUsing Taylor expansion \\(\\exp(x) = 1 + x + O(x^2)\\):\n\\[\nP(r) = \\frac{1 + \\alpha \\cdot V(r) + O(\\alpha^2)}{\\sum_{j \\in \\mathcal{R}} (1 + \\alpha \\cdot V(j) + O(\\alpha^2))}\n\\]\n\\[\n= \\frac{1 + \\alpha \\cdot V(r) + O(\\alpha^2)}{|\\mathcal{R}| + \\alpha \\cdot \\sum_j V(j) + O(\\alpha^2)}\n\\]\nAs \\(\\alpha \\to 0\\): \\[\n\\lim_{\\alpha \\to 0} P(r) = \\frac{1}{|\\mathcal{R}|} \\quad \\blacksquare\n\\]\nAlternative proof via logarithms:\n\\[\n\\log P(r) = \\alpha \\cdot V(r) - \\log\\left[\\sum_{j \\in \\mathcal{R}} \\exp(\\alpha \\cdot V(j))\\right]\n\\]\nExpanding the log-sum-exp: \\[\n\\log\\left[\\sum_{j \\in \\mathcal{R}} \\exp(\\alpha \\cdot V(j))\\right] = \\log|\\mathcal{R}| + \\frac{\\alpha \\cdot \\sum_j V(j)}{|\\mathcal{R}|} + O(\\alpha^2)\n\\]\nTherefore: \\[\n\\log P(r) = -\\log|\\mathcal{R}| + \\alpha \\cdot \\left[V(r) - \\frac{\\sum_j V(j)}{|\\mathcal{R}|}\\right] + O(\\alpha^2)\n\\]\nAs \\(\\alpha \\to 0\\): \\(\\log P(r) \\to -\\log|\\mathcal{R}|\\), hence \\(P(r) \\to 1/|\\mathcal{R}|\\). \\(\\blacksquare\\)\n\n\n\n\n\n0.3.4 Summary: The Three Properties\n\n\nShow code\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Values for a 3-alternative problem\nvalues = np.array([0.3, 0.5, 0.9])  # Third is optimal\n\n# Property 1: Monotonicity\nalphas = np.linspace(0.01, 8, 100)\nprobs = np.array([softmax(a * values) for a in alphas])\n\naxes[0].plot(alphas, probs[:, 2], 'b-', linewidth=2.5, label='Optimal (η=0.9)')\naxes[0].plot(alphas, probs[:, 1], 'orange', linewidth=2, label='Middle (η=0.5)')\naxes[0].plot(alphas, probs[:, 0], 'r-', linewidth=2, label='Low (η=0.3)')\naxes[0].axhline(y=1/3, color='gray', linestyle='--', alpha=0.5)\naxes[0].set_xlabel('α', fontsize=12)\naxes[0].set_ylabel('χ (choice probability)', fontsize=12)\naxes[0].set_title('Property 1: Monotonicity', fontsize=12, fontweight='bold')\naxes[0].legend(fontsize=9)\naxes[0].set_ylim(0, 1)\naxes[0].grid(True, alpha=0.3)\n\n# Property 2: α → ∞\nalpha_large = 20\nprobs_large = softmax(alpha_large * values)\naxes[1].bar(['η=0.3', 'η=0.5', 'η=0.9'], probs_large, color=['red', 'orange', 'blue'])\naxes[1].axhline(y=1, color='blue', linestyle='--', alpha=0.5, label='Limit')\naxes[1].set_ylabel('χ (choice probability)', fontsize=12)\naxes[1].set_title('Property 2: α → ∞\\n(Deterministic Optimal)', fontsize=12, fontweight='bold')\naxes[1].set_ylim(0, 1.1)\naxes[1].grid(True, alpha=0.3, axis='y')\n\n# Property 3: α → 0\nalpha_small = 0.01\nprobs_small = softmax(alpha_small * values)\naxes[2].bar(['η=0.3', 'η=0.5', 'η=0.9'], probs_small, color=['red', 'orange', 'blue'])\naxes[2].axhline(y=1/3, color='gray', linestyle='--', alpha=0.5, label='Uniform')\naxes[2].set_ylabel('χ (choice probability)', fontsize=12)\naxes[2].set_title('Property 3: α → 0\\n(Uniform Random)', fontsize=12, fontweight='bold')\naxes[2].set_ylim(0, 0.5)\naxes[2].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Visual summary of the three fundamental properties. Left: Monotonicity—optimal alternative probability increases with α. Middle: Limiting behavior at α→∞ (deterministic optimal choice). Right: Limiting behavior at α→0 (uniform random choice)."
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#application-to-subjective-expected-utility",
    "href": "foundations/01_abstract_formulation.html#application-to-subjective-expected-utility",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.4 Application to Subjective Expected Utility",
    "text": "0.4 Application to Subjective Expected Utility\nWe now specialize the general softmax framework to the case where values are subjective expected utilities (SEU).\n\n0.4.1 SEU as a Value Function\n\n\n\n\n\n\nNoteNotation Summary (SEU Specialization)\n\n\n\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(K\\)\nNumber of possible consequences (outcomes)\n\n\n\\(\\boldsymbol{\\upsilon} \\in \\mathbb{R}^K\\)\nUtility vector over consequences\n\n\n\\(\\boldsymbol{\\psi}_r \\in \\Delta^{K-1}\\)\nSubjective probability distribution over consequences for alternative \\(r\\)\n\n\n\\(\\eta_r = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon}\\)\nExpected utility of alternative \\(r\\)\n\n\n\\(\\boldsymbol{\\chi}_m\\)\nChoice probability vector for problem \\(m\\)\n\n\n\n\n\n\n\n\n\n\n\nNoteDefinition: Subjective Expected Utility\n\n\n\nEach alternative \\(r\\) is associated with:\n\nSubjective probabilities \\(\\boldsymbol{\\psi}_r \\in \\Delta^{K-1}\\) over \\(K\\) consequences\nAn expected utility computed as: \\[\n\\eta_r = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon} = \\sum_{k=1}^{K} \\psi_{r,k} \\cdot \\upsilon_k\n\\]\n\nThe choice probability for alternative \\(r\\) in problem \\(m\\) is then: \\[\n\\chi_{m,r} = \\frac{\\exp(\\alpha \\cdot \\eta_r)}{\\sum_{j: I_{m,j}=1} \\exp(\\alpha \\cdot \\eta_j)}\n\\]\nwhere \\(I_{m,r} = 1\\) indicates that alternative \\(r\\) is available in problem \\(m\\).\n\n\nKey observation: The expected utility \\(\\eta_r\\) serves as our value function \\(V(r) = \\eta_r\\). Therefore, all three properties proved above apply immediately.\n\n\n0.4.2 Corollaries for SEU\nBy substituting \\(V(r) = \\eta_r = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon}\\) into Properties 1-3:\n\n\n\n\n\n\nNoteCorollary 1 (Monotonicity for SEU)\n\n\n\nHolding utilities \\(\\boldsymbol{\\upsilon}\\) and beliefs \\(\\boldsymbol{\\psi}\\) fixed, higher sensitivity \\(\\alpha\\) increases the probability of choosing alternatives that maximize expected utility \\(\\eta\\).\n\n\n\n\n\n\n\n\nNoteCorollary 2 (Perfect Rationality)\n\n\n\nAs \\(\\alpha \\to \\infty\\), the decision maker chooses uniformly among SEU-maximizing alternatives (those with highest \\(\\eta\\)) with probability 1. When there is a unique maximizer, choice becomes deterministic.\n\n\n\n\n\n\n\n\nNoteCorollary 3 (Random Choice)\n\n\n\nAs \\(\\alpha \\to 0\\), the decision maker chooses uniformly at random over available alternatives, independent of \\(\\eta\\) values.\n\n\n\n\n0.4.3 What SEU Adds to the Framework\nWhile the mathematical properties of softmax choice hold for any value function, the SEU construction provides:\n\nDecomposition: Expected utilities \\(\\eta\\) decompose into beliefs (\\(\\boldsymbol{\\psi}\\)) and utilities (\\(\\boldsymbol{\\upsilon}\\)), allowing separate analysis of epistemic and preference components\nNormative content: SEU maximization is a rationality criterion—Properties 1-3 characterize adherence to this normative standard (Savage 1954)\nEmpirical predictions: The model predicts that choices will track \\(\\eta\\), providing testable restrictions"
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#scale-invariance-and-representation",
    "href": "foundations/01_abstract_formulation.html#scale-invariance-and-representation",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.5 Scale Invariance and Representation",
    "text": "0.5 Scale Invariance and Representation\n\n0.5.1 The Representation Problem\nA fundamental property of utility functions is that they are unique only up to positive affine transformations. This raises a critical question: how can we meaningfully interpret \\(\\alpha\\) when the scale of utility is arbitrary?\n\n\n\n\n\n\nNoteTheorem 4 (Scale Invariance)\n\n\n\nLet \\(\\boldsymbol{\\upsilon}\\) be a utility vector and define a rescaled utility: \\[\n\\tilde{\\upsilon}_k = a \\cdot \\upsilon_k + b \\quad \\text{where } a &gt; 0\n\\]\nThen:\n\n\\(\\tilde{\\eta}_r = a \\cdot \\eta_r + b\\) for all alternatives \\(r\\)\n\\(P(\\text{choose } r \\mid \\alpha, \\tilde{\\boldsymbol{\\upsilon}}) = P(\\text{choose } r \\mid \\alpha \\cdot a, \\boldsymbol{\\upsilon})\\)\n\nThe pair \\((\\alpha, \\boldsymbol{\\upsilon})\\) and \\((\\alpha \\cdot a, \\tilde{\\boldsymbol{\\upsilon}})\\) generate identical choice probabilities.\n\n\n\n\n\n\n\n\nTipProof of Theorem 4\n\n\n\n\n\nPart 1: \\[\n\\tilde{\\eta}_r = \\sum_k \\psi_{r,k} \\cdot [a \\cdot \\upsilon_k + b]\n= a \\cdot \\sum_k \\psi_{r,k} \\cdot \\upsilon_k + b \\cdot \\sum_k \\psi_{r,k}\n= a \\cdot \\eta_r + b\n\\]\nsince \\(\\sum_k \\psi_{r,k} = 1\\).\nPart 2: \\[\nP(\\text{choose } r \\mid \\alpha, \\tilde{\\boldsymbol{\\upsilon}}) = \\frac{\\exp(\\alpha \\cdot [a \\cdot \\eta_r + b])}{\\sum_j \\exp(\\alpha \\cdot [a \\cdot \\eta_j + b])}\n\\]\n\\[\n= \\frac{\\exp(\\alpha a \\cdot \\eta_r) \\cdot \\exp(\\alpha b)}{\\sum_j \\exp(\\alpha a \\cdot \\eta_j) \\cdot \\exp(\\alpha b)}\n= \\frac{\\exp(\\alpha a \\cdot \\eta_r)}{\\sum_j \\exp(\\alpha a \\cdot \\eta_j)}\n= P(\\text{choose } r \\mid \\alpha a, \\boldsymbol{\\upsilon}) \\quad \\blacksquare\n\\]\n\n\n\nKey Implication: Without fixing the utility scale, \\(\\alpha\\) and the scale of utility are confounded—they cannot be separately interpreted from choice behavior alone. Scaling utilities by a factor \\(a\\) is equivalent to scaling sensitivity by \\(1/a\\).\n\n\n0.5.2 Resolution: Utility Standardization\nTo make \\(\\alpha\\) interpretable as “sensitivity to expected utility differences,” we adopt a standard standardization convention.\n\n\n\n\n\n\nImportantStandardization Convention\n\n\n\nWe constrain utilities to lie in \\([0,1]\\) by assigning:\n\n\\(\\upsilon_{\\text{worst}} = 0\\) (utility of the worst consequence)\n\\(\\upsilon_{\\text{best}} = 1\\) (utility of the best consequence)\n\nFor \\(K\\) ordered consequences, this means: \\[\n0 = \\upsilon_1 \\leq \\upsilon_2 \\leq \\cdots \\leq \\upsilon_K = 1\n\\]\nThis is the standard standardization in decision theory, where the utility function is anchored at the endpoints of the consequence space.\n\n\nThis standardization is without loss of generality—it simply fixes a representation from the equivalence class of utility functions related by positive affine transformations. Any utility function can be rescaled to satisfy this convention.\nInterpretive Result: Given this standardization, \\(\\alpha\\) measures sensitivity to expected utility differences on a standardized scale where the full range of possible utilities spans exactly one unit.\n\n\n0.5.3 Interpretation of α Under Standardization\nWith utilities standardized to \\([0,1]\\), expected utilities satisfy \\(\\eta_r \\in [0,1]\\) for all alternatives \\(r\\) (since \\(\\eta_r\\) is a convex combination of utilities). The maximum possible difference in expected utility is therefore 1.\nThe sensitivity parameter \\(\\alpha\\) has a precise interpretation via the log-odds ratio:\n\\[\n\\log\\left[\\frac{\\chi_{r}}{\\chi_{s}}\\right] = \\alpha \\cdot [\\eta_r - \\eta_s]\n\\]\n\n\nShow code\nimport pandas as pd\n\nalpha_vals = [0.5, 1, 2, 3, 5, 10]\ndata = []\nfor a in alpha_vals:\n    odds_ratio = np.exp(a)\n    prob_better = odds_ratio / (1 + odds_ratio)\n    data.append({\n        'α': a,\n        'Log-odds': f'{a:.1f}',\n        'Odds ratio': f'{odds_ratio:.2f}',\n        'P(higher η)': f'{prob_better:.1%}'\n    })\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\nTable 1: Interpretation of α for a one-unit expected utility difference (maximum possible difference with standardized utilities).\n\n\n\n\n\n\n\n\n\n\nα\nLog-odds\nOdds ratio\nP(higher η)\n\n\n\n\n0\n0.5\n0.5\n1.65\n62.2%\n\n\n1\n1.0\n1.0\n2.72\n73.1%\n\n\n2\n2.0\n2.0\n7.39\n88.1%\n\n\n3\n3.0\n3.0\n20.09\n95.3%\n\n\n4\n5.0\n5.0\n148.41\n99.3%\n\n\n5\n10.0\n10.0\n22026.47\n100.0%\n\n\n\n\n\n\n\n\n\n\nGeneral interpretation: \\(\\alpha\\) measures the log-odds change per unit of expected utility difference. Higher \\(\\alpha\\) means choices become more deterministically aligned with \\(\\eta\\) rankings."
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#rates-of-convergence",
    "href": "foundations/01_abstract_formulation.html#rates-of-convergence",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.6 Rates of Convergence",
    "text": "0.6 Rates of Convergence\nThe limiting behavior established in Properties 2 and 3 occurs at different rates, which we now characterize precisely.\n\n0.6.1 Convergence Rate for Property 2 (\\(\\alpha \\to \\infty\\))\n\n\n\n\n\n\nNoteTheorem 5 (Exponential Convergence to Optimality)\n\n\n\nLet \\(\\Delta = \\min\\{V^* - V(r) : r \\notin \\mathcal{R}^*\\}\\) be the minimum gap between optimal and suboptimal values. For any suboptimal alternative \\(r \\notin \\mathcal{R}^*\\):\n\\[\nP(\\text{choose } r \\mid \\alpha, V) = O\\left(e^{-\\alpha \\Delta}\\right) \\quad \\text{as } \\alpha \\to \\infty\n\\]\nConvergence to the optimality limit is exponential with rate \\(\\Delta\\).\n\n\n\n\n\n\n\n\nTipProof of Theorem 5\n\n\n\n\n\nFor \\(r \\notin \\mathcal{R}^*\\), recall from the proof of Property 2: \\[\nP(\\text{choose } r) = \\frac{\\exp(\\alpha \\cdot [V(r) - V^*])}{|\\mathcal{R}^*| + \\sum_{j \\in \\mathcal{R}^-} \\exp(\\alpha \\cdot [V(j) - V^*])}\n\\]\nSince \\(V(r) - V^* \\leq -\\Delta &lt; 0\\): \\[\nP(\\text{choose } r) \\leq \\frac{\\exp(-\\alpha \\Delta)}{|\\mathcal{R}^*|} = \\frac{1}{|\\mathcal{R}^*|} e^{-\\alpha \\Delta}\n\\]\nThe denominator is bounded below by \\(|\\mathcal{R}^*| \\geq 1\\), giving: \\[\nP(\\text{choose } r) = O(e^{-\\alpha \\Delta}) \\quad \\blacksquare\n\\]\n\n\n\nInterpretation: Larger value gaps \\(\\Delta\\) lead to faster concentration on optimal alternatives. When the best alternative is clearly superior (large \\(\\Delta\\)), even moderate \\(\\alpha\\) yields near-deterministic choice.\n\n\n0.6.2 Convergence Rate for Property 3 (\\(\\alpha \\to 0\\))\n\n\n\n\n\n\nNoteTheorem 6 (Linear Convergence to Uniformity)\n\n\n\nFor any alternative \\(r \\in \\mathcal{R}\\), let \\(\\bar{V} = \\frac{1}{|\\mathcal{R}|}\\sum_j V(j)\\) denote the arithmetic mean of values. Then:\n\\[\nP(\\text{choose } r \\mid \\alpha, V) = \\frac{1}{|\\mathcal{R}|} + \\alpha \\cdot \\left[V(r) - \\bar{V}\\right] \\cdot \\frac{1}{|\\mathcal{R}|} + O(\\alpha^2)\n\\]\nConvergence to uniformity is first-order (linear) in \\(\\alpha\\).\n\n\n\n\n\n\n\n\nTipProof of Theorem 6\n\n\n\n\n\nExpanding \\(\\exp(\\alpha V(r)) = 1 + \\alpha V(r) + \\frac{\\alpha^2 V(r)^2}{2} + O(\\alpha^3)\\):\n\\[\nP(\\text{choose } r) = \\frac{1 + \\alpha V(r) + O(\\alpha^2)}{\\sum_j [1 + \\alpha V(j) + O(\\alpha^2)]}\n= \\frac{1 + \\alpha V(r) + O(\\alpha^2)}{|\\mathcal{R}| + \\alpha \\sum_j V(j) + O(\\alpha^2)}\n\\]\nLet \\(S = \\sum_j V(j) = |\\mathcal{R}| \\cdot \\bar{V}\\). Using the expansion \\((1+x)^{-1} = 1 - x + O(x^2)\\):\n\\[\nP(\\text{choose } r) = \\frac{1 + \\alpha V(r)}{|\\mathcal{R}|} \\cdot \\left(1 + \\frac{\\alpha S}{|\\mathcal{R}|}\\right)^{-1} + O(\\alpha^2)\n\\]\n\\[\n= \\frac{1 + \\alpha V(r)}{|\\mathcal{R}|} \\cdot \\left(1 - \\frac{\\alpha S}{|\\mathcal{R}|}\\right) + O(\\alpha^2)\n\\]\n\\[\n= \\frac{1}{|\\mathcal{R}|} + \\frac{\\alpha V(r)}{|\\mathcal{R}|} - \\frac{\\alpha S}{|\\mathcal{R}|^2} + O(\\alpha^2)\n\\]\n\\[\n= \\frac{1}{|\\mathcal{R}|} + \\frac{\\alpha}{|\\mathcal{R}|} \\left[V(r) - \\frac{S}{|\\mathcal{R}|}\\right] + O(\\alpha^2)\n\\]\n\\[\n= \\frac{1}{|\\mathcal{R}|} + \\frac{\\alpha}{|\\mathcal{R}|} \\left[V(r) - \\bar{V}\\right] + O(\\alpha^2) \\quad \\blacksquare\n\\]\n\n\n\nInterpretation: Near \\(\\alpha = 0\\), deviations from uniform choice are proportional to \\(\\alpha\\) and to how much an alternative’s value exceeds the mean. The coefficient \\([V(r) - \\bar{V}]/|\\mathcal{R}|\\) determines the direction and magnitude of the first-order effect."
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#discussion",
    "href": "foundations/01_abstract_formulation.html#discussion",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.7 Discussion",
    "text": "0.7 Discussion\nHaving established the mathematical properties of the model, we turn to its interpretation.\n\n0.7.1 What the Model Describes\nOur model describes decision makers who are committed to SEU maximization but have limited sensitivity to its implications. The sensitivity parameter \\(\\alpha\\) measures how reliably their choices track the SEU ranking:\n\nHigh \\(\\alpha\\): Choices reliably favor higher-\\(\\eta\\) alternatives\nLow \\(\\alpha\\): Choices are noisy relative to the SEU ranking\n\\(\\alpha \\to \\infty\\): Near-deterministic choice of SEU-maximizing alternatives\n\\(\\alpha \\to 0\\): Choices become independent of SEU values\n\nThis is not a model of rational agents in the classical sense. SEU maximization—the normative standard—corresponds only to the limit \\(\\alpha \\to \\infty\\). For any finite \\(\\alpha\\), the model permits systematic departures from SEU maximization: lower-\\(\\eta\\) alternatives are chosen with positive probability.\n\n\n0.7.2 What the Model Is Not\nNot a cognitive process model. We make no claim about how decision makers actually deliberate. The model is silent on whether agents compute probabilities, form expectations, or engage in any particular mental procedure. It specifies a distribution over choices, not a mechanism that generates them. Our concern is to investigate the extent to which a decision maker’s behavior can be captured by viewing that behavior as-if it came from a decision maker who is committed to subjective expected utility maximization but has limited sensitivity to its implications.\nNot bounded rationality. Bounded rationality programs, in their various formulations (Simon 1955; Gigerenzer and Goldstein 1996), typically propose alternative decision procedures—heuristics, satisficing rules, or fast-and-frugal strategies—that agents use in place of optimization. Our model posits no such alternative procedures. Nor do we invoke notions of ecological rationality or fit between heuristics and environmental structure. The model simply describes a stochastic relationship between SEU values and choice probabilities.\n\n\n0.7.3 A Conceptual Lens: Commitment and Performance\nA useful conceptual lens comes from Isaac Levi’s distinction between commitment and performance (Levi 1980). We may be committed to standards we fail to perform up to. Most of us are committed to the laws of arithmetic despite occasionally making calculation errors; the errors are failures of performance, not rejections of the standard.\nIf we take SEU theory as specifying the decision maker’s normative commitments, then \\(\\alpha\\) measures their tendency to perform in accordance with those commitments. This framing preserves SEU as normatively fundamental while allowing systematic departures in observed behavior.\nOur framework differs from Levi’s in important ways:\n\nDecision maker’s vs. observer’s perspective. Levi’s framework is primarily concerned with the decision maker’s own deliberative standards—what an agent should believe and prefer from the first-person standpoint. Our framework, by contrast, adopts the observer’s perspective: we model choice behavior as it appears to an external analyst who observes decisions and infers parameters.\nProbabilistic vs. algebraic theories. Following Luce’s (Luce 1959) distinction, our framework is a probabilistic theory of choice—it specifies a probability distribution over choices given the decision problem. Levi’s framework is more aligned with algebraic theories that characterize rational choice through axioms on preference orderings rather than stochastic choice rules.\nSEU as the normative standard. Levi famously rejected subjective expected utility theory as the standard of rational choice, instead advocating for generalizations that accommodate indeterminate probabilities and utilities (Levi 1980). Our framework, by contrast, takes SEU maximization as the normative standard and models departures from it through the sensitivity parameter \\(\\alpha\\)."
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#summary",
    "href": "foundations/01_abstract_formulation.html#summary",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.8 Summary",
    "text": "0.8 Summary\nWe have established three fundamental properties of the softmax choice model:\n\nMonotonicity: Higher \\(\\alpha\\) increases probability of choosing alternatives with higher value \\(V(r)\\)\nPerfect optimization limit: As \\(\\alpha \\to \\infty\\), choices become deterministically concentrated on value-maximizing alternatives\nRandom choice limit: As \\(\\alpha \\to 0\\), choices become uniformly random\n\nAdditionally, we characterized the rates at which these limits are approached:\n\nConvergence to optimality is exponential with rate determined by the value gap \\(\\Delta\\)\nConvergence to uniformity is linear (first-order) in \\(\\alpha\\)\n\nThese properties hold for any value function \\(V\\). When \\(V\\) is taken to be subjective expected utility, the framework provides a model of decision-making that interpolates between random choice and SEU maximization, with \\(\\alpha\\) governing the degree of sensitivity to expected utility differences.\nThe standardization of utilities to \\([0,1]\\) fixes a representation from the equivalence class of utility functions, making \\(\\alpha\\) interpretable as sensitivity to standardized expected utility differences."
  },
  {
    "objectID": "foundations/01_abstract_formulation.html#references",
    "href": "foundations/01_abstract_formulation.html#references",
    "title": "Abstract Formulation of the SEU Sensitivity Model",
    "section": "0.9 References",
    "text": "0.9 References\n\n\nGigerenzer, Gerd, and Daniel G. Goldstein. 1996. “Reasoning the Fast and Frugal Way: Models of Bounded Rationality.” In Psychological Review, 103:650–69. 4.\n\n\nLevi, Isaac. 1980. The Enterprise of Knowledge: An Essay on Knowledge, Credal Probability, and Chance. Cambridge, MA: MIT Press.\n\n\nLuce, R. Duncan. 1959. “Individual Choice Behavior: A Theoretical Analysis.”\n\n\nMcFadden, Daniel. 1974. “Conditional Logit Analysis of Qualitative Choice Behavior.” Frontiers in Econometrics, 105–42.\n\n\nNeumann, John von, and Oskar Morgenstern. 1947. Theory of Games and Economic Behavior. 2nd ed. Princeton, NJ: Princeton University Press.\n\n\nSavage, Leonard J. 1954. “The Foundations of Statistics.”\n\n\nSimon, Herbert A. 1955. “A Behavioral Model of Rational Choice.” The Quarterly Journal of Economics 69 (1): 99–118."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html",
    "href": "foundations/05_adding_risky_choices.html",
    "title": "Adding Risky Choices",
    "section": "",
    "text": "Report 4 revealed differential parameter recovery in model m_0: while the sensitivity parameter α and feature weights β are recovered well, the utility increments δ show wider uncertainty and slower learning as sample size increases. This doesn’t mean δ is uninformed by the data—the posteriors do concentrate relative to the prior—but the information accumulates more slowly than for other parameters.\nOne approach to improving δ recovery is simply to collect more data. However, decision theory suggests an alternative: incorporating risky decisions where probabilities are known to the decision-maker. This approach, grounded in the Anscombe-Aumann framework, provides a different type of information that may help constrain utility parameters more directly."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#introduction",
    "href": "foundations/05_adding_risky_choices.html#introduction",
    "title": "Adding Risky Choices",
    "section": "",
    "text": "Report 4 revealed differential parameter recovery in model m_0: while the sensitivity parameter α and feature weights β are recovered well, the utility increments δ show wider uncertainty and slower learning as sample size increases. This doesn’t mean δ is uninformed by the data—the posteriors do concentrate relative to the prior—but the information accumulates more slowly than for other parameters.\nOne approach to improving δ recovery is simply to collect more data. However, decision theory suggests an alternative: incorporating risky decisions where probabilities are known to the decision-maker. This approach, grounded in the Anscombe-Aumann framework, provides a different type of information that may help constrain utility parameters more directly."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#historical-background-risk-vs.-uncertainty",
    "href": "foundations/05_adding_risky_choices.html#historical-background-risk-vs.-uncertainty",
    "title": "Adding Risky Choices",
    "section": "0.2 Historical Background: Risk vs. Uncertainty",
    "text": "0.2 Historical Background: Risk vs. Uncertainty\nThe distinction between decision-making under risk and under uncertainty is one of the oldest and most fundamental in decision theory.\n\n0.2.1 Knight’s Distinction\nFrank Knight (1921) drew a sharp distinction between:\n\nRisk: Situations where probabilities of outcomes are objectively known (e.g., a fair die)\nUncertainty: Situations where probabilities are unknown or unknowable (e.g., will it rain tomorrow?)\n\nKnight argued that these represent fundamentally different decision environments, with profit in business arising primarily from uncertainty rather than risk.\n\n\n0.2.2 Von Neumann-Morgenstern: Expected Utility Under Risk\nThe modern theory of rational choice under risk was axiomatized by Neumann and Morgenstern (1944). Their key insight was that preferences over lotteries—probability distributions over outcomes—could be represented by a utility function if certain axioms (completeness, transitivity, continuity, independence) were satisfied.\nFor a lottery \\(L = (p_1, o_1; p_2, o_2; \\ldots; p_K, o_K)\\) giving outcome \\(o_k\\) with known probability \\(p_k\\), the expected utility is: \\[\nU(L) = \\sum_{k=1}^K p_k \\cdot u(o_k)\n\\]\nThe vNM framework applies only to risk—the probabilities \\(p_k\\) are taken as given.\n\n\n0.2.3 Savage: Subjective Expected Utility Under Uncertainty\nLeonard Savage (1954) extended expected utility theory to uncertainty by introducing subjective probabilities. In his framework, the decision-maker acts as if assigning personal probabilities to states of the world and maximizing expected utility with respect to these beliefs.\nSavage’s axioms imply the existence of both:\n\nA utility function \\(u\\) over consequences\nA probability measure \\(P\\) over states\n\nHowever, Savage’s approach requires eliciting utilities and probabilities simultaneously from choice behavior—raising the identification problem we encountered with m_0.\n\n\n0.2.4 Anscombe-Aumann: The Horse Lottery Resolution\nAnscombe and Aumann (1963) proposed an elegant solution that bridges risk and uncertainty. Their key innovation was to consider choice objects that combine both:\n\n\n\n\n\n\nNoteThe Horse Lottery Framework\n\n\n\nAn Anscombe-Aumann act maps states of the world (the “horses”) to lotteries (probability distributions over prizes).\n\nThe probabilities in each lottery are objective (known)\nThe probabilities across states are subjective (uncertain)\n\nBy varying the lotteries while holding the state-dependence fixed, one can identify the utility function from risky preferences. By varying the state-dependence while holding lotteries fixed, one can identify subjective probabilities.\n\n\nThis insight is precisely what motivates model m_1. By observing choices in both:\n\nUncertain decisions (like Savage’s acts, mapped through features to subjective probabilities)\nRisky decisions (like vNM lotteries, with known probabilities)\n\n…we can separately identify utilities and subjective probabilities.\n\n\n0.2.5 The Identification Logic\nAs Kreps (1988) explains in his exposition of the Anscombe-Aumann framework:\n\n“The introduction of objective lotteries… provides a way to calibrate cardinal utility… Once cardinal utility is pinned down by choices among lotteries, subjective probability can be inferred from choices among acts.”\n\nThis is exactly our strategy:\n\n\n\n\n\n\n\n\nChoice Type\nWhat It Reveals\nModel Component\n\n\n\n\nRisky (known \\(p\\))\nUtility function shape\nδ parameters\n\n\nUncertain (unknown \\(p\\))\nSubjective probability formation\nβ parameters\n\n\nBoth\nChoice sensitivity\nα parameter"
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#model-m_1-specification",
    "href": "foundations/05_adding_risky_choices.html#model-m_1-specification",
    "title": "Adding Risky Choices",
    "section": "0.3 Model m_1 Specification",
    "text": "0.3 Model m_1 Specification\nModel m_1 extends m_0 by adding N risky choice problems alongside the M uncertain choice problems. Both share the same utility function (υ, determined by δ) and sensitivity parameter (α).\n\n0.3.1 Data Structure\n\n\nShow code\n# From m_1.stan data block:\n\n# === UNCERTAIN DECISIONS (as in m_0) ===\n# M: number of uncertain decision problems\n# K: number of possible consequences  \n# D: dimensions of alternative features\n# R: number of distinct uncertain alternatives\n# w[R]: feature vectors for uncertain alternatives (vector[D] each)\n# I[M,R]: indicator array for which alternatives appear in which problems\n# y[M]: choices for uncertain problems\n\n# === RISKY DECISIONS (new in m_1) ===\n# N: number of risky decision problems\n# S: number of distinct risky alternatives  \n# x[S]: probability simplexes for risky alternatives (simplex[K] each)\n# J[N,S]: indicator array for which risky alternatives appear in which problems\n# z[N]: choices for risky problems\n\n\n\n\n0.3.2 Key Differences from m_0\nThe crucial difference is in how probabilities enter:\nUncertain alternatives (same as m_0): \\[\n\\psi_{rk} = \\frac{\\exp(\\boldsymbol{\\beta}_k^\\top \\mathbf{w}_r)}{\\sum_{k'=1}^K \\exp(\\boldsymbol{\\beta}_{k'}^\\top \\mathbf{w}_r)}\n\\] The probabilities ψ are derived from features via the learned mapping β.\nRisky alternatives (new in m_1): \\[\n\\pi_{sk} = x_{sk} \\quad \\text{(given as data)}\n\\] The probabilities π are provided directly—they are the objective lottery probabilities.\n\n\n0.3.3 Expected Utilities\nFor uncertain alternatives, expected utility is: \\[\n\\eta^{(u)}_r = \\sum_{k=1}^K \\psi_{rk} \\cdot \\upsilon_k = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon}\n\\]\nFor risky alternatives, expected utility is: \\[\n\\eta^{(r)}_s = \\sum_{k=1}^K \\pi_{sk} \\cdot \\upsilon_k = \\boldsymbol{\\pi}_s^\\top \\boldsymbol{\\upsilon}\n\\]\nThe key insight: risky expected utilities depend only on υ (and hence δ), not on β. This breaks the confounding that plagued m_0.\n\n\n0.3.4 Choice Probabilities\nBoth choice types use the same softmax rule with shared α:\n\\[\n\\chi^{(u)}_{mi} = \\frac{\\exp(\\alpha \\cdot \\eta^{(u)}_{mi})}{\\sum_{j=1}^{N^{(u)}_m} \\exp(\\alpha \\cdot \\eta^{(u)}_{mj})}\n\\quad\\text{and}\\quad\n\\chi^{(r)}_{ni} = \\frac{\\exp(\\alpha \\cdot \\eta^{(r)}_{ni})}{\\sum_{j=1}^{N^{(r)}_n} \\exp(\\alpha \\cdot \\eta^{(r)}_{nj})}\n\\]\n\n\n0.3.5 Likelihood\nThe log-likelihood is the sum of contributions from both choice types: \\[\n\\log p(y, z | \\theta) = \\sum_{m=1}^M \\log \\chi^{(u)}_{m, y_m} + \\sum_{n=1}^N \\log \\chi^{(r)}_{n, z_n}\n\\]"
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#examining-the-stan-implementation",
    "href": "foundations/05_adding_risky_choices.html#examining-the-stan-implementation",
    "title": "Adding Risky Choices",
    "section": "0.4 Examining the Stan Implementation",
    "text": "0.4 Examining the Stan Implementation\nLet’s examine key portions of m_1.stan:\n\n\n\n// === PARAMETERS ===\nparameters {\n  real&lt;lower=0&gt; alpha;           // sensitivity (shared)\n  matrix[K,D] beta;              // feature-to-probability mapping\n  simplex[K-1] delta;            // utility increments (shared)\n}\n\n// === TRANSFORMED PARAMETERS ===\ntransformed parameters {\n  // Shared utility function\n  ordered[K] upsilon = cumulative_sum(append_row(0, delta));\n\n  // UNCERTAIN: subjective probabilities via softmax\n  for (i in 1:total_uncertain_alts) {\n    psi[i] = softmax(beta * x_uncertain[i]);\n  }\n\n  // UNCERTAIN: expected utilities\n  for (i in 1:total_uncertain_alts) {\n    eta_uncertain[i] = dot_product(psi[i], upsilon);\n  }\n\n  // RISKY: expected utilities (no beta involved!)\n  for (i in 1:total_risky_alts) {\n    eta_risky[i] = dot_product(x_risky[i], upsilon);  // x_risky is known\n  }\n\n  // Choice probabilities via softmax with shared alpha\n  // ... (for both uncertain and risky problems)\n}\n\n// === MODEL ===\nmodel {\n  // Priors (same as m_0)\n  alpha ~ lognormal(0, 1);\n  to_vector(beta) ~ std_normal();\n  delta ~ dirichlet(rep_vector(1, K-1));\n\n  // Likelihood: uncertain choices\n  for (m in 1:M) {\n    y[m] ~ categorical(chi_uncertain[m]);\n  }\n\n  // Likelihood: risky choices\n  for (n in 1:N) {\n    z[n] ~ categorical(chi_risky[n]);\n  }\n}\n\n\n\nThe critical line is eta_risky[i] = dot_product(x_risky[i], upsilon). Unlike uncertain alternatives where β mediates between features and probabilities, risky alternatives have their probabilities given directly. This means risky choices provide direct information about the utility vector υ."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#why-risky-choices-identify-δ",
    "href": "foundations/05_adding_risky_choices.html#why-risky-choices-identify-δ",
    "title": "Adding Risky Choices",
    "section": "0.5 Why Risky Choices Identify δ",
    "text": "0.5 Why Risky Choices Identify δ\nConsider a concrete example. Suppose we have K=3 consequences with utilities \\(\\upsilon = (0, \\upsilon_2, 1)\\) where \\(\\upsilon_2 = \\delta_1\\) (since \\(\\delta_1 + \\delta_2 = 1\\) and utilities are constructed cumulatively).\nRisky choice: Choose between:\n\nLottery A: (0.5, 0, 0.5) → outcomes 1 or 3 with equal probability\nLottery B: (0, 1, 0) → outcome 2 with certainty\n\nExpected utilities: \\[\n\\eta_A = 0.5 \\cdot 0 + 0 \\cdot \\upsilon_2 + 0.5 \\cdot 1 = 0.5\n\\] \\[\n\\eta_B = 0 \\cdot 0 + 1 \\cdot \\upsilon_2 + 0 \\cdot 1 = \\upsilon_2\n\\]\nThe decision-maker prefers the alternative with higher expected utility:\n\nIf they choose A, then \\(\\eta_A &gt; \\eta_B\\), implying \\(0.5 &gt; \\upsilon_2\\)\nIf they choose B, then \\(\\eta_B &gt; \\eta_A\\), implying \\(\\upsilon_2 &gt; 0.5\\)\n\nThis preference directly constrains υ₂ (and hence δ₁) without any confounding from subjective probabilities! More generally, by presenting lotteries that span the consequence space, we can triangulate the utility function with arbitrary precision (given sufficient data and lottery diversity)."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#study-design-for-m_1",
    "href": "foundations/05_adding_risky_choices.html#study-design-for-m_1",
    "title": "Adding Risky Choices",
    "section": "0.6 Study Design for m_1",
    "text": "0.6 Study Design for m_1\nTo demonstrate that adding risky choices resolves the δ identification problem, we need a controlled comparison between m_0 and m_1. Crucially, the uncertain decision problems must be identical between the two models—any differences in recovery should be attributable solely to the addition of risky choices.\nWe first create a base m_0 study design (matching Report 4’s configuration), then use the from_base_study() method to create an m_1 design that inherits the same uncertain alternatives and problems:\n\n\nShow code\nfrom utils.study_design import StudyDesign\n\n# Configuration for both models (uncertain component)\nconfig_base = {\n    \"M\": 25,                    # Number of uncertain decision problems\n    \"K\": 3,                     # Number of consequences\n    \"D\": 5,                     # Feature dimensions\n    \"R\": 15,                    # Distinct uncertain alternatives\n    \"min_alts_per_problem\": 2,\n    \"max_alts_per_problem\": 5,\n    \"feature_dist\": \"normal\",\n    \"feature_params\": {\"loc\": 0, \"scale\": 1},\n}\n\n# Risky problems configuration (added for m_1)\nconfig_risky = {\n    \"N\": 25,                    # Number of risky decision problems\n    \"S\": 15,                    # Distinct risky alternatives\n}\n\n# Create the base m_0 study design\nstudy_base = StudyDesign(\n    M=config_base[\"M\"],\n    K=config_base[\"K\"],\n    D=config_base[\"D\"],\n    R=config_base[\"R\"],\n    min_alts_per_problem=config_base[\"min_alts_per_problem\"],\n    max_alts_per_problem=config_base[\"max_alts_per_problem\"],\n    feature_dist=config_base[\"feature_dist\"],\n    feature_params=config_base[\"feature_params\"],\n    design_name=\"m0_base_for_comparison\"\n)\nstudy_base.generate()\n\nprint(f\"Base Study Design (for m_0 and m_1 uncertain component):\")\nprint(f\"  M = {study_base.M} uncertain decision problems\")\nprint(f\"  R = {study_base.R} distinct alternatives\")\nprint(f\"  K = {study_base.K} consequences\")\nprint(f\"\\nRisky Problems (added for m_1):\")\nprint(f\"  N = {config_risky['N']} risky decision problems\")\nprint(f\"  S = {config_risky['S']} distinct lotteries\")\n\n\nBase Study Design (for m_0 and m_1 uncertain component):\n  M = 25 uncertain decision problems\n  R = 15 distinct alternatives\n  K = 3 consequences\n\nRisky Problems (added for m_1):\n  N = 25 risky decision problems\n  S = 15 distinct lotteries"
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#parameter-recovery-comparison",
    "href": "foundations/05_adding_risky_choices.html#parameter-recovery-comparison",
    "title": "Adding Risky Choices",
    "section": "0.7 Parameter Recovery Comparison",
    "text": "0.7 Parameter Recovery Comparison\nIn Report 4, we found that δ recovery was weaker than α and β recovery in model m_0, with credible intervals that narrowed more slowly as sample size increased. We hypothesized that adding risky choices might help constrain utility parameters more directly.\nNow we test this hypothesis. If the Anscombe-Aumann logic holds, risky choices should provide more direct information about the utility function, potentially improving δ recovery.\n\n0.7.1 Study Design for m_1\nWe create the m_1 study design using from_base_study(), which ensures that the uncertain component (alternatives and problems) is identical to the base m_0 study. Only the risky problems are newly generated. This gives us 50 total decision problems—matching the sample size used in the M=50 analysis of m_0 (Report 4):\n\n\nShow code\nfrom utils.study_design_m1 import StudyDesignM1\nfrom analysis.parameter_recovery import ParameterRecovery\nimport tempfile\n\n# Create the m_1 study design FROM the base study\n# This ensures identical uncertain problems for valid comparison\nstudy_m1 = StudyDesignM1.from_base_study(\n    base_study=study_base,\n    N=config_risky[\"N\"],        # 25 risky problems\n    S=config_risky[\"S\"],        # 15 risky alternatives\n    risky_probs=\"random\",       # Random simplex probabilities (diverse lotteries)\n    design_name=\"m1_parameter_recovery\"\n)\n\n\n\n\nm_1 Study Design Generated:\n\n  Uncertain Problems (inherited from base study):\n    M = 25 problems\n    R = 15 distinct alternatives\n    Total uncertain choices: ~87\n\n  Risky Problems (newly generated):\n    N = 25 problems\n    S = 15 distinct lotteries\n    Total risky choices: ~88\n\n  Shared:\n    K = 3 consequences\n    Total choices: ~175\n\n\n\n\n0.7.2 Running Parameter Recovery for m_1\n\n\nShow code\n# Create output directory\noutput_dir_m1 = tempfile.mkdtemp(prefix=\"param_recovery_m1_\")\n\n# Initialize and run parameter recovery for m_1\nrecovery_m1 = ParameterRecovery(\n    inference_model_path=os.path.join(project_root, \"models\", \"m_1.stan\"),\n    sim_model_path=os.path.join(project_root, \"models\", \"m_1_sim.stan\"),\n    study_design=study_m1,\n    output_dir=output_dir_m1,\n    n_mcmc_samples=1000,\n    n_mcmc_chains=4,\n    n_iterations=50\n)\n\ntrue_params_m1, posterior_summaries_m1 = recovery_m1.run()\n\n\n\n\n\nm_1 Parameter Recovery Complete:\n  Successful iterations: 50\n\n\n\n\n0.7.3 Comparing m_0 and m_1 Recovery\nNow we compare the key parameter recovery metrics between models. We focus particularly on δ, the parameter that was poorly identified in m_0:\n\n\nShow code\n# Compute recovery metrics for m_1\n\n# Alpha\nalpha_true_m1 = np.array([p['alpha'] for p in true_params_m1])\nalpha_mean_m1 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m1])\nalpha_lower_m1 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m1])\nalpha_upper_m1 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m1])\n\nalpha_bias_m1 = np.mean(alpha_mean_m1 - alpha_true_m1)\nalpha_rmse_m1 = np.sqrt(np.mean((alpha_mean_m1 - alpha_true_m1)**2))\nalpha_coverage_m1 = np.mean((alpha_true_m1 &gt;= alpha_lower_m1) & (alpha_true_m1 &lt;= alpha_upper_m1))\nalpha_ci_width_m1 = np.mean(alpha_upper_m1 - alpha_lower_m1)\n\n# Delta (using config from base study)\nK_minus_1 = config_base['K'] - 1\ndelta_stats_m1 = []\nfor k in range(K_minus_1):\n    param_name = f\"delta[{k+1}]\"\n    \n    delta_true = np.array([p['delta'][k] for p in true_params_m1])\n    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m1])\n    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m1])\n    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m1])\n    \n    delta_stats_m1.append({\n        'parameter': f'δ_{k+1}',\n        'true': delta_true,\n        'mean': delta_mean,\n        'lower': delta_lower,\n        'upper': delta_upper,\n        'bias': np.mean(delta_mean - delta_true),\n        'rmse': np.sqrt(np.mean((delta_mean - delta_true)**2)),\n        'coverage': np.mean((delta_true &gt;= delta_lower) & (delta_true &lt;= delta_upper)),\n        'ci_width': np.mean(delta_upper - delta_lower)\n    })\n\ndelta_df_m1 = pd.DataFrame(delta_stats_m1)\n\n# Beta (for comparison)\nK, D = config_base['K'], config_base['D']\nbeta_stats_m1 = []\nfor k in range(K):\n    for d in range(D):\n        param_name = f\"beta[{k+1},{d+1}]\"\n        beta_true = np.array([p['beta'][k][d] for p in true_params_m1])\n        beta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m1])\n        beta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m1])\n        beta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m1])\n        \n        beta_stats_m1.append({\n            'rmse': np.sqrt(np.mean((beta_mean - beta_true)**2)),\n            'coverage': np.mean((beta_true &gt;= beta_lower) & (beta_true &lt;= beta_upper)),\n            'ci_width': np.mean(beta_upper - beta_lower)\n        })\n\nbeta_df_m1 = pd.DataFrame(beta_stats_m1)\n\n\n\n\nShow code\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nfor k in range(K_minus_1):\n    # m_1 True vs Estimated (top row)\n    ax = axes[0, k]\n    ax.scatter(delta_stats_m1[k]['true'], delta_stats_m1[k]['mean'], \n               alpha=0.7, s=60, c='forestgreen', edgecolor='white', label='m_1')\n    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Identity')\n    ax.set_xlim(-0.05, 1.05)\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_xlabel(f'True δ_{k+1}', fontsize=11)\n    ax.set_ylabel(f'Estimated δ_{k+1}', fontsize=11)\n    ax.set_title(f'm_1: δ_{k+1} Recovery (RMSE={delta_stats_m1[k][\"rmse\"]:.3f})', fontsize=11)\n    ax.set_aspect('equal')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # m_1 Coverage (bottom row)\n    ax = axes[1, k]\n    iterations = np.arange(len(delta_stats_m1[k]['true']))\n    for i in range(len(delta_stats_m1[k]['true'])):\n        covered = ((delta_stats_m1[k]['true'][i] &gt;= delta_stats_m1[k]['lower'][i]) & \n                   (delta_stats_m1[k]['true'][i] &lt;= delta_stats_m1[k]['upper'][i]))\n        color = 'forestgreen' if covered else 'crimson'\n        ax.plot([i, i], [delta_stats_m1[k]['lower'][i], delta_stats_m1[k]['upper'][i]], \n                color=color, linewidth=2, alpha=0.7)\n        ax.scatter(i, delta_stats_m1[k]['mean'][i], color=color, s=40, zorder=3)\n    ax.scatter(iterations, delta_stats_m1[k]['true'], color='black', s=60, marker='x',\n               label='True value', zorder=4, linewidth=2)\n    ax.set_xlabel('Iteration', fontsize=11)\n    ax.set_ylabel(f'δ_{k+1}', fontsize=11)\n    ax.set_title(f'm_1: δ_{k+1} Coverage = {delta_stats_m1[k][\"coverage\"]:.0%}', fontsize=11)\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Comparison of δ recovery between m_0 and m_1. Adding risky choices dramatically improves recovery: narrower credible intervals and better coverage.\n\n\n\n\n\n\n\n\n\nShow code\n# Run m_0 recovery on the SAME base study used for m_1's uncertain component\n# This ensures a valid comparison: identical uncertain problems, differing only\n# in whether risky choices are added\noutput_dir_m0 = tempfile.mkdtemp(prefix=\"param_recovery_m0_compare_\")\nrecovery_m0_compare = ParameterRecovery(\n    inference_model_path=None,  # Default m_0.stan\n    sim_model_path=None,        # Default m_0_sim.stan\n    study_design=study_base,    # Use the same base study as m_1 (M=25)\n    output_dir=output_dir_m0,\n    n_mcmc_samples=1000,\n    n_mcmc_chains=4,\n    n_iterations=50\n)\ntrue_params_m0, posterior_summaries_m0 = recovery_m0_compare.run()\n\n# Run m_0 with M=50 using SAME alternatives (only new problems)\n# This uses the SAME alternatives as study_base, just with 25 additional problems\nstudy_m50 = study_base.extend(additional_M=25, design_name=\"m0_extended_m50\")\n\noutput_dir_m0_m50 = tempfile.mkdtemp(prefix=\"param_recovery_m0_m50_\")\nrecovery_m0_m50 = ParameterRecovery(\n    inference_model_path=None,\n    sim_model_path=None,\n    study_design=study_m50,\n    output_dir=output_dir_m0_m50,\n    n_mcmc_samples=1000,\n    n_mcmc_chains=4,\n    n_iterations=50\n)\ntrue_params_m0_m50, posterior_summaries_m0_m50 = recovery_m0_m50.run()\n\n# Run m_0 with M=50 using NEW alternatives (15 new feature vectors)\n# This parallels how m_1 adds 15 new risky alternatives\nstudy_m50_new_alts = study_base.extend_with_alternatives(\n    additional_M=25,\n    additional_R=15,  # Same as S in m_1\n    design_name=\"m0_extended_m50_new_alts\",\n    new_problems_use_new_alts_only=True  # New problems only use new alternatives\n)\n\noutput_dir_m0_m50_new = tempfile.mkdtemp(prefix=\"param_recovery_m0_m50_new_alts_\")\nrecovery_m0_m50_new = ParameterRecovery(\n    inference_model_path=None,\n    sim_model_path=None,\n    study_design=study_m50_new_alts,\n    output_dir=output_dir_m0_m50_new,\n    n_mcmc_samples=1000,\n    n_mcmc_chains=4,\n    n_iterations=50\n)\ntrue_params_m0_m50_new, posterior_summaries_m0_m50_new = recovery_m0_m50_new.run()\n\n# Compute m_0 delta metrics for all three m_0 conditions\ndelta_stats_m0 = []\ndelta_stats_m0_m50 = []\ndelta_stats_m0_m50_new = []\nfor k in range(K_minus_1):\n    param_name = f\"delta[{k+1}]\"\n    \n    # M=25 (base)\n    delta_true = np.array([p['delta'][k] for p in true_params_m0])\n    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0])\n    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0])\n    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0])\n    \n    delta_stats_m0.append({\n        'parameter': f'δ_{k+1}',\n        'rmse': np.sqrt(np.mean((delta_mean - delta_true)**2)),\n        'coverage': np.mean((delta_true &gt;= delta_lower) & (delta_true &lt;= delta_upper)),\n        'ci_width': np.mean(delta_upper - delta_lower)\n    })\n    \n    # M=50 (same alternatives)\n    delta_true_m50 = np.array([p['delta'][k] for p in true_params_m0_m50])\n    delta_mean_m50 = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0_m50])\n    delta_lower_m50 = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0_m50])\n    delta_upper_m50 = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0_m50])\n    \n    delta_stats_m0_m50.append({\n        'parameter': f'δ_{k+1}',\n        'rmse': np.sqrt(np.mean((delta_mean_m50 - delta_true_m50)**2)),\n        'coverage': np.mean((delta_true_m50 &gt;= delta_lower_m50) & (delta_true_m50 &lt;= delta_upper_m50)),\n        'ci_width': np.mean(delta_upper_m50 - delta_lower_m50)\n    })\n    \n    # M=50 (new alternatives)\n    delta_true_m50_new = np.array([p['delta'][k] for p in true_params_m0_m50_new])\n    delta_mean_m50_new = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0_m50_new])\n    delta_lower_m50_new = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0_m50_new])\n    delta_upper_m50_new = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0_m50_new])\n    \n    delta_stats_m0_m50_new.append({\n        'parameter': f'δ_{k+1}',\n        'rmse': np.sqrt(np.mean((delta_mean_m50_new - delta_true_m50_new)**2)),\n        'coverage': np.mean((delta_true_m50_new &gt;= delta_lower_m50_new) & (delta_true_m50_new &lt;= delta_upper_m50_new)),\n        'ci_width': np.mean(delta_upper_m50_new - delta_lower_m50_new)\n    })\n\n# Also compute alpha metrics for all m_0 conditions\nalpha_true_m0_m50 = np.array([p['alpha'] for p in true_params_m0_m50])\nalpha_mean_m0_m50 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m0_m50])\nalpha_lower_m0_m50 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m0_m50])\nalpha_upper_m0_m50 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m0_m50])\nalpha_rmse_m0_m50 = np.sqrt(np.mean((alpha_mean_m0_m50 - alpha_true_m0_m50)**2))\nalpha_coverage_m0_m50 = np.mean((alpha_true_m0_m50 &gt;= alpha_lower_m0_m50) & (alpha_true_m0_m50 &lt;= alpha_upper_m0_m50))\nalpha_ci_width_m0_m50 = np.mean(alpha_upper_m0_m50 - alpha_lower_m0_m50)\n\nalpha_true_m0_m50_new = np.array([p['alpha'] for p in true_params_m0_m50_new])\nalpha_mean_m0_m50_new = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m0_m50_new])\nalpha_lower_m0_m50_new = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m0_m50_new])\nalpha_upper_m0_m50_new = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m0_m50_new])\nalpha_rmse_m0_m50_new = np.sqrt(np.mean((alpha_mean_m0_m50_new - alpha_true_m0_m50_new)**2))\nalpha_coverage_m0_m50_new = np.mean((alpha_true_m0_m50_new &gt;= alpha_lower_m0_m50_new) & (alpha_true_m0_m50_new &lt;= alpha_upper_m0_m50_new))\nalpha_ci_width_m0_m50_new = np.mean(alpha_upper_m0_m50_new - alpha_lower_m0_m50_new)\n\n\n\nFigure 2\n\n\n\n\n\nShow code\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nparams = ['δ₁', 'δ₂']\nx = np.arange(len(params))\nwidth = 0.2\n\n# RMSE comparison\nax = axes[0]\nrmse_m0 = [delta_stats_m0[k]['rmse'] for k in range(K_minus_1)]\nrmse_m0_m50 = [delta_stats_m0_m50[k]['rmse'] for k in range(K_minus_1)]\nrmse_m0_m50_new = [delta_stats_m0_m50_new[k]['rmse'] for k in range(K_minus_1)]\nrmse_m1 = [delta_stats_m1[k]['rmse'] for k in range(K_minus_1)]\n\nbars1 = ax.bar(x - 1.5*width, rmse_m0, width, label='m_0 (M=25)', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x - 0.5*width, rmse_m0_m50, width, label='m_0 (M=50 same)', color='coral', alpha=0.7)\nbars3 = ax.bar(x + 0.5*width, rmse_m0_m50_new, width, label='m_0 (M=50 new)', color='mediumseagreen', alpha=0.7)\nbars4 = ax.bar(x + 1.5*width, rmse_m1, width, label='m_1 (M=25+N=25)', color='purple', alpha=0.7)\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('RMSE', fontsize=12)\nax.set_title('δ RMSE by Condition', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.legend(loc='upper right', fontsize=9)\nax.grid(True, alpha=0.3, axis='y')\n\n# Coverage comparison\nax = axes[1]\ncov_m0 = [delta_stats_m0[k]['coverage'] for k in range(K_minus_1)]\ncov_m0_m50 = [delta_stats_m0_m50[k]['coverage'] for k in range(K_minus_1)]\ncov_m0_m50_new = [delta_stats_m0_m50_new[k]['coverage'] for k in range(K_minus_1)]\ncov_m1 = [delta_stats_m1[k]['coverage'] for k in range(K_minus_1)]\n\nbars1 = ax.bar(x - 1.5*width, cov_m0, width, label='m_0 (M=25)', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x - 0.5*width, cov_m0_m50, width, label='m_0 (M=50 same)', color='coral', alpha=0.7)\nbars3 = ax.bar(x + 0.5*width, cov_m0_m50_new, width, label='m_0 (M=50 new)', color='mediumseagreen', alpha=0.7)\nbars4 = ax.bar(x + 1.5*width, cov_m1, width, label='m_1 (M=25+N=25)', color='purple', alpha=0.7)\nax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('Coverage', fontsize=12)\nax.set_title('δ 90% CI Coverage by Condition', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.set_ylim(0, 1.05)\nax.legend(loc='lower right', fontsize=9)\nax.grid(True, alpha=0.3, axis='y')\n\n# CI Width comparison\nax = axes[2]\nci_m0 = [delta_stats_m0[k]['ci_width'] for k in range(K_minus_1)]\nci_m0_m50 = [delta_stats_m0_m50[k]['ci_width'] for k in range(K_minus_1)]\nci_m0_m50_new = [delta_stats_m0_m50_new[k]['ci_width'] for k in range(K_minus_1)]\nci_m1 = [delta_stats_m1[k]['ci_width'] for k in range(K_minus_1)]\n\nbars1 = ax.bar(x - 1.5*width, ci_m0, width, label='m_0 (M=25)', color='steelblue', alpha=0.7)\nbars2 = ax.bar(x - 0.5*width, ci_m0_m50, width, label='m_0 (M=50 same)', color='coral', alpha=0.7)\nbars3 = ax.bar(x + 0.5*width, ci_m0_m50_new, width, label='m_0 (M=50 new)', color='mediumseagreen', alpha=0.7)\nbars4 = ax.bar(x + 1.5*width, ci_m1, width, label='m_1 (M=25+N=25)', color='purple', alpha=0.7)\nax.set_xlabel('Parameter', fontsize=12)\nax.set_ylabel('CI Width', fontsize=12)\nax.set_title('δ 90% CI Width by Condition', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(params)\nax.legend(loc='upper right', fontsize=9)\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Comparison of δ recovery across four conditions: m_0 (M=25), m_0 (M=50 same alts), m_0 (M=50 new alts), and m_1 (M=25 + N=25 risky). The key comparison is between m_0 with new uncertain alternatives vs. m_1 with new risky alternatives.\n\n\n\n\n\n\n\n0.7.4 Summary Statistics\n\nShow code\n# Build comparison table with all four conditions\ncomparison_rows = []\n\n# Compute alpha metrics for m_0 M=25\nalpha_true_m0 = np.array([p['alpha'] for p in true_params_m0])\nalpha_mean_m0 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m0])\nalpha_lower_m0 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m0])\nalpha_upper_m0 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m0])\nalpha_rmse_m0 = np.sqrt(np.mean((alpha_mean_m0 - alpha_true_m0)**2))\nalpha_coverage_m0 = np.mean((alpha_true_m0 &gt;= alpha_lower_m0) & (alpha_true_m0 &lt;= alpha_upper_m0))\nalpha_ci_width_m0 = np.mean(alpha_upper_m0 - alpha_lower_m0)\n\n# Alpha row\ncomparison_rows.append({\n    'Parameter': 'α',\n    'm_0 (M=25)': f'{alpha_rmse_m0:.4f}',\n    'm_0 (M=50 same)': f'{alpha_rmse_m0_m50:.4f}',\n    'm_0 (M=50 new)': f'{alpha_rmse_m0_m50_new:.4f}',\n    'm_1': f'{alpha_rmse_m1:.4f}',\n})\n\n# Delta rows\nfor k in range(K_minus_1):\n    comparison_rows.append({\n        'Parameter': f'δ_{k+1}',\n        'm_0 (M=25)': f'{delta_stats_m0[k][\"rmse\"]:.4f}',\n        'm_0 (M=50 same)': f'{delta_stats_m0_m50[k][\"rmse\"]:.4f}',\n        'm_0 (M=50 new)': f'{delta_stats_m0_m50_new[k][\"rmse\"]:.4f}',\n        'm_1': f'{delta_stats_m1[k][\"rmse\"]:.4f}',\n    })\n\ncomparison_df = pd.DataFrame(comparison_rows)\nprint(\"RMSE Comparison:\")\nprint(comparison_df.to_string(index=False))\n\n\n\n\nTable 1: Parameter recovery comparison across four conditions. The key comparison is m_0 with new uncertain alternatives vs. m_1 with new risky alternatives.\n\n\n\nRMSE Comparison:\nParameter m_0 (M=25) m_0 (M=50 same) m_0 (M=50 new)    m_1\n        α     1.1685          0.7617         0.7589 0.7329\n      δ_1     0.3048          0.2946         0.2850 0.2722\n      δ_2     0.3048          0.2946         0.2850 0.2722\n\n\n\n\n\nShow code\n# Build CI width comparison table\nci_rows = []\n\nci_rows.append({\n    'Parameter': 'α',\n    'm_0 (M=25)': f'{alpha_ci_width_m0:.3f}',\n    'm_0 (M=50 same)': f'{alpha_ci_width_m0_m50:.3f}',\n    'm_0 (M=50 new)': f'{alpha_ci_width_m0_m50_new:.3f}',\n    'm_1 (M=25+N=25)': f'{alpha_ci_width_m1:.3f}'\n})\n\nfor k in range(K_minus_1):\n    ci_rows.append({\n        'Parameter': f'δ_{k+1}',\n        'm_0 (M=25)': f'{delta_stats_m0[k][\"ci_width\"]:.3f}',\n        'm_0 (M=50 same)': f'{delta_stats_m0_m50[k][\"ci_width\"]:.3f}',\n        'm_0 (M=50 new)': f'{delta_stats_m0_m50_new[k][\"ci_width\"]:.3f}',\n        'm_1 (M=25+N=25)': f'{delta_stats_m1[k][\"ci_width\"]:.3f}'\n    })\n\nci_df = pd.DataFrame(ci_rows)\nprint(ci_df.to_string(index=False))\n\n\n\n\nTable 2: 90% credible interval widths across all four conditions.\n\n\n\nParameter m_0 (M=25) m_0 (M=50 same) m_0 (M=50 new) m_1 (M=25+N=25)\n        α      3.252           2.550          2.673           2.694\n      δ_1      0.892           0.876          0.879           0.837\n      δ_2      0.892           0.876          0.879           0.837\n\n\n\n\n\n\n\n\n\n\nTipKey Result: Comparing Uncertain vs. Risky Alternative Extensions\n\n\n\nThe four-way comparison now isolates what drives δ recovery improvement:\n\n\n\n\n\n\n\n\nCondition\nDescription\nWhat it tests\n\n\n\n\nm_0 (M=25)\nBase study\nBaseline\n\n\nm_0 (M=50 same)\n+25 problems, same R=15 alternatives\nMore data, same feature space\n\n\nm_0 (M=50 new)\n+25 problems using 15 new alternatives\nExpanded feature space\n\n\nm_1 (M=25+N=25)\n+25 risky problems using 15 new lotteries\nKnown probabilities\n\n\n\nThe crucial comparison is between conditions 3 and 4: - Both add 25 new problems - Both introduce 15 new choice objects (uncertain alternatives vs. risky lotteries) - The key difference: risky alternatives have known probabilities, uncertain alternatives have inferred probabilities\nIf m_1 outperforms m_0 (M=50 new), it demonstrates that the type of data matters—not just the amount or diversity. Risky choices provide qualitatively different information about utilities because they bypass the probability inference step entirely.\n\n\n\n\n0.7.5 Why This Helps\nThe improvement from adding risky choices can be understood through the likelihood structure:\nIn m_0 (uncertain choices only): \\[\n\\log p(y | \\theta) = \\sum_{m=1}^M \\log \\chi_{m, y_m} \\quad \\text{where} \\quad \\chi \\propto \\exp(\\alpha \\cdot \\boldsymbol{\\psi}^\\top \\boldsymbol{\\upsilon})\n\\]\nHere, ψ (from β) and υ (from δ) appear together in expected utilities. While not completely confounded, information about δ must be extracted indirectly.\nIn m_1 (uncertain + risky choices): \\[\n\\log p(y, z | \\theta) = \\underbrace{\\sum_{m=1}^M \\log \\chi^{(u)}_{m, y_m}}_{\\text{Informs } \\alpha, \\beta, \\delta} + \\underbrace{\\sum_{n=1}^N \\log \\chi^{(r)}_{n, z_n}}_{\\text{More directly informs } \\delta}\n\\]\nThe risky choice likelihood depends on δ through υ = cumsum(δ), but not on β. This provides a more direct channel for learning about utilities, supplementing the indirect information from uncertain choices."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#summary",
    "href": "foundations/05_adding_risky_choices.html#summary",
    "title": "Adding Risky Choices",
    "section": "0.8 Summary",
    "text": "0.8 Summary\nThe extension from m_0 to m_1 follows a principled path grounded in decision theory:\n\nDifferential recovery in m_0 (Report 4): Utility parameters show wider uncertainty than sensitivity parameters, though they do respond to data\nThree extensions compared: More problems (same alternatives), more problems (new alternatives), risky problems (new lotteries)\nThe historical insight: Anscombe-Aumann recognized that combining risk and uncertainty provides complementary information about preferences\nThe key comparison: m_0 with new uncertain alternatives vs. m_1 with new risky alternatives isolates whether known probabilities provide unique advantages\n\n\n\n\n\n\n\nTipKey Insight\n\n\n\nThe four-way comparison reveals whether the benefit of risky choices comes from: - Simply having more diverse choice problems (testable via m_0 with new alternatives), or - The qualitatively different information provided by known probabilities (unique to m_1)\nIf m_1 outperforms the expanded m_0, it validates the Anscombe-Aumann insight: risky choices provide a more direct route to utility identification because they bypass the subjective probability inference step.\nThe choice between approaches depends on practical considerations: the feasibility of including risky choice tasks, the importance of precise utility estimation, and whether the research question requires separating utilities from beliefs."
  },
  {
    "objectID": "foundations/05_adding_risky_choices.html#formal-identification-analysis",
    "href": "foundations/05_adding_risky_choices.html#formal-identification-analysis",
    "title": "Adding Risky Choices",
    "section": "0.9 Formal Identification Analysis",
    "text": "0.9 Formal Identification Analysis\nThe empirical results above demonstrate that m_1 achieves better parameter recovery than m_0, particularly for the utility parameters δ. In this section, we provide a formal analysis of why this occurs by establishing identification results for both models.\n\n0.9.1 Notation and Setup\nLet the full parameter vector be \\(\\theta = (\\alpha, \\beta, \\delta)\\) where:\n\n\\(\\alpha &gt; 0\\) is the sensitivity parameter\n\\(\\beta \\in \\mathbb{R}^{K \\times D}\\) is the feature-to-probability mapping matrix\n\\(\\delta \\in \\Delta^{K-2}\\) is the utility increment simplex, i.e., \\(\\delta = (\\delta_1, \\ldots, \\delta_{K-1})\\) with \\(\\delta_k &gt; 0\\) and \\(\\sum_{k=1}^{K-1} \\delta_k = 1\\)\n\nThe utility vector \\(\\upsilon \\in \\mathbb{R}^K\\) is constructed as: \\[\n\\upsilon_1 = 0, \\quad \\upsilon_k = \\sum_{j=1}^{k-1} \\delta_j \\text{ for } k = 2, \\ldots, K\n\\]\nThis ensures \\(\\upsilon\\) is ordered with \\(0 = \\upsilon_1 &lt; \\upsilon_2 &lt; \\cdots &lt; \\upsilon_K = 1\\).\nFor an uncertain alternative with feature vector \\(\\mathbf{w} \\in \\mathbb{R}^D\\), the subjective probability vector is: \\[\n\\psi_k(\\beta, \\mathbf{w}) = \\frac{\\exp(\\boldsymbol{\\beta}_k^\\top \\mathbf{w})}{\\sum_{j=1}^K \\exp(\\boldsymbol{\\beta}_j^\\top \\mathbf{w})}\n\\]\nFor a risky alternative with probability vector \\(\\boldsymbol{\\pi} \\in \\Delta^{K-1}\\), the probabilities are simply \\(\\boldsymbol{\\pi}\\) (given as data).\nExpected utilities are: \\[\n\\eta^{(u)}(\\theta, \\mathbf{w}) = \\boldsymbol{\\psi}(\\beta, \\mathbf{w})^\\top \\boldsymbol{\\upsilon}(\\delta), \\quad \\eta^{(r)}(\\delta, \\boldsymbol{\\pi}) = \\boldsymbol{\\pi}^\\top \\boldsymbol{\\upsilon}(\\delta)\n\\]\nChoice probabilities in a problem with alternatives yielding expected utilities \\(\\eta_1, \\ldots, \\eta_n\\) are: \\[\n\\chi_i(\\alpha, \\eta) = \\frac{\\exp(\\alpha \\cdot \\eta_i)}{\\sum_{j=1}^n \\exp(\\alpha \\cdot \\eta_j)}\n\\]\n\n\n0.9.2 Definition of Identifiability\n\n\n\n\n\n\nNoteGlobal Identifiability\n\n\n\nParameters \\(\\theta\\) are globally identifiable from a class of decision problems \\(\\mathcal{P}\\) if for any \\(\\theta \\neq \\theta'\\), there exists at least one problem \\(P \\in \\mathcal{P}\\) such that the induced choice probability distributions differ: \\[\n\\chi^P(\\theta) \\neq \\chi^P(\\theta')\n\\]\n\n\nThis definition captures whether, in principle, sufficiently rich data can distinguish any two parameter configurations. It is a necessary condition for consistent estimation.\n\n\n0.9.3 Identification in Model m_1\nWe first establish that m_1 achieves global identification under mild conditions.\n\n\n\n\n\n\nImportantTheorem (Global Identifiability of m_1)\n\n\n\nConsider model m_1 with \\(K \\geq 2\\) consequences and \\(D \\geq 1\\) feature dimensions. Let the class of decision problems include:\n\nRisky problems with probability vectors from a set \\(\\mathcal{X} \\subseteq \\Delta^{K-1}\\) that contains \\(K\\) affinely independent probability vectors\nUncertain problems with feature vectors from a set \\(\\mathcal{W} \\subseteq \\mathbb{R}^D\\) that spans \\(\\mathbb{R}^D\\)\n\nThen for any \\(\\theta = (\\alpha, \\beta, \\delta) \\neq \\theta' = (\\alpha', \\beta', \\delta')\\) with \\(\\alpha, \\alpha' &gt; 0\\), there exists a binary choice problem (with exactly two alternatives) where the choice probabilities under \\(\\theta\\) and \\(\\theta'\\) differ.\n\n\nBefore proving the theorem, we establish a key lemma.\n\n\n\n\n\n\nNoteLemma (Affine Independence Implies Linear Independence for Probability Vectors)\n\n\n\nLet \\(\\boldsymbol{\\pi}^{(1)}, \\ldots, \\boldsymbol{\\pi}^{(K)} \\in \\Delta^{K-1}\\) be \\(K\\) affinely independent probability vectors. Then they are linearly independent in \\(\\mathbb{R}^K\\).\n\n\nProof of Lemma. Recall that vectors \\(\\boldsymbol{\\pi}^{(1)}, \\ldots, \\boldsymbol{\\pi}^{(K)}\\) are affinely dependent if there exist scalars \\(c_1, \\ldots, c_K\\), not all zero, such that \\(\\sum_s c_s = 0\\) and \\(\\sum_s c_s \\boldsymbol{\\pi}^{(s)} = \\mathbf{0}\\). Equivalently, they are affinely independent if no such non-trivial relation exists.\nNow suppose \\(\\sum_{s=1}^K c_s \\boldsymbol{\\pi}^{(s)} = \\mathbf{0}\\) for some scalars \\(c_1, \\ldots, c_K\\) (a linear dependence relation). Taking the inner product with \\(\\mathbf{1} = (1, \\ldots, 1)^\\top\\) and using the fact that \\(\\mathbf{1}^\\top \\boldsymbol{\\pi}^{(s)} = 1\\) for all probability vectors, we obtain: \\[\n0 = \\mathbf{1}^\\top \\mathbf{0} = \\mathbf{1}^\\top \\left( \\sum_{s=1}^K c_s \\boldsymbol{\\pi}^{(s)} \\right) = \\sum_{s=1}^K c_s (\\mathbf{1}^\\top \\boldsymbol{\\pi}^{(s)}) = \\sum_{s=1}^K c_s\n\\]\nThus any linear dependence among probability vectors automatically satisfies \\(\\sum_s c_s = 0\\). This means any linear dependence would also constitute an affine dependence. Since our vectors are affinely independent by assumption, no non-trivial linear dependence can exist. Therefore \\(c_s = 0\\) for all \\(s\\), establishing linear independence. \\(\\square\\)\nProof of Theorem. We proceed by cases, showing that any difference in parameters can be detected by a binary choice problem.\nCase 1: \\(\\delta \\neq \\delta'\\) (utilities differ).\nBy assumption, \\(\\mathcal{X}\\) contains \\(K\\) affinely independent probability vectors \\(\\boldsymbol{\\pi}^{(1)}, \\ldots, \\boldsymbol{\\pi}^{(K)}\\). By the Lemma, these are linearly independent in \\(\\mathbb{R}^K\\).\nLet \\(\\Pi\\) be the \\(K \\times K\\) matrix with rows \\((\\boldsymbol{\\pi}^{(s)})^\\top\\). Since these rows are linearly independent, \\(\\Pi\\) is invertible. Consider the expected utilities under both parameter configurations: \\[\n\\eta^{(r)}_s = (\\boldsymbol{\\pi}^{(s)})^\\top \\boldsymbol{\\upsilon}(\\delta), \\quad (\\eta')^{(r)}_s = (\\boldsymbol{\\pi}^{(s)})^\\top \\boldsymbol{\\upsilon}(\\delta')\n\\]\nIn vector form: \\(\\boldsymbol{\\eta} = \\Pi \\boldsymbol{\\upsilon}(\\delta)\\) and \\(\\boldsymbol{\\eta}' = \\Pi \\boldsymbol{\\upsilon}(\\delta')\\). Since \\(\\Pi\\) is invertible: \\[\n\\boldsymbol{\\eta} = \\boldsymbol{\\eta}' \\iff \\boldsymbol{\\upsilon}(\\delta) = \\boldsymbol{\\upsilon}(\\delta')\n\\]\nNow, \\(\\delta \\neq \\delta'\\) implies \\(\\boldsymbol{\\upsilon}(\\delta) \\neq \\boldsymbol{\\upsilon}(\\delta')\\) because the map \\(\\delta \\mapsto \\boldsymbol{\\upsilon}(\\delta) = \\text{cumsum}([0, \\delta])\\) is injective on \\(\\Delta^{K-2}\\). Therefore, \\(\\boldsymbol{\\eta} \\neq \\boldsymbol{\\eta}'\\), meaning at least one pair of expected utilities differs: \\(\\eta^{(r)}_s \\neq (\\eta')^{(r)}_s\\) for some \\(s\\).\nConstruct a risky choice problem with alternatives \\(\\boldsymbol{\\pi}^{(s)}\\) and \\(\\boldsymbol{\\pi}^{(t)}\\) where the expected utility differences satisfy: \\[\n\\eta^{(r)}_s - \\eta^{(r)}_t \\neq (\\eta')^{(r)}_s - (\\eta')^{(r)}_t\n\\] (This is achievable since \\(\\boldsymbol{\\eta} \\neq \\boldsymbol{\\eta}'\\).) The choice probability is: \\[\n\\chi_1 = \\frac{\\exp(\\alpha \\cdot \\eta^{(r)}_s)}{\\exp(\\alpha \\cdot \\eta^{(r)}_s) + \\exp(\\alpha \\cdot \\eta^{(r)}_t)} = \\sigma\\bigl(\\alpha(\\eta^{(r)}_s - \\eta^{(r)}_t)\\bigr)\n\\] where \\(\\sigma\\) is the logistic function. Since \\(\\sigma\\) is strictly monotonic and the utility differences differ, we have \\(\\chi_1(\\theta) \\neq \\chi_1(\\theta')\\) (regardless of whether \\(\\alpha = \\alpha'\\) or not).\nCase 2: \\(\\delta = \\delta'\\) but \\(\\alpha \\neq \\alpha'\\).\nConsider any risky choice problem with two alternatives having distinct expected utilities \\(\\eta_1 \\neq \\eta_2\\). The choice probability for alternative 1 is: \\[\n\\chi_1 = \\sigma\\bigl(\\alpha(\\eta_1 - \\eta_2)\\bigr)\n\\]\nThis is a strictly monotonic function of \\(\\alpha\\) when \\(\\eta_1 \\neq \\eta_2\\), so \\(\\chi_1(\\alpha) \\neq \\chi_1(\\alpha')\\).\nCase 3: \\(\\delta = \\delta'\\), \\(\\alpha = \\alpha'\\), but \\(\\beta \\neq \\beta'\\).\nSince \\(\\beta \\neq \\beta'\\) as \\(K \\times D\\) matrices, there exist indices \\((k, d)\\) such that \\(\\beta_{kd} \\neq \\beta'_{kd}\\). Since \\(\\mathcal{W}\\) spans \\(\\mathbb{R}^D\\), we can find \\(\\mathbf{w} \\in \\mathcal{W}\\) such that the difference \\((\\beta - \\beta')\\mathbf{w}\\) is not a constant vector (i.e., not a scalar multiple of \\(\\mathbf{1}\\)).\nConsider the subjective probability vectors \\(\\boldsymbol{\\psi}(\\beta, \\mathbf{w}) = \\text{softmax}(\\beta \\mathbf{w})\\) and \\(\\boldsymbol{\\psi}(\\beta', \\mathbf{w}) = \\text{softmax}(\\beta' \\mathbf{w})\\). The softmax function has the property that \\(\\text{softmax}(\\mathbf{v}) = \\text{softmax}(\\mathbf{v}')\\) if and only if \\(\\mathbf{v} - \\mathbf{v}' = c \\cdot \\mathbf{1}\\) for some scalar \\(c\\). Since \\(\\beta \\mathbf{w} - \\beta' \\mathbf{w}\\) is not a constant vector, we have \\(\\boldsymbol{\\psi}(\\beta, \\mathbf{w}) \\neq \\boldsymbol{\\psi}(\\beta', \\mathbf{w})\\).\nSince \\(\\delta = \\delta'\\) implies \\(\\boldsymbol{\\upsilon}(\\delta) = \\boldsymbol{\\upsilon}(\\delta')\\), and \\(\\boldsymbol{\\psi} \\neq \\boldsymbol{\\psi}'\\), the expected utilities differ: \\[\n\\eta^{(u)}(\\mathbf{w}) = \\boldsymbol{\\psi}^\\top \\boldsymbol{\\upsilon} \\neq (\\boldsymbol{\\psi}')^\\top \\boldsymbol{\\upsilon} = (\\eta')^{(u)}(\\mathbf{w})\n\\] (The equality would require \\(\\boldsymbol{\\psi} - \\boldsymbol{\\psi}'\\) to be orthogonal to \\(\\boldsymbol{\\upsilon}\\), which fails except on a set of measure zero since \\(\\boldsymbol{\\upsilon}\\) has distinct ordered components.)\nConstructing an uncertain choice problem with this alternative yields different choice probabilities.\nCombining the cases: For any \\(\\theta \\neq \\theta'\\), at least one of the three cases applies, guaranteeing the existence of a distinguishing decision problem. \\(\\square\\)\n\n\n0.9.4 Non-Identifiability in Model m_0\nIn contrast, model m_0 faces a more challenging identification situation because the risky choice channel is absent.\n\n\n\n\n\n\nWarningProposition (Identification Structure of m_0)\n\n\n\nIn model m_0 (uncertain choices only):\n\nThe sensitivity parameter \\(\\alpha\\) is globally identified.\nThe expected utility function \\(\\eta^{(u)}: \\mathbb{R}^D \\to [0,1]\\) is globally identified.\nThe individual components \\(\\beta\\) and \\(\\delta\\) are weakly identified through the composite mapping \\((\\beta, \\delta) \\mapsto \\eta^{(u)}(\\cdot)\\). While generic point identification holds (distinct \\((\\beta, \\delta)\\) pairs produce distinct expected utility functions except on a set of measure zero), information about these parameters accumulates more slowly from data compared to m_1, resulting in wider posterior credible intervals.\n\n\n\nProof. We establish each claim.\nClaim 1 (\\(\\alpha\\) is identified): Consider any two uncertain alternatives with features \\(\\mathbf{w}_1, \\mathbf{w}_2\\) yielding distinct expected utilities \\(\\eta_1 \\neq \\eta_2\\) (such alternatives exist generically). The choice probability is \\(\\chi_1 = \\sigma(\\alpha(\\eta_1 - \\eta_2))\\) where \\(\\sigma\\) is the logistic function. Since \\(\\sigma\\) is strictly monotonic and \\((\\eta_1 - \\eta_2) \\neq 0\\), different values of \\(\\alpha\\) yield different choice probabilities.\nClaim 2 (\\(\\eta^{(u)}(\\cdot)\\) is identified): Suppose two parameter configurations \\((\\alpha, \\beta, \\delta)\\) and \\((\\alpha', \\beta', \\delta')\\) yield the same expected utility function: \\(\\eta^{(u)}(\\mathbf{w}; \\beta, \\delta) = \\eta^{(u)}(\\mathbf{w}; \\beta', \\delta')\\) for all \\(\\mathbf{w}\\). Then for any uncertain choice problem with alternatives having features \\(\\mathbf{w}_1, \\ldots, \\mathbf{w}_n\\), the expected utilities are identical, and hence (given \\(\\alpha = \\alpha'\\) from Claim 1) the choice probabilities are identical. Conversely, if \\(\\eta^{(u)}(\\mathbf{w}; \\beta, \\delta) \\neq \\eta^{(u)}(\\mathbf{w}; \\beta', \\delta')\\) for some \\(\\mathbf{w}\\), we can construct a binary choice problem distinguishing the two configurations (by the same argument as in the m_1 proof).\nClaim 3 (weak identification of components): We analyze the information structure for identifying \\(\\beta\\) and \\(\\delta\\) separately. Consider \\(K = 3\\) with utility vector \\(\\boldsymbol{\\upsilon} = (0, \\delta_1, 1)\\). The expected utility is: \\[\n\\eta^{(u)}(\\mathbf{w}) = \\psi_2(\\beta, \\mathbf{w}) \\cdot \\delta_1 + \\psi_3(\\beta, \\mathbf{w})\n\\] where \\(\\psi_2 + \\psi_3 = 1 - \\psi_1\\). The softmax probabilities \\(\\psi_k\\) depend on \\(\\beta\\) only through the differences \\(\\boldsymbol{\\beta}_k - \\boldsymbol{\\beta}_1\\) (the softmax is invariant to adding a constant to all logits). Thus \\(\\beta\\) has a \\((K-1) \\times D = 2D\\) dimensional “effective” parameter space for determining \\(\\psi\\).\nThe expected utility function \\(\\eta^{(u)}: \\mathbb{R}^D \\to [0,1]\\) is determined by how \\(\\psi_2\\) and \\(\\psi_3\\) vary with \\(\\mathbf{w}\\), weighted by \\(\\delta_1\\) and \\(1\\) respectively. The question is whether distinct \\((\\beta, \\delta)\\) pairs can produce identical \\(\\eta^{(u)}(\\cdot)\\) functions.\nGeneric point identification holds: For the equality \\(\\psi_2(\\beta, \\mathbf{w}) \\cdot \\delta_1 + \\psi_3(\\beta, \\mathbf{w}) = \\psi_2(\\beta', \\mathbf{w}) \\cdot \\delta'_1 + \\psi_3(\\beta', \\mathbf{w})\\) to hold for all \\(\\mathbf{w} \\in \\mathbb{R}^D\\), we would need a precise functional relationship between \\((\\beta, \\delta)\\) and \\((\\beta', \\delta')\\) that holds across the entire feature space. This occurs only on a set of measure zero in the parameter space.\nWeak identification in practice: While generic point identification holds, the rate at which information accumulates about \\(\\beta\\) and \\(\\delta\\) separately is slower than in m_1. In m_0, all information about \\(\\delta\\) must be extracted through uncertain choices where \\(\\beta\\) and \\(\\delta\\) interact multiplicatively in the expected utility. This interaction means that uncertainty about \\(\\beta\\) propagates to uncertainty about \\(\\delta\\) and vice versa, resulting in slower posterior concentration. \\(\\square\\)\nDiscussion. The proposition clarifies what m_0 can and cannot identify:\n\nWhat is strongly identified: The sensitivity \\(\\alpha\\) and the expected utility function \\(\\eta^{(u)}(\\cdot)\\) are fully determined by choice data. This means m_0 can predict choice behavior for any new uncertain alternative, given its features.\nWhat is weakly identified: The decomposition of \\(\\eta^{(u)}\\) into beliefs (\\(\\psi\\), via \\(\\beta\\)) and utilities (\\(\\upsilon\\), via \\(\\delta\\)) faces a more challenging inference problem. While distinct \\((\\beta, \\delta)\\) pairs generically produce distinct expected utility functions (point identification), the information geometry is unfavorable: \\(\\beta\\) and \\(\\delta\\) interact multiplicatively in the expected utility, so uncertainty in one parameter propagates to the other. In m_1, risky choices directly constrain \\(\\delta\\) through the likelihood term \\(\\sum_n \\log \\chi^{(r)}_{n, z_n}\\), which depends on \\(\\delta\\) but not \\(\\beta\\). This separation breaks the multiplicative coupling and allows faster learning.\nPractical consequence: The weak identification of \\(\\delta\\) in m_0 manifests as slower posterior concentration—wider credible intervals that shrink more slowly with sample size compared to m_1. This matches our empirical parameter recovery findings. Note that this is a rate phenomenon, not a fundamental non-identification: with sufficient data, m_0 posteriors do concentrate on the true values.\n\n\n\n0.9.5 Implications for Experimental Design\nThe identification results have direct implications for experimental design:\n\n\n\n\n\n\n\nDesign Choice\nEffect on Identification\n\n\n\n\nInclude risky choices with diverse lotteries\nEnables direct identification of \\(\\delta\\) (and hence \\(\\upsilon\\))\n\n\nUse spanning set of probability vectors\nEnsures full utility function is identified, not just projections\n\n\nUse spanning set of feature vectors\nEnsures full \\(\\beta\\) matrix is identified\n\n\nInclude both simple (degenerate) and mixed lotteries\nProvides both “anchoring” and “interpolation” information about utilities\n\n\n\n\n\n\n\n\n\nNotePractical Guidance\n\n\n\nFor m_1, the spanning conditions are easily satisfied:\n\nRisky alternatives: Include the \\(K\\) “corner” lotteries \\(\\mathbf{e}_1, \\ldots, \\mathbf{e}_K\\) (certain outcomes) plus several interior lotteries. The corners alone provide \\(K\\) linearly independent vectors.\nUncertain alternatives: Use feature vectors that vary across all \\(D\\) dimensions. Random draws from a continuous distribution (as in our study design) satisfy this almost surely.\n\nThe key practical constraint is whether risky choice tasks are feasible and meaningful in the application domain.\n\n\n\n\n0.9.6 Remarks on the Identification Results\nThe identification analysis clarifies why m_1 achieves better parameter recovery than m_0:\n\nSeparation of constraints: In m_1, risky choices provide constraints on \\((\\alpha, \\delta)\\) that are independent of \\(\\beta\\). This separation means that information about utilities accumulates directly from risky choice data, without needing to simultaneously infer the belief-formation process.\nThe role of known probabilities: The key advantage of risky alternatives is not merely that they add more data, but that their probability vectors are given rather than inferred. This eliminates one layer of the inference problem.\nConnection to classical results: Our identification result for m_1 is related to—but distinct from—the Anscombe-Aumann representation theorem. That theorem establishes when preferences can be represented by subjective expected utility; our result establishes when the parameters of a specific SEU model can be recovered from choice data. The underlying intuition is similar: mixing objective and subjective probabilities provides complementary information."
  },
  {
    "objectID": "foundations/03_prior_analysis.html",
    "href": "foundations/03_prior_analysis.html",
    "title": "Prior Predictive Analysis",
    "section": "",
    "text": "Having established the abstract formulation (Report 1) and its concrete implementation (Report 2), we now examine the prior predictive distribution: what range of choice behaviors does the model permit before observing any data?\nPrior predictive analysis serves several purposes:\n\nPrior validation: Do the priors produce sensible behaviors?\nModel understanding: What parameter combinations are a priori plausible?\nExperimental design: Is the design rich enough to distinguish different behaviors?\n\n\n\n\n\n\n\nNoteThe Prior Predictive Distribution\n\n\n\nThe prior predictive distribution is the distribution over observables (choices) induced by:\n\nDrawing parameters from the prior: \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\delta}) \\sim p(\\theta)\\)\nComputing derived quantities: \\(\\boldsymbol{\\psi}, \\boldsymbol{\\upsilon}, \\boldsymbol{\\eta}, \\boldsymbol{\\chi}\\)\nSimulating choices: \\(y_m \\sim \\text{Categorical}(\\boldsymbol{\\chi}_m)\\)\n\nThis gives us a distribution over possible datasets before conditioning on actual observations."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#introduction",
    "href": "foundations/03_prior_analysis.html#introduction",
    "title": "Prior Predictive Analysis",
    "section": "",
    "text": "Having established the abstract formulation (Report 1) and its concrete implementation (Report 2), we now examine the prior predictive distribution: what range of choice behaviors does the model permit before observing any data?\nPrior predictive analysis serves several purposes:\n\nPrior validation: Do the priors produce sensible behaviors?\nModel understanding: What parameter combinations are a priori plausible?\nExperimental design: Is the design rich enough to distinguish different behaviors?\n\n\n\n\n\n\n\nNoteThe Prior Predictive Distribution\n\n\n\nThe prior predictive distribution is the distribution over observables (choices) induced by:\n\nDrawing parameters from the prior: \\((\\alpha, \\boldsymbol{\\beta}, \\boldsymbol{\\delta}) \\sim p(\\theta)\\)\nComputing derived quantities: \\(\\boldsymbol{\\psi}, \\boldsymbol{\\upsilon}, \\boldsymbol{\\eta}, \\boldsymbol{\\chi}\\)\nSimulating choices: \\(y_m \\sim \\text{Categorical}(\\boldsymbol{\\chi}_m)\\)\n\nThis gives us a distribution over possible datasets before conditioning on actual observations."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#study-design",
    "href": "foundations/03_prior_analysis.html#study-design",
    "title": "Prior Predictive Analysis",
    "section": "0.2 Study Design",
    "text": "0.2 Study Design\nWe use a moderately complex study design to illustrate the prior predictive distribution:\n\n\nShow code\n# Study design configuration\nconfig = {\n    \"M\": 25,                    # Number of decision problems\n    \"K\": 3,                     # Number of consequences\n    \"D\": 5,                     # Feature dimensions\n    \"R\": 15,                    # Number of distinct alternatives\n    \"min_alts_per_problem\": 2,  # Minimum alternatives per problem\n    \"max_alts_per_problem\": 5,  # Maximum alternatives per problem\n    \"feature_dist\": \"normal\",   # Feature distribution\n    \"feature_params\": {\"loc\": 0, \"scale\": 1}\n}\n\nprint(f\"Study Design Configuration:\")\nprint(f\"  Decision problems (M): {config['M']}\")\nprint(f\"  Consequences (K): {config['K']}\")\nprint(f\"  Feature dimensions (D): {config['D']}\")\nprint(f\"  Distinct alternatives (R): {config['R']}\")\nprint(f\"  Alternatives per problem: {config['min_alts_per_problem']}-{config['max_alts_per_problem']}\")\n\n\nStudy Design Configuration:\n  Decision problems (M): 25\n  Consequences (K): 3\n  Feature dimensions (D): 5\n  Distinct alternatives (R): 15\n  Alternatives per problem: 2-5\n\n\n\n\nShow code\nfrom utils.study_design import StudyDesign\n\n# Create and generate the study design\nstudy = StudyDesign(\n    M=config[\"M\"],\n    K=config[\"K\"],\n    D=config[\"D\"],\n    R=config[\"R\"],\n    min_alts_per_problem=config[\"min_alts_per_problem\"],\n    max_alts_per_problem=config[\"max_alts_per_problem\"],\n    feature_dist=config[\"feature_dist\"],\n    feature_params=config[\"feature_params\"],\n    design_name=\"prior_analysis\"\n)\nstudy.generate()\n\n\n\n\n\nGenerated Study Design:\n  Total alternatives across problems: 87\n  Alternatives per problem: min=2, max=5, mean=3.5\n  Alternative appearances: min=3, max=9, mean=5.8\n\n\n\n0.2.1 Design Characteristics\n\n\nShow code\nM = config[\"M\"]\nR = config[\"R\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Alternatives per problem\naxes[0].bar(range(1, M+1), N, color='steelblue', alpha=0.7)\naxes[0].axhline(y=N.mean(), color='red', linestyle='--', label=f'Mean = {N.mean():.1f}')\naxes[0].set_xlabel('Decision Problem')\naxes[0].set_ylabel('Number of Alternatives')\naxes[0].set_title('Alternatives per Problem')\naxes[0].legend()\n\n# Alternative frequency\nalt_freq = I.sum(axis=0)\naxes[1].bar(range(1, R+1), alt_freq, color='coral', alpha=0.7)\naxes[1].axhline(y=alt_freq.mean(), color='red', linestyle='--', label=f'Mean = {alt_freq.mean():.1f}')\naxes[1].set_xlabel('Alternative')\naxes[1].set_ylabel('Frequency (# problems)')\naxes[1].set_title('Alternative Appearance Frequency')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Study design characteristics. Left: Number of alternatives in each decision problem. Right: Frequency with which each alternative appears across problems."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#prior-predictive-simulation-using-m_0_sim.stan",
    "href": "foundations/03_prior_analysis.html#prior-predictive-simulation-using-m_0_sim.stan",
    "title": "Prior Predictive Analysis",
    "section": "0.3 Prior Predictive Simulation Using m_0_sim.stan",
    "text": "0.3 Prior Predictive Simulation Using m_0_sim.stan\nWe use the simulation model m_0_sim.stan to generate prior predictive samples. This ensures that our analysis matches exactly how the model generates data, including all numerical details of the Stan implementation.\n\n\nShow code\nfrom analysis.prior_predictive import PriorPredictiveAnalysis\nimport tempfile\n\n# Create a temporary output directory for this analysis\noutput_dir = tempfile.mkdtemp(prefix=\"prior_pred_\")\n\n# Run prior predictive analysis using m_0_sim.stan\nanalysis = PriorPredictiveAnalysis(\n    model_path=None,  # Uses default m_0_sim.stan\n    study_design=study,\n    output_dir=output_dir,\n    n_param_samples=200,    # Number of parameter configurations\n    n_choice_samples=5      # Choice samples per parameter config\n)\n\n# Run the analysis (this calls Stan)\nsamples = analysis.run()\n\n\n\n\nPrior Predictive Simulation Complete:\n  Parameter configurations sampled: 200\n  Total samples: 1000\n  Columns available: 676"
  },
  {
    "objectID": "foundations/03_prior_analysis.html#prior-distribution-of-parameters",
    "href": "foundations/03_prior_analysis.html#prior-distribution-of-parameters",
    "title": "Prior Predictive Analysis",
    "section": "0.4 Prior Distribution of Parameters",
    "text": "0.4 Prior Distribution of Parameters\n\n0.4.1 Sensitivity Parameter (\\(\\alpha\\))\nThe sensitivity parameter is sampled from a Lognormal(0, 1) prior in m_0_sim.stan:\n\n\nShow code\n# Extract unique alpha values (one per parameter set)\nalpha_samples = samples.groupby('param_set')['alpha'].first().values\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Histogram with truncation for visualization\nalpha_plot = alpha_samples[alpha_samples &lt; 15]\naxes[0].hist(alpha_plot, bins=40, density=True, alpha=0.7, color='steelblue', edgecolor='white')\naxes[0].axvline(x=np.median(alpha_samples), color='red', linestyle='--', linewidth=2, \n                label=f'Median = {np.median(alpha_samples):.2f}')\naxes[0].axvline(x=1, color='orange', linestyle=':', linewidth=2, label='α = 1')\naxes[0].set_xlabel('Sensitivity (α)')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Prior Distribution of α')\naxes[0].legend()\naxes[0].set_xlim(0, 15)\n\n# Log scale to see full distribution\naxes[1].hist(np.log10(alpha_samples + 0.01), bins=40, density=True, alpha=0.7, \n             color='steelblue', edgecolor='white')\naxes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='α = 1')\naxes[1].set_xlabel('log₁₀(α)')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Prior Distribution of α (log scale)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nAlpha summary statistics:\")\nprint(f\"  Mean: {np.mean(alpha_samples):.2f}\")\nprint(f\"  Median: {np.median(alpha_samples):.2f}\")\nprint(f\"  Std: {np.std(alpha_samples):.2f}\")\nprint(f\"  95% interval: [{np.percentile(alpha_samples, 2.5):.2f}, {np.percentile(alpha_samples, 97.5):.2f}]\")\n\n\n\n\n\n\n\n\nFigure 2: Prior distribution of the sensitivity parameter α sampled from m_0_sim.stan. The distribution has median ≈ 1 with substantial probability mass on both low sensitivity (random-like choice) and high sensitivity (near-optimal choice).\n\n\n\n\n\n\nAlpha summary statistics:\n  Mean: 1.58\n  Median: 0.96\n  Std: 1.87\n  95% interval: [0.15, 6.34]\n\n\nThe lognormal prior covers a wide range of sensitivity values:\n\nLow α (&lt; 0.5): Near-random choice, weak sensitivity to SEU\nModerate α (0.5–3): Probabilistic choice with clear SEU preference\nHigh α (&gt; 5): Near-deterministic choice, strong SEU maximization\n\n\n\n0.4.2 Utility Parameters\n\n\nShow code\n# Extract unique delta and upsilon values\ndelta_cols = [col for col in samples.columns if col.startswith('delta[')]\nupsilon_cols = [col for col in samples.columns if col.startswith('upsilon[')]\n\n# Get one value per parameter set\nparam_samples = samples.groupby('param_set').first()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Middle utility distribution (upsilon[2])\nif 'upsilon[2]' in param_samples.columns:\n    upsilon_2 = param_samples['upsilon[2]'].values\n    axes[0].hist(upsilon_2, bins=40, density=True, alpha=0.7, color='forestgreen', edgecolor='white')\n    axes[0].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label=f'Mean ≈ 0.5')\n    axes[0].set_xlabel('Middle Utility (υ₂)')\n    axes[0].set_ylabel('Density')\n    axes[0].set_title('Prior Distribution of υ₂')\n    axes[0].legend()\n    axes[0].set_xlim(0, 1)\n\n# Delta distribution\nif delta_cols:\n    delta_1 = param_samples[delta_cols[0]].values\n    delta_2 = param_samples[delta_cols[1]].values if len(delta_cols) &gt; 1 else 1 - delta_1\n    \n    axes[1].scatter(delta_1, delta_2, alpha=0.3, s=20, c='forestgreen')\n    axes[1].plot([0, 1], [1, 0], 'r--', linewidth=2, label='δ₁ + δ₂ = 1')\n    axes[1].set_xlabel('δ₁ (first increment)')\n    axes[1].set_ylabel('δ₂ (second increment)')\n    axes[1].set_title('Prior Distribution on Utility Simplex')\n    axes[1].set_xlim(-0.05, 1.05)\n    axes[1].set_ylim(-0.05, 1.05)\n    axes[1].legend()\n    axes[1].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Prior distribution of utilities under Dirichlet(1,1) as sampled from m_0_sim.stan. Left: Distribution of the middle utility υ₂. Right: Distribution of utility increments δ.\n\n\n\n\n\nThe symmetric Dirichlet(1,1) prior induces a uniform distribution over the simplex of valid utility configurations.\n\n\n0.4.3 Feature-to-Probability Weights (\\(\\boldsymbol{\\beta}\\))\nThe matrix \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^{K \\times D}\\) maps alternative features to (unnormalized) log-probabilities over consequences. In m_0_sim.stan, each element is drawn independently from \\(\\text{Normal}(0, \\sigma_\\beta)\\):\n\n\nShow code\n# Extract beta coefficients\nbeta_cols = [col for col in samples.columns if col.startswith('beta[')]\n\n# Get unique values per parameter set\nparam_samples = samples.groupby('param_set').first()\n\n# Collect all beta values\nall_betas = []\nfor col in beta_cols:\n    all_betas.extend(param_samples[col].values)\nall_betas = np.array(all_betas)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Distribution of all beta coefficients\naxes[0].hist(all_betas, bins=50, density=True, alpha=0.7, color='purple', edgecolor='white')\n# Overlay theoretical normal\nx_range = np.linspace(all_betas.min(), all_betas.max(), 100)\naxes[0].plot(x_range, stats.norm.pdf(x_range, 0, 1), 'r-', linewidth=2, label='Normal(0,1)')\naxes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\naxes[0].set_xlabel('β coefficient value')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Prior Distribution of β Coefficients')\naxes[0].legend()\n\n# Heatmap of median beta values (not absolute value)\nK, D = config['K'], config['D']\nbeta_medians = np.zeros((K, D))\nfor k in range(K):\n    for d in range(D):\n        col = f'beta[{k+1},{d+1}]'\n        if col in param_samples.columns:\n            beta_medians[k, d] = np.median(param_samples[col].values)\n\nim = axes[1].imshow(beta_medians, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\naxes[1].set_xlabel('Feature Dimension (d)')\naxes[1].set_ylabel('Consequence (k)')\naxes[1].set_title('Median β by Position')\naxes[1].set_xticks(range(D))\naxes[1].set_xticklabels([f'{d+1}' for d in range(D)])\naxes[1].set_yticks(range(K))\naxes[1].set_yticklabels([f'{k+1}' for k in range(K)])\nplt.colorbar(im, ax=axes[1], label='Median β')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBeta coefficient summary:\")\nprint(f\"  Mean: {np.mean(all_betas):.3f}\")\nprint(f\"  Std: {np.std(all_betas):.3f}\")\nprint(f\"  95% interval: [{np.percentile(all_betas, 2.5):.2f}, {np.percentile(all_betas, 97.5):.2f}]\")\n\n\n\n\n\n\n\n\nFigure 4: Prior distribution of the feature-to-probability weights β sampled from m_0_sim.stan. Left: Distribution of all β coefficients. Right: Heatmap of median β values by position.\n\n\n\n\n\n\nBeta coefficient summary:\n  Mean: 0.016\n  Std: 0.980\n  95% interval: [-1.90, 1.94]\n\n\nThe prior on \\(\\boldsymbol{\\beta}\\) is deliberately uninformative—it does not favor any particular feature-consequence relationship. This allows the data to determine how features predict consequences.\n\n\n0.4.4 Subjective Probabilities Over Consequences (\\(\\boldsymbol{\\psi}\\))\nThe weights \\(\\boldsymbol{\\beta}\\) interact with alternative features to produce subjective probabilities over consequences via the softmax transformation: \\[\n\\psi_{rk} = \\frac{\\exp(\\boldsymbol{\\beta}_k^\\top \\mathbf{x}_r)}{\\sum_{k'=1}^K \\exp(\\boldsymbol{\\beta}_{k'}^\\top \\mathbf{x}_r)}\n\\]\nWe compute \\(\\boldsymbol{\\psi}\\) for each of the \\(R\\) distinct alternatives from the sampled \\(\\boldsymbol{\\beta}\\) values.\n\n\nShow code\n# Compute psi from beta for the R distinct alternatives\n# (The psi in Stan's generated quantities is vectorized over problems, causing repetition)\nK = config['K']\nR = config['R']\nD = config['D']\n\n# Feature matrix for distinct alternatives\nw_array = np.array(study.w)  # Shape: (R, D)\n\n# Get unique parameter sets\nparam_sets = samples['param_set'].unique()\nn_param_samples = len(param_sets)\n\n# For each parameter sample, compute psi for all R distinct alternatives\n# Shape: (n_param_samples, R, K)\npsi_array = np.zeros((n_param_samples, R, K))\n\nfor p_idx, param_set in enumerate(param_sets):\n    # Get one row from this param_set (parameters are same within a set)\n    row = samples[samples['param_set'] == param_set].iloc[0]\n    \n    # Extract beta matrix\n    beta = np.zeros((K, D))\n    for k in range(K):\n        for d in range(D):\n            col = f'beta[{k+1},{d+1}]'\n            if col in samples.columns:\n                beta[k, d] = row[col]\n    \n    # Compute psi for each distinct alternative\n    for r in range(R):\n        logits = beta @ w_array[r]  # K-dimensional\n        psi_array[p_idx, r] = np.exp(logits - np.max(logits))  # numerically stable softmax\n        psi_array[p_idx, r] /= psi_array[p_idx, r].sum()\n\n# Compute mean psi across samples for each (alternative, consequence)\nmean_psi = psi_array.mean(axis=0)  # Shape: (R, K)\nall_psi_values = psi_array.flatten()\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\n# Heatmap of mean psi for each distinct alternative and consequence\nim = ax.imshow(mean_psi.T, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\nax.set_xlabel('Distinct Alternative (r)')\nax.set_ylabel('Consequence (k)')\nax.set_title('Mean ψ by Alternative and Consequence')\nax.set_yticks(range(K))\nax.set_yticklabels([f'{k+1}' for k in range(K)])\n# Use odd numbers for x-axis labels (1, 3, 5, 7, ...)\nxtick_positions = list(range(0, R, 2))  # 0, 2, 4, 6, ... (indices)\nax.set_xticks(xtick_positions)\nax.set_xticklabels([f'{x+1}' for x in xtick_positions])  # 1, 3, 5, 7, ... (labels)\nplt.colorbar(im, ax=ax, label='Mean ψ')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nSubjective probability summary (from {n_param_samples} prior parameter draws):\")\nprint(f\"  Mean ψ: {np.mean(all_psi_values):.3f} (theoretical uniform = {1/K:.3f})\")\nprint(f\"  Std ψ: {np.std(all_psi_values):.3f}\")\nprint(f\"  P(ψ &lt; 0.1): {np.mean(all_psi_values &lt; 0.1):.3f}\")\nprint(f\"  P(ψ &gt; 0.9): {np.mean(all_psi_values &gt; 0.9):.3f}\")\n\n\n\n\n\n\n\n\nFigure 5: Mean subjective probability for each distinct alternative and consequence, averaged over all prior draws.\n\n\n\n\n\n\nSubjective probability summary (from 200 prior parameter draws):\n  Mean ψ: 0.333 (theoretical uniform = 0.333)\n  Std ψ: 0.321\n  P(ψ &lt; 0.1): 0.357\n  P(ψ &gt; 0.9): 0.081\n\n\n\n\n\n\n\n\nImportantInterpretation of Subjective Probabilities\n\n\n\nThe vector \\(\\boldsymbol{\\psi}_r\\) represents the decision-maker’s subjective beliefs about which consequence will obtain if alternative \\(r\\) is chosen. The Normal(0,1) prior on \\(\\boldsymbol{\\beta}\\) induces a prior on \\(\\boldsymbol{\\psi}\\) that:\n\nCenters on uniformity when features are balanced\nAllows extreme beliefs (\\(\\psi_{rk} \\approx 0\\) or \\(\\approx 1\\)) when features strongly predict consequences\nVaries across alternatives based on their feature profiles\n\nThis captures the idea that alternatives with different feature profiles may induce quite different probability distributions over consequences.\n\n\n\n\nShow code\n# Show how psi varies across distinct alternatives for a few parameter draws\n# We already computed psi_array above with shape (n_param_samples, R, K)\n\nfig, axes = plt.subplots(2, 3, figsize=(14, 8))\naxes = axes.flatten()\n\n# Select 6 diverse parameter draws\nnp.random.seed(123)\nsample_indices = np.random.choice(n_param_samples, min(6, n_param_samples), replace=False)\n\nfor idx, (ax, sample_idx) in enumerate(zip(axes, sample_indices)):\n    # Get psi matrix for this parameter draw (already computed above)\n    psi_matrix = psi_array[sample_idx]  # Shape: (R, K)\n    \n    # Heatmap\n    im = ax.imshow(psi_matrix.T, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n    ax.set_xlabel('Distinct Alternative (r)')\n    ax.set_ylabel('Consequence')\n    ax.set_title(f'Prior Draw {idx+1}', fontsize=11)\n    ax.set_yticks(range(K))\n    ax.set_yticklabels([f'k={k+1}' for k in range(K)])\n    # Show fewer x-ticks for readability\n    n_xticks = min(8, R)\n    xtick_positions = np.linspace(0, R-1, n_xticks, dtype=int)\n    ax.set_xticks(xtick_positions)\n    ax.set_xticklabels([f'{x+1}' for x in xtick_positions])\n\n# Add a shared colorbar\nfig.subplots_adjust(right=0.88)\ncbar_ax = fig.add_axes([0.91, 0.15, 0.02, 0.7])\nfig.colorbar(im, cax=cbar_ax, label='ψ (subjective probability)')\n\nplt.suptitle('Subjective Probability Distributions Under Individual Prior Draws', \n             fontsize=13, y=0.98)\nplt.tight_layout(rect=[0, 0, 0.88, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Heatmaps showing subjective probability distributions across the R distinct alternatives for six individual prior draws. Each heatmap shows how ψ varies across alternatives (columns) and consequences (rows) under a single parameter configuration.\n\n\n\n\n\nThe diversity of \\(\\boldsymbol{\\psi}\\) profiles across alternatives—driven by variation in \\(\\boldsymbol{\\beta}\\)—is what allows the model to capture differences in how decision-makers perceive distinct alternatives."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#expected-utilities-under-the-prior",
    "href": "foundations/03_prior_analysis.html#expected-utilities-under-the-prior",
    "title": "Prior Predictive Analysis",
    "section": "0.5 Expected Utilities Under the Prior",
    "text": "0.5 Expected Utilities Under the Prior\nThe expected utilities \\(\\eta_r = \\boldsymbol{\\psi}_r^\\top \\boldsymbol{\\upsilon}\\) depend on both the subjective probabilities (\\(\\boldsymbol{\\psi}\\), derived from \\(\\boldsymbol{\\beta}\\)) and the utility vector (\\(\\boldsymbol{\\upsilon}\\)).\n\n\nShow code\n# Extract problem_etas columns\neta_cols = [col for col in samples.columns if 'problem_etas' in col]\n\n# Get data for first 10 problems\nn_problems_to_show = min(10, config[\"M\"])\n\nfig, axes = plt.subplots(2, 5, figsize=(15, 8))\naxes = axes.flatten()\n\nfor m in range(n_problems_to_show):\n    ax = axes[m]\n    \n    # Get columns for this problem (1-indexed in Stan)\n    problem_cols = [col for col in eta_cols if f'problem_etas[{m+1},' in col]\n    \n    # Filter out padding values (very negative)\n    valid_data = []\n    valid_labels = []\n    for col in problem_cols:\n        data = samples[col].values\n        if np.median(data) &gt; -1e9:  # Not padding\n            valid_data.append(data)\n            alt_idx = col.split(',')[1].rstrip(']')\n            valid_labels.append(alt_idx)\n    \n    if valid_data:\n        bp = ax.boxplot(valid_data, patch_artist=True)\n        for patch in bp['boxes']:\n            patch.set_facecolor('steelblue')\n            patch.set_alpha(0.7)\n        ax.set_xticklabels(valid_labels)\n        ax.set_title(f'Problem {m+1}')\n        ax.set_xlabel('Alternative')\n        ax.set_ylabel('η')\n        ax.set_ylim(0, 1)\n\nplt.suptitle('Prior Distribution of Expected Utilities by Problem', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Distribution of expected utilities across a subset of decision problems. Each boxplot shows the distribution of η for alternatives in that problem across prior samples."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#seu-maximizer-selection",
    "href": "foundations/03_prior_analysis.html#seu-maximizer-selection",
    "title": "Prior Predictive Analysis",
    "section": "0.6 SEU Maximizer Selection",
    "text": "0.6 SEU Maximizer Selection\nA key diagnostic is the probability of selecting the SEU-maximizing alternative under the prior. The simulation model m_0_sim.stan tracks this directly via the selected_seu_max variable.\n\n\n\n\n\n\nNoteThe Random Baseline\n\n\n\nFor each decision problem with \\(N_m\\) alternatives, a random chooser would select the SEU maximizer with probability \\(1/N_m\\) (assuming a unique maximizer). We use this as a baseline: values above \\(1/N_m\\) indicate that the prior allows for above-chance SEU-maximizing behavior. The baseline varies across problems because problems have different numbers of alternatives.\n\n\n\n0.6.1 By Decision Problem\n\n\nShow code\n# Extract SEU maximizer selection for each problem\nseu_max_cols = [col for col in samples.columns if 'selected_seu_max' in col and 'total' not in col]\n\nM = config[\"M\"]\nprob_seu_max = np.zeros(M)\n\nfor m in range(M):\n    col = f'selected_seu_max[{m+1}]'\n    if col in samples.columns:\n        prob_seu_max[m] = samples[col].mean()\n\n# Random baseline\nrandom_baseline = 1.0 / N\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\nx = np.arange(1, M+1)\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, prob_seu_max, width, label='Prior Predictive', \n               color='steelblue', alpha=0.7)\nbars2 = ax.bar(x + width/2, random_baseline, width, label='Random Choice Baseline', \n               color='coral', alpha=0.7)\n\nax.set_xlabel('Decision Problem')\nax.set_ylabel('P(SEU Maximizer Selected)')\nax.set_title('Probability of Selecting SEU Maximizer by Problem')\nax.set_xticks(x)\nax.legend()\nax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nSummary:\")\nprint(f\"  Mean P(SEU max): {prob_seu_max.mean():.3f}\")\nprint(f\"  Mean random baseline: {random_baseline.mean():.3f}\")\nprint(f\"  Ratio (observed/random): {prob_seu_max.mean() / random_baseline.mean():.2f}x\")\n\n\n\n\n\n\n\n\nFigure 8: Probability of selecting the SEU-maximizing alternative for each decision problem, computed from m_0_sim.stan samples. The coral bars show the random choice baseline (1/N for each problem).\n\n\n\n\n\n\nSummary:\n  Mean P(SEU max): 0.433\n  Mean random baseline: 0.320\n  Ratio (observed/random): 1.35x\n\n\n\n\n0.6.2 Total SEU Maximizers Selected\n\n\nShow code\n# Get total SEU max selected\nif 'total_seu_max_selected' in samples.columns:\n    total_seu_max = samples['total_seu_max_selected'].values\nelse:\n    # Compute from individual columns\n    total_seu_max = np.zeros(len(samples))\n    for m in range(M):\n        col = f'selected_seu_max[{m+1}]'\n        if col in samples.columns:\n            total_seu_max += samples[col].values\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.hist(total_seu_max, bins=range(0, M+2), density=True, alpha=0.7, \n        color='steelblue', edgecolor='white', align='left')\n\n# Expected under random choice\nexpected_random = sum(1/n for n in N)\nax.axvline(x=expected_random, color='coral', linestyle='--', linewidth=2, \n           label=f'Expected (random): {expected_random:.1f}')\nax.axvline(x=total_seu_max.mean(), color='red', linestyle='-', linewidth=2,\n           label=f'Prior mean: {total_seu_max.mean():.1f}')\n\nax.set_xlabel(f'Number of SEU Maximizers Selected (out of {M})')\nax.set_ylabel('Density')\nax.set_title('Prior Predictive Distribution of Total SEU Maximizer Selections')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTotal SEU Maximizer Selection Summary:\")\nprint(f\"  Mean: {total_seu_max.mean():.1f}\")\nprint(f\"  Std: {total_seu_max.std():.1f}\")\nprint(f\"  Min: {total_seu_max.min():.0f}\")\nprint(f\"  Max: {total_seu_max.max():.0f}\")\nprint(f\"  Expected under random: {expected_random:.1f}\")\n\n\n\n\n\n\n\n\nFigure 9: Distribution of the total number of SEU maximizers selected (out of 25 problems) across prior samples. This is computed directly by m_0_sim.stan.\n\n\n\n\n\n\nTotal SEU Maximizer Selection Summary:\n  Mean: 10.8\n  Std: 3.5\n  Min: 3\n  Max: 23\n  Expected under random: 8.0\n\n\n\n\n0.6.3 Relationship with \\(\\alpha\\)\n\n\nShow code\n# Get alpha and total SEU max for each sample\nalpha_per_sample = samples['alpha'].values\nseu_max_per_sample = total_seu_max / M  # Proportion\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Scatter plot\naxes[0].scatter(alpha_per_sample, seu_max_per_sample, alpha=0.2, s=15, c='steelblue')\naxes[0].set_xlabel('Sensitivity (α)')\naxes[0].set_ylabel('Proportion SEU Max Selected')\naxes[0].set_title('SEU Maximizer Selection vs. Sensitivity')\naxes[0].set_xlim(0, min(15, np.percentile(alpha_per_sample, 99)))\n\n# Binned averages\nalpha_bins = [0, 0.25, 0.5, 1, 2, 3, 5, 10, 100]\nbin_centers = []\nprop_means = []\nprop_stds = []\n\nfor i in range(len(alpha_bins) - 1):\n    mask = (alpha_per_sample &gt;= alpha_bins[i]) & (alpha_per_sample &lt; alpha_bins[i+1])\n    if mask.sum() &gt; 5:\n        bin_centers.append((alpha_bins[i] + alpha_bins[i+1])/2)\n        prop_means.append(seu_max_per_sample[mask].mean())\n        prop_stds.append(seu_max_per_sample[mask].std())\n\naxes[1].errorbar(bin_centers, prop_means, yerr=prop_stds, fmt='o-', \n                 color='steelblue', linewidth=2, markersize=8, capsize=5)\naxes[1].axhline(y=1/np.mean(N), color='coral', linestyle='--', \n                label=f'Random baseline: {1/np.mean(N):.2f}')\naxes[1].set_xlabel('Sensitivity (α)')\naxes[1].set_ylabel('Mean Proportion SEU Max Selected')\naxes[1].set_title('SEU Maximizer Selection vs. α (Binned)')\naxes[1].set_xscale('log')\naxes[1].legend()\naxes[1].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Relationship between sensitivity α and the proportion of SEU maximizers selected. Higher α leads to more consistent SEU maximization, as predicted by the theoretical properties from Report 1.\n\n\n\n\n\nThis plot empirically confirms Property 1 (Monotonicity) from Report 1: as \\(\\alpha\\) increases, the probability of selecting SEU-maximizing alternatives increases."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#prior-predictive-checks-are-the-priors-reasonable",
    "href": "foundations/03_prior_analysis.html#prior-predictive-checks-are-the-priors-reasonable",
    "title": "Prior Predictive Analysis",
    "section": "0.7 Prior Predictive Checks: Are the Priors Reasonable?",
    "text": "0.7 Prior Predictive Checks: Are the Priors Reasonable?\nBased on the prior predictive analysis using m_0_sim.stan, we can assess whether the default priors produce sensible behaviors:\n\n0.7.1 ✓ The Prior Covers the Full Range of Behaviors\n\n\nShow code\n# Summarize the range of behaviors\nprop_seu_max_by_sample = total_seu_max / M\n\nprint(\"Proportion of SEU Maximizers Selected (across prior samples):\")\nprint(f\"  Minimum: {prop_seu_max_by_sample.min():.2%}\")\nprint(f\"  5th percentile: {np.percentile(prop_seu_max_by_sample, 5):.2%}\")\nprint(f\"  Median: {np.median(prop_seu_max_by_sample):.2%}\")\nprint(f\"  95th percentile: {np.percentile(prop_seu_max_by_sample, 95):.2%}\")\nprint(f\"  Maximum: {prop_seu_max_by_sample.max():.2%}\")\n\n\nProportion of SEU Maximizers Selected (across prior samples):\n  Minimum: 12.00%\n  5th percentile: 24.00%\n  Median: 40.00%\n  95th percentile: 72.00%\n  Maximum: 92.00%\n\n\nThe prior places substantial probability on: - Near-random choice (~20-30% SEU max selection) - Moderate SEU-sensitivity (~40-60% SEU max selection) - Strong SEU-maximization (~80-100% SEU max selection)\n\n\n0.7.2 ✓ No Pathological Behaviors\nThe prior does not generate: - Negative expected utilities (impossible given \\(\\upsilon \\in [0,1]\\)) - Degenerate choice probabilities (softmax always assigns positive probability) - Systematic biases toward particular alternatives (symmetric prior on \\(\\boldsymbol{\\beta}\\))\n\n\n0.7.3 ✓ Alignment with Theoretical Properties\nThe simulations confirm the three properties from Report 1: 1. Monotonicity: Higher \\(\\alpha\\) → higher P(SEU max selection) ✓ 2. Perfect rationality limit: Very high \\(\\alpha\\) → near-deterministic SEU max selection ✓ 3. Random choice limit: Very low \\(\\alpha\\) → near-uniform choice ✓"
  },
  {
    "objectID": "foundations/03_prior_analysis.html#summary",
    "href": "foundations/03_prior_analysis.html#summary",
    "title": "Prior Predictive Analysis",
    "section": "0.8 Summary",
    "text": "0.8 Summary\nThe prior predictive analysis using m_0_sim.stan reveals that the default priors for model m_0 produce a sensible range of choice behaviors:\n\n\n\n\n\n\n\nAspect\nFinding\n\n\n\n\nSensitivity (\\(\\alpha\\))\nLognormal(0,1) covers low, moderate, and high sensitivity\n\n\nSEU max selection\nPrior mean ~50%, ranging from ~20% to ~100%\n\n\nChoice probabilities\nNeither too concentrated nor too diffuse\n\n\nTheoretical alignment\nAll three fundamental properties confirmed empirically\n\n\n\nThe prior is weakly informative—it does not strongly constrain parameters but rules out implausible behaviors. This is appropriate for a default prior when we have limited domain knowledge about the specific decision context.\n\n\n\n\n\n\nNoteDesign Considerations\n\n\n\nThe study design used here (M=25, K=3, D=5, R=15) is moderately complex. Key features:\n\n25 decision problems: Provides reasonable statistical power\n5 feature dimensions: Allows rich feature-to-probability mapping\n15 alternatives: Creates varied choice sets across problems\n2-5 alternatives per problem: Realistic range for typical experiments\n\nThe next report examines whether this design supports accurate parameter recovery."
  },
  {
    "objectID": "foundations/03_prior_analysis.html#references",
    "href": "foundations/03_prior_analysis.html#references",
    "title": "Prior Predictive Analysis",
    "section": "0.9 References",
    "text": "0.9 References"
  }
]