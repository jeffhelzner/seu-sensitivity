# SEU Sensitivity

![Work in Progress](https://img.shields.io/badge/status-work%20in%20progress-orange)

**Note**: This is an active research project. Code and documentation are evolving.

A Bayesian framework for modeling and analyzing decision-making behavior through the lens of Subjective Expected Utility (SEU) theory. This project provides tools for generating experimental designs, fitting computational models using Stan, and assessing the rationality of decision makersâ€”including Large Language Models (LLMs).

## Development Status

ðŸš§ **This project is currently under active development** ðŸš§

Current status:
- âœ… Core theoretical framework established
- âœ… Base Stan model (m_0) for uncertain choice implemented and tested
- âœ… Combined model (m_1) with risky and uncertain choice implemented and tested
- âœ… Separate sensitivity model (m_2) implemented and tested
- âœ… Proportional sensitivity model (m_3) implemented and tested
- âœ… Calibrated prior variant (m_01) implemented
- âœ… Study design tools functional (m_0 and m_1)
- âœ… Analysis pipeline complete (parameter recovery, SBC, prior/posterior predictive)
- âœ… Quarto-based documentation and reports
- ðŸ”„ Prompt framing study application in progress
- ðŸ”„ Temperature study application in progress
- ðŸ“ Documentation being expanded
- ðŸ”¬ Empirical validation ongoing

**Note for users**: While the core functionality is stable, API and features may change as the project evolves. Feedback and contributions are welcome!

## Overview

This framework implements a computational approach to understanding decision makers who form beliefs and make choices under uncertainty. The core insight is that we can measure an agent's "rationality" by estimating their sensitivity parameter (Î±), which governs how consistently they maximize subjective expected utility.

**Key Features:**

- **Theoretical Foundation**: Framework for softmax choice models with SEU
- **Stan Models**: Bayesian inference for rationality parameters using Hamiltonian Monte Carlo
- **Study Design Tools**: Generate and analyze experimental designs for decision studies
- **LLM Benchmarking**: Assess the rationality of Large Language Models through text-based decision problems
- **Visualization**: Create comprehensive plots and diagnostics for model results

## Project Structure

```
seu-sensitivity/
â”œâ”€â”€ reports/                 # Quarto-based documentation and reports
â”‚   â”œâ”€â”€ _quarto.yml         # Quarto project configuration
â”‚   â”œâ”€â”€ _metadata.yml       # Shared metadata for reports
â”‚   â”œâ”€â”€ index.qmd           # Main documentation index
â”‚   â”œâ”€â”€ report_utils.py     # Shared Python utilities for reports
â”‚   â”œâ”€â”€ references.bib      # Bibliography
â”‚   â”œâ”€â”€ foundations/        # Foundational theoretical reports
â”‚   â”‚   â”œâ”€â”€ 01_abstract_formulation.qmd   # Mathematical framework
â”‚   â”‚   â”œâ”€â”€ 02_concrete_implementation.qmd # Implementation details
â”‚   â”‚   â”œâ”€â”€ 03_prior_analysis.qmd         # Prior predictive analysis
â”‚   â”‚   â”œâ”€â”€ 04_parameter_recovery.qmd     # Parameter recovery study
â”‚   â”‚   â”œâ”€â”€ 05_adding_risky_choices.qmd   # m_1 model development
â”‚   â”‚   â””â”€â”€ 06_sbc_validation.qmd         # Simulation-based calibration
â”‚   â”œâ”€â”€ applications/       # Applied research reports
â”‚   â”‚   â”œâ”€â”€ prompt_framing_study/
â”‚   â”‚   â””â”€â”€ temperature_study/
â”‚   â”œâ”€â”€ blog/               # Blog-style posts
â”‚   â”œâ”€â”€ styles/             # Custom CSS/SCSS styles
â”‚   â””â”€â”€ legacy/             # Archived legacy reports
â”œâ”€â”€ models/                  # Stan model implementations
â”‚   â”œâ”€â”€ m_0.stan            # Base SEU model (uncertain choice only)
â”‚   â”œâ”€â”€ m_0_sim.stan        # m_0 simulation model
â”‚   â”œâ”€â”€ m_0_sbc.stan        # m_0 SBC model
â”‚   â”œâ”€â”€ m_01.stan           # m_0 with calibrated priors
â”‚   â”œâ”€â”€ m_01_sbc.stan       # m_01 SBC model
â”‚   â”œâ”€â”€ m_01_sim.stan       # m_01 simulation model
â”‚   â”œâ”€â”€ m_1.stan            # Combined model (risky + uncertain, shared Î±)
â”‚   â”œâ”€â”€ m_1_sim.stan        # m_1 simulation model
â”‚   â”œâ”€â”€ m_1_sbc.stan        # m_1 SBC model
â”‚   â”œâ”€â”€ m_2.stan            # Separate sensitivity model (Î± for uncertain, Ï‰ for risky)
â”‚   â”œâ”€â”€ m_2_sim.stan        # m_2 simulation model
â”‚   â”œâ”€â”€ m_2_sbc.stan        # m_2 SBC model
â”‚   â”œâ”€â”€ m_3.stan            # Proportional sensitivity model (Ï‰ = ÎºÎ±)
â”‚   â”œâ”€â”€ m_3_sim.stan        # m_3 simulation model
â”‚   â”œâ”€â”€ m_3_sbc.stan        # m_3 SBC model
â”‚   â””â”€â”€ README_m1.md        # m_1 implementation guide
â”œâ”€â”€ utils/                   # Core utilities
â”‚   â”œâ”€â”€ __init__.py         # Shared utilities, model detection
â”‚   â”œâ”€â”€ study_design.py     # Experimental design generation (m_0)
â”‚   â”œâ”€â”€ study_design_m1.py  # Extended design for m_1
â”‚   â””â”€â”€ README.md           # Utils documentation
â”œâ”€â”€ analysis/                # Analysis scripts
â”‚   â”œâ”€â”€ model_estimation.py # Model fitting utilities
â”‚   â”œâ”€â”€ parameter_recovery.py # Parameter recovery analysis
â”‚   â”œâ”€â”€ posterior_predictive_checks.py # Posterior predictive checks
â”‚   â”œâ”€â”€ prior_predictive.py # Prior predictive checks
â”‚   â”œâ”€â”€ sbc.py              # Simulation-based calibration
â”‚   â””â”€â”€ sample_size_estimation.py # Sample size planning
â”œâ”€â”€ applications/            # Applied research projects
â”‚   â”œâ”€â”€ prompt_framing_study/ # Prompt framing effects on LLM rationality
â”‚   â”œâ”€â”€ temperature_study/  # LLM temperature effects on sensitivity
â”‚   â””â”€â”€ llm_rationality/    # Legacy LLM benchmarking (deprecated)
â”œâ”€â”€ scripts/                 # Executable scripts
â”‚   â”œâ”€â”€ run_study_design.py # Generate study designs
â”‚   â”œâ”€â”€ run_m1_study_design.py # Generate m_1 study designs
â”‚   â”œâ”€â”€ run_model_estimation.py # Fit models
â”‚   â”œâ”€â”€ run_parameter_recovery.py # Run recovery analysis
â”‚   â”œâ”€â”€ run_prior_predictive.py # Prior predictive analysis
â”‚   â”œâ”€â”€ run_prior_predictive_grid.py # Prior predictive grid search
â”‚   â”œâ”€â”€ run_sbc.py          # SBC validation
â”‚   â”œâ”€â”€ run_sample_size_estimation.py # Sample size analysis
â”‚   â”œâ”€â”€ run_temperature_analysis.py # Temperature study analysis
â”‚   â”œâ”€â”€ refit_with_ppc.py   # Refit models with posterior predictive checks
â”‚   â”œâ”€â”€ copy_figures_for_report.py # Copy figures into reports
â”‚   â”œâ”€â”€ cleanup_temp_files.py # Clean up temporary files
â”‚   â””â”€â”€ test_m1_model.py    # m_1 model tests
â”œâ”€â”€ configs/                 # Configuration files for studies
â”œâ”€â”€ results/                 # Generated results and outputs
â”‚   â”œâ”€â”€ designs/            # Study designs
â”‚   â”œâ”€â”€ parameter_recovery/ # Recovery analysis results
â”‚   â”œâ”€â”€ prior_predictive/   # Prior predictive results
â”‚   â”œâ”€â”€ sample_size_estimation/ # Sample size results
â”‚   â””â”€â”€ sbc/                # SBC results
â”œâ”€â”€ prompts/                 # LLM prompt templates
â”œâ”€â”€ environment.yml          # Conda environment specification
â”œâ”€â”€ requirements.txt         # Pip requirements (alternative)
â””â”€â”€ README.md               # This file
```

## Installation

### Prerequisites

- Python 3.10+ (Python 3.10 recommended)
- Conda (recommended) or pip
- Stan (installed via CmdStanPy)

### Option 1: Using Conda (Recommended)

```bash
# Clone the repository
git clone https://github.com/your-org/seu-sensitivity.git
cd seu-sensitivity

# Create environment from environment.yml
conda env create -f environment.yml

# Activate the environment
conda activate seu-sensitivity

# Install Stan
python -c "import cmdstanpy; cmdstanpy.install_cmdstan()"
```

### Option 2: Using pip

```bash
# Clone the repository
git clone https://github.com/your-org/seu-sensitivity.git
cd seu-sensitivity

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Stan
python -c "import cmdstanpy; cmdstanpy.install_cmdstan()"
```

### For LLM Benchmarking

The project supports both OpenAI and Anthropic APIs for LLM studies:

```bash
# Set up API keys (create .env file or export directly)
echo "OPENAI_API_KEY=your-openai-key-here" >> .env
echo "ANTHROPIC_API_KEY=your-anthropic-key-here" >> .env
```

See `.env.example` for a template.

## Quick Start

### 1. Generate a Study Design

```python
from utils.study_design import StudyDesign

# Create a study design with 20 decision problems
design = StudyDesign(M=20, K=3, D=2, R=10)
design.generate()
design.analyze()
design.save("results/designs/my_study.json")
```

### 2. Fit the Model

```python
from cmdstanpy import CmdStanModel
import json

# Load study design
with open("results/designs/my_study.json", "r") as f:
    stan_data = json.load(f)

# Compile and fit model
model = CmdStanModel(stan_file="models/m_0.stan")
fit = model.sample(data=stan_data)

# Extract rationality parameter
alpha = fit.stan_variable("alpha")
print(f"Estimated sensitivity (Î±): {alpha.mean():.2f}")
```

### 3. Combined Risky and Uncertain Choice (m_1 Model)

For better parameter identification, use the m_1 model which combines risky (known probabilities) and uncertain (feature-derived probabilities) choice problems:

```python
from utils.study_design_m1 import StudyDesignM1
from cmdstanpy import CmdStanModel

# Create design with both risky (N) and uncertain (M) problems
design = StudyDesignM1(M=20, N=20, K=3, D=2, R=10, S=8)
design.generate()
design.save("results/designs/m1_study.json")

# Fit model
model = CmdStanModel(stan_file="models/m_1.stan")
fit = model.sample(data=design.get_data_dict())
```

See [models/README_m1.md](models/README_m1.md) for detailed m_1 documentation.

### 4. Benchmark LLM Rationality

```bash
cd applications/prompt_framing_study

# Run the full study pipeline
python -m prompt_framing_study
```

See [applications/prompt_framing_study/README.md](applications/prompt_framing_study/README.md) for detailed workflow on investigating how prompt framing affects LLM rationality.

## Analysis Pipeline

The project includes a complete analysis pipeline accessible via scripts:

```bash
# Generate a study design
python scripts/run_study_design.py --config configs/study_config.json

# Run prior predictive analysis
python scripts/run_prior_predictive.py --config configs/prior_analysis_config.json

# Run parameter recovery study
python scripts/run_parameter_recovery.py --config configs/parameter_recovery_config.json

# Run simulation-based calibration
python scripts/run_sbc.py --config configs/sbc_config.json

# Run sample size estimation
python scripts/run_sample_size_estimation.py --config configs/sample_size_config.json
```

For m_1, m_2, and m_3 models, use the corresponding `m1_*`, `m2_*`, and `m3_*` config files.

## Theoretical Background

Overview of three fundamental properties of SEU sensitivity:

1. **Monotonicity**: Higher sensitivity increases probability of choosing value-maximizing alternatives
2. **Perfect Rationality Limit**: As Î± â†’ âˆž, agents deterministically choose optimal alternatives
3. **Random Choice Limit**: As Î± â†’ 0, agents choose uniformly at random

These properties hold for any value function, with SEU providing the substantive behavioral interpretation.

**Key Result**: With utilities normalized to [0,1], the sensitivity parameter Î± has a precise interpretation as the log-odds change per unit of standardized SEU difference.

**SEU Maximizer Selection**: The prior predictive analysis tracks the probability of selecting SEU-maximizing alternatives for each problem, providing a diagnostic for model rationality under the prior.

See [reports/foundations/01_abstract_formulation.qmd](reports/foundations/01_abstract_formulation.qmd) for complete mathematical details and proofs. To render and view the reports locally:

```bash
cd reports
quarto render
open _output/index.html
```

## Model m_0 Specification

The base model (`models/m_0.stan`) implements a softmax choice model for uncertain choice problems:

- **Subjective probabilities** determined by alternative features through softmax transformation
- **Ordered utilities** with incremental differences on unit scale
- **Choice probabilities** following softmax of expected utilities scaled by sensitivity Î±

**Parameters:**
- `alpha`: Sensitivity to expected utility (â‰¥ 0)
- `beta`: Feature-to-probability mapping (K Ã— D matrix)
- `delta`: Utility increments on unit scale ((K-1)-simplex)

**Data Requirements:**
- `M`: Number of decision problems
- `K`: Number of possible consequences
- `D`: Feature dimensions
- `R`: Number of distinct alternatives
- `w`: Feature vectors for each alternative
- `I`: Indicator array (which alternatives in which problems)
- `y`: Observed choices

## Model m_1 Specification

The combined model (`models/m_1.stan`) extends m_0 by adding risky choice problems with known objective probabilities:

**Additional Parameters for m_1:**
- `N`: Number of risky choice problems
- `S`: Number of risky alternatives
- `x`: Objective probability vectors for risky alternatives
- `J`: Indicator array for risky problems
- `z`: Observed risky choices

**Key Advantage:** Separate identification of utility function (from risky choices) and subjective probability mapping (from uncertain choices).

See [models/README_m1.md](models/README_m1.md) for detailed m_1 documentation.

## Model m_2 Specification

The separate-sensitivity model (`models/m_2.stan`) extends m_1 by allowing independent sensitivity parameters for uncertain and risky choices:

- `alpha`: Sensitivity for uncertain choices
- `omega`: Sensitivity for risky choices (independent of Î±)
- Shared utility function across both choice types

**Use Case:** Testing whether decision makers exhibit different levels of sensitivity when probabilities are known (risky) vs. derived from features (uncertain).

## Model m_3 Specification

The proportional-sensitivity model (`models/m_3.stan`) introduces a proportional relationship between sensitivities:

- `alpha`: Sensitivity for uncertain choices
- `kappa`: Association parameter (Ï‰ = ÎºÎ±)
- `omega`: Sensitivity for risky choices (derived, not free)

When Îº = 1, m_3 reduces to m_1 (shared Î±). When Îº â‰  1, risky sensitivity differs proportionally from uncertain sensitivity.

## Study Design Tools

The `utils/study_design.py` module provides comprehensive tools for creating experimental designs:

```python
# Generate from configuration file
design = StudyDesign.from_config("configs/my_config.json")

# Save with metadata and visualizations
design.save(
    "results/designs/my_design.json",
    include_metadata=True,
    include_plots=True
)

# Load existing design
loaded = StudyDesign.load("results/designs/my_design.json")
loaded.analyze()
```

**Features:**
- Flexible feature generation (normal, uniform distributions)
- Configurable problem complexity
- Comprehensive metadata and diagnostics
- Automatic visualization generation

See [utils/README.md](utils/README.md) for complete documentation.

## Applications

### Prompt Framing Study

Investigate how prompt framing (rationality emphasis) affects an LLM's sensitivity to expected utility maximization.

**Research Question**: Does explicitly framing a decision problem in terms of utility maximization change how "rational" an LLM appears to be?

**Key Features:**
- Contextualized embeddings that capture prompt-specific claim perception
- Multiple prompt variants from minimal to maximal rationality emphasis
- Robustness analysis across embedding models and dimensions

See [applications/prompt_framing_study/README.md](applications/prompt_framing_study/README.md) for complete workflow.

### Temperature Study

Investigate how LLM sampling temperature affects estimated sensitivity (Î±) to expected utility maximization.

**Research Question**: How does LLM temperature affect the rationality parameter Î±?

**Key Features:**
- Controlled experiment across multiple temperature levels (0.0, 0.3, 0.7, 1.0, 1.5)
- Position counterbalancing and transparent NA handling
- Deliberative embeddings

See [applications/temperature_study/README.md](applications/temperature_study/README.md) for the full experimental design.

### Legacy: LLM Rationality Benchmarking (Deprecated)

The original `llm_rationality` module provides basic LLM benchmarking capabilities. This module is being superseded by `prompt_framing_study` and `temperature_study` which offer improved methodology.

See [applications/llm_rationality/README.md](applications/llm_rationality/README.md) for legacy documentation.

## Configuration Files

Study designs can be specified via JSON configuration files:

```json
{
  "M": 30,
  "K": 4,
  "D": 3,
  "R": 15,
  "min_alts_per_problem": 2,
  "max_alts_per_problem": 6,
  "feature_dist": "uniform",
  "feature_params": {
    "low": -2,
    "high": 2
  },
  "design_name": "uniform_large_study"
}
```

## Output and Results

All results are organized in timestamped subdirectories:

```
results/
â”œâ”€â”€ designs/              # Study designs
â”‚   â”œâ”€â”€ my_study.json
â”‚   â””â”€â”€ my_study_plots/  # Visualizations
â””â”€â”€ run_YYYYMMDD_HHMMSS/ # Benchmark runs
    â”œâ”€â”€ raw_choices.json
    â”œâ”€â”€ embeddings.npz
    â”œâ”€â”€ stan_data_*.json
    â””â”€â”€ run_metadata.json
```

## Advanced Usage

### Custom Feature Distributions

```python
design = StudyDesign(
    feature_dist="uniform",
    feature_params={"low": -2, "high": 2}
)
```

### Model Diagnostics

```python
# Check convergence
print(fit.diagnose())

# Extract posterior samples
alpha_samples = fit.stan_variable("alpha")
beta_samples = fit.stan_variable("beta")

# Posterior predictive checks
y_pred = fit.stan_variable("y_pred")
```

## License

See [LICENSE](LICENSE) for details.

## Acknowledgments

### AI Tools

This project has been developed with significant assistance from AI tools, which have contributed to code development, documentation, mathematical derivations, and research design:

- **Claude Opus 4.5** (Anthropic) â€” Primary AI assistant for complex reasoning, mathematical formulations, and code architecture
- **Claude Sonnet 4.5** (Anthropic) â€” Used for code implementation, debugging, and documentation
- **GitHub Copilot** â€” Code completion and suggestions during development

We acknowledge that AI-assisted development is an evolving practice, and we have endeavored to verify AI-generated content for correctness and appropriateness throughout the project.

