---
title: "Simulation-Based Calibration"
subtitle: "Foundational Report 6"
description: |
  SBC validation comparing parameter identification between m_0 and m_1,
  providing further evidence that the addition of risky choices improves identification of δ.
categories: [foundations, validation, m_0, m_1]
execute:
  cache: true
---

```{python}
#| label: setup
#| include: false

import sys
import os

# Add parent directories to path
sys.path.insert(0, os.path.join(os.getcwd(), '..'))
project_root = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, project_root)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import json
import tempfile
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
```

## Introduction

In [Report 4](04_parameter_recovery.qmd), we observed poor recovery of the utility increment parameters δ in model m_0, and in [Report 5](05_adding_risky_choices.qmd), we hypothesized that adding risky choices would improve identification. The parameter recovery experiments in Report 5 suggested improvement, but with only 20 iterations, the results may not be statistically robust.

**Simulation-Based Calibration (SBC)** provides a more principled approach to assess parameter identification. Rather than asking "can we recover specific true values?" (as in parameter recovery), SBC asks: "does the posterior correctly represent uncertainty about the parameters?" That is, if we repeatedly (i) draw parameters from the prior, (ii) simulate data given those parameters, and (iii) compute the posterior, then the posterior should, on average, be calibrated—assigning the correct probability to regions of parameter space. SBC operationalizes this idea by checking whether the true parameter value occupies a *uniform* rank position within the posterior samples; systematic departures from uniformity signal that the posterior is too wide, too narrow, or shifted relative to the data-generating process.

::: {.callout-note}
## The SBC Principle
If the inference algorithm is correct and the model is identified, then:

1. Draw θ* from the prior: $\theta^* \sim p(\theta)$
2. Simulate data: $y \sim p(y | \theta^*)$
3. Compute posterior: $p(\theta | y)$
4. Calculate the rank of θ* in the posterior samples

The distribution of thinned ranks should be **uniform** if the model is well-calibrated. (Thinning is necessary because HMC produces dependent posterior samples; by retaining only every $t$-th draw, we reduce autocorrelation and better approximate the independence assumption underlying the rank statistic.) Non-uniformity indicates problems with either the inference algorithm or parameter identification.
:::

## SBC Methodology

### Rank Statistics

For each parameter θ, we compute the rank of the true value θ* within the posterior samples {θ₁, θ₂, ..., θₛ}:

$$
\text{rank}(\theta^*) = \sum_{s=1}^{S} \mathbf{1}[\theta_s < \theta^*]
$$

If the posterior is calibrated, this rank follows a discrete uniform distribution on {0, 1, ..., S}.

### Diagnostics

We use several complementary diagnostics to assess calibration. Each provides a different lens on the same underlying question—whether the posterior is well-calibrated—so that no single summary statistic drives our conclusions.

1. **Rank histograms**: A visual check for uniformity. Each bin of the histogram counts how many SBC simulations produced a thinned rank in a given range. If the posterior is calibrated, all bins should contain roughly the same number of counts (i.e., the histogram should be approximately flat). Systematic patterns—such as a U-shape (overdispersed posteriors) or an inverted-U (underdispersed posteriors)—indicate specific calibration failures.

2. **ECDF plots**: The empirical cumulative distribution function of the normalized ranks is plotted against the CDF of a Uniform(0, 1) distribution (the diagonal line). Departures from the diagonal reveal the same calibration issues as the rank histogram, but can be easier to read when the number of SBC simulations is small.

3. **Chi-square goodness-of-fit tests**: A formal test of whether the observed bin counts in the rank histogram are consistent with a uniform distribution. The test statistic $\chi^2 = \sum_b (O_b - E_b)^2 / E_b$ is compared to a $\chi^2$ distribution with $B - 1$ degrees of freedom (where $B$ is the number of bins). A small $p$-value (e.g., $p < 0.05$) suggests the ranks are not uniform. However, with only 100 SBC simulations, power is limited: the test may fail to reject uniformity even when mild miscalibration is present.

4. **Kolmogorov-Smirnov (KS) tests**: A non-parametric test comparing the rank ECDF to the Uniform(0, 1) CDF. The KS statistic equals the maximum vertical distance between the two curves. Like the chi-square test, a small $p$-value suggests non-uniformity. The KS test does not require binning, but can behave conservatively with discrete rank data.

### Study Design

We use matched study designs for m_0 and m_1 to enable fair comparison:

```{python}
#| label: study-config
#| echo: true

# Study design configurations
config_m0 = {
    "M": 25,                    # Number of uncertain decision problems
    "K": 3,                     # Number of consequences
    "D": 5,                     # Feature dimensions
    "R": 15,                    # Distinct alternatives
    "min_alts_per_problem": 2,
    "max_alts_per_problem": 5,
    "feature_dist": "normal",
    "feature_params": {"loc": 0, "scale": 1}
}

config_m1 = {
    **config_m0,                # Same uncertain problem structure
    "N": 25,                    # Risky problems (matching M)
    "S": 15,                    # Risky alternatives (matching R)
}

print("Study Design Comparison:")
print(f"\nm_0 (Uncertain Only):")
print(f"  M = {config_m0['M']} decision problems")
print(f"  Total choices: ~{config_m0['M'] * 3.5:.0f}")

print(f"\nm_1 (Uncertain + Risky):")
print(f"  M = {config_m1['M']} uncertain + N = {config_m1['N']} risky")
print(f"  Total choices: ~{(config_m1['M'] + config_m1['N']) * 3.5:.0f}")
```

## Running SBC for m_0

First, we run SBC for model m_0 (uncertain choices only) to establish a baseline:

```{python}
#| label: setup-sbc-m0
#| output: false

from utils.study_design import StudyDesign
from analysis.sbc import SimulationBasedCalibration

# Create m_0 study design
study_m0 = StudyDesign(
    M=config_m0["M"],
    K=config_m0["K"],
    D=config_m0["D"],
    R=config_m0["R"],
    min_alts_per_problem=config_m0["min_alts_per_problem"],
    max_alts_per_problem=config_m0["max_alts_per_problem"],
    feature_dist=config_m0["feature_dist"],
    feature_params=config_m0["feature_params"],
    design_name="sbc_m0"
)
study_m0.generate()
```

```{python}
#| label: run-sbc-m0
#| output: false

# Create output directory
output_dir_m0 = tempfile.mkdtemp(prefix="sbc_m0_")

# Run SBC for m_0
# Use thinning to get approximately independent draws from MCMC
# With 1000 samples and thin=10, we get ~100 effective ranks per simulation
sbc_m0 = SimulationBasedCalibration(
    sbc_model_path=os.path.join(project_root, "models", "m_0_sbc.stan"),
    study_design=study_m0,
    output_dir=output_dir_m0,
    n_sbc_sims=100,           # 100 SBC iterations
    n_mcmc_samples=1000,      # Posterior samples
    n_mcmc_chains=4,
    thin=10                   # Thin to reduce autocorrelation
)

ranks_m0, true_params_m0 = sbc_m0.run()
```

```{python}
#| label: sbc-m0-summary
#| echo: false

print(f"m_0 SBC Complete:")
print(f"  Simulations: {ranks_m0.shape[0]}")
print(f"  Parameters: {ranks_m0.shape[1]}")
```

## Running SBC for m_1

Now we run SBC for model m_1 (uncertain + risky choices):

```{python}
#| label: setup-sbc-m1
#| output: false

from utils.study_design_m1 import StudyDesignM1

# Create m_1 study design
study_m1 = StudyDesignM1(
    M=config_m1["M"],
    N=config_m1["N"],
    K=config_m1["K"],
    D=config_m1["D"],
    R=config_m1["R"],
    S=config_m1["S"],
    min_alts_per_problem=config_m1["min_alts_per_problem"],
    max_alts_per_problem=config_m1["max_alts_per_problem"],
    risky_probs="random",
    feature_dist=config_m1["feature_dist"],
    feature_params=config_m1["feature_params"],
    design_name="sbc_m1"
)
study_m1.generate()
```

```{python}
#| label: run-sbc-m1
#| output: false

# Create output directory  
output_dir_m1 = tempfile.mkdtemp(prefix="sbc_m1_")

# Run SBC for m_1
sbc_m1 = SimulationBasedCalibration(
    sbc_model_path=os.path.join(project_root, "models", "m_1_sbc.stan"),
    study_design=study_m1,
    output_dir=output_dir_m1,
    n_sbc_sims=100,           # 100 SBC iterations
    n_mcmc_samples=1000,      # Posterior samples
    n_mcmc_chains=4,
    thin=10                   # Thin to reduce autocorrelation
)

ranks_m1, true_params_m1 = sbc_m1.run()
```

```{python}
#| label: sbc-m1-summary
#| echo: false

print(f"m_1 SBC Complete:")
print(f"  Simulations: {ranks_m1.shape[0]}")
print(f"  Parameters: {ranks_m1.shape[1]}")
```

## Comparing Rank Distributions

The key diagnostic is comparing rank distributions between m_0 and m_1. For well-calibrated parameters, ranks should be uniformly distributed.

### α (Sensitivity Parameter)

```{python}
#| label: fig-alpha-sbc
#| fig-cap: "SBC rank distributions for α. Both models pass a chi-square test at the 0.05 level, but m_1 typically shows a higher p-value, suggesting somewhat better calibration. See the printed test statistics below for the exact values in this run."

K, D = config_m0['K'], config_m0['D']

# Calculate the effective number of rank bins based on thinning
# With thin=10 and 1000 samples, ranks range from 0 to 100
n_mcmc = 1000
thin = 10
max_rank = n_mcmc // thin  # 100 effective ranks
n_bins = 20

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# m_0 alpha ranks (index 0)
ax = axes[0]
counts_m0, bins_m0, _ = ax.hist(ranks_m0[:, 0], bins=n_bins, alpha=0.7, 
                                 color='steelblue', edgecolor='white')
# Expected count per bin for uniform distribution
expected_count = len(ranks_m0) / n_bins
ax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, 
           label=f'Uniform (E={expected_count:.1f})')
ax.set_xlabel('Rank', fontsize=11)
ax.set_ylabel('Count', fontsize=11)
ax.set_title('m_0: α Rank Distribution', fontsize=12)
ax.legend()
ax.grid(True, alpha=0.3)

# m_1 alpha ranks (index 0)
ax = axes[1]
counts_m1, bins_m1, _ = ax.hist(ranks_m1[:, 0], bins=n_bins, alpha=0.7, 
                                 color='forestgreen', edgecolor='white')
ax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, 
           label=f'Uniform (E={expected_count:.1f})')
ax.set_xlabel('Rank', fontsize=11)
ax.set_ylabel('Count', fontsize=11)
ax.set_title('m_1: α Rank Distribution', fontsize=12)
ax.legend()
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Chi-square tests
chi2_m0_alpha, p_m0_alpha = stats.chisquare(np.histogram(ranks_m0[:, 0], bins=n_bins)[0])
chi2_m1_alpha, p_m1_alpha = stats.chisquare(np.histogram(ranks_m1[:, 0], bins=n_bins)[0])
print(f"\nα Uniformity Tests:")
print(f"  m_0: χ² = {chi2_m0_alpha:.2f}, p = {p_m0_alpha:.3f}")
print(f"  m_1: χ² = {chi2_m1_alpha:.2f}, p = {p_m1_alpha:.3f}")
```

::: {.callout-note}
## Interpreting the α results
Both models are expected to identify α reasonably well, since sensitivity influences *all* choice probabilities. However, the two p-values may differ noticeably across simulation runs (e.g., $p \approx 0.07$ for m_0 vs. $p \approx 0.71$ for m_1 in one run). With only 100 SBC simulations, the chi-square test has limited power, so moderate p-values do not provide strong evidence of miscalibration. A p-value near the 0.05 threshold for m_0 does not necessarily conflict with the rank histogram appearing roughly uniform—it may simply reflect sampling variability. If m_0 consistently shows lower p-values for α across independent runs, that could indicate a mild calibration issue stemming from the β–δ identification problem propagating to α estimates; the parameter recovery results in [Report 4](04_parameter_recovery.qmd) are relevant context here.
:::

This is the critical comparison. In m_0, the β–δ interaction makes δ difficult to identify from uncertain choices alone ([Report 4](04_parameter_recovery.qmd)); in m_1, risky choices supply direct information about utilities that should improve δ calibration:

```{python}
#| label: fig-delta-sbc
#| fig-cap: "SBC rank distributions for δ parameters. Compare the shape of the rank histograms across models: departures from uniformity indicate poor calibration. See the formal test results below for quantitative assessment."

# Delta parameters are after alpha (1) and beta (K*D)
delta_start_idx = 1 + K * D
K_minus_1 = K - 1

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Expected count per bin for uniform distribution
expected_count = len(ranks_m0) / n_bins

for k in range(K_minus_1):
    param_idx = delta_start_idx + k
    
    # m_0
    ax = axes[0, k]
    counts_m0_delta, _, _ = ax.hist(ranks_m0[:, param_idx], bins=n_bins, alpha=0.7, 
                                     color='steelblue', edgecolor='white')
    ax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, 
               label=f'Uniform (E={expected_count:.1f})')
    ax.set_xlabel('Rank', fontsize=11)
    ax.set_ylabel('Count', fontsize=11)
    ax.set_title(f'm_0: δ_{k+1} Rank Distribution', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # m_1
    ax = axes[1, k]
    counts_m1_delta, _, _ = ax.hist(ranks_m1[:, param_idx], bins=n_bins, alpha=0.7,
                                     color='forestgreen', edgecolor='white')
    ax.axhline(y=expected_count, color='red', linestyle='--', linewidth=2, 
               label=f'Uniform (E={expected_count:.1f})')
    ax.set_xlabel('Rank', fontsize=11)
    ax.set_ylabel('Count', fontsize=11)
    ax.set_title(f'm_1: δ_{k+1} Rank Distribution', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
#| label: delta-sbc-tests
#| echo: false

# Compute chi-square statistics for delta parameters
print("δ Parameter Uniformity Tests:")
print("-" * 50)

delta_chi2_results = []
for k in range(K_minus_1):
    param_idx = delta_start_idx + k
    
    # Chi-square test for m_0
    counts_m0 = np.histogram(ranks_m0[:, param_idx], bins=n_bins)[0]
    chi2_m0, p_m0 = stats.chisquare(counts_m0)
    
    # Chi-square test for m_1
    counts_m1 = np.histogram(ranks_m1[:, param_idx], bins=n_bins)[0]
    chi2_m1, p_m1 = stats.chisquare(counts_m1)
    
    # KS test for m_0 - normalize by max_rank (effective samples after thinning)
    ks_m0, ks_p_m0 = stats.kstest(ranks_m0[:, param_idx] / max_rank, 'uniform')
    
    # KS test for m_1
    ks_m1, ks_p_m1 = stats.kstest(ranks_m1[:, param_idx] / max_rank, 'uniform')
    
    delta_chi2_results.append({
        'parameter': f'δ_{k+1}',
        'm0_chi2': chi2_m0,
        'm0_p': p_m0,
        'm1_chi2': chi2_m1,
        'm1_p': p_m1,
        'm0_ks': ks_m0,
        'm0_ks_p': ks_p_m0,
        'm1_ks': ks_m1,
        'm1_ks_p': ks_p_m1
    })
    
    print(f"\nδ_{k+1}:")
    print(f"  m_0: χ² = {chi2_m0:.2f}, p = {p_m0:.3f} | KS = {ks_m0:.3f}, p = {ks_p_m0:.3f}")
    print(f"  m_1: χ² = {chi2_m1:.2f}, p = {p_m1:.3f} | KS = {ks_m1:.3f}, p = {ks_p_m1:.3f}")
```

::: {.callout-note}
## Interpreting the δ results
Calibration and identification are related but distinct concepts. **Calibration** refers to whether the posterior accurately represents uncertainty (uniform SBC ranks). **Identification** refers to whether the data are informative enough to pin down the parameter. Poor identification *leads to* poor calibration in SBC, because when the data are uninformative the posterior fails to update appropriately from the prior, which manifests as non-uniform ranks (often a central peak, indicating the posterior is too diffuse).

When reading the test statistics, note the following caveats:

- **Chi-square p-values** depend on the particular binning and are sensitive to the random SBC draws. With 100 simulations and 20 bins, expected counts per bin are only 5, which is at the lower limit of the chi-square approximation. A p-value near 0.5 for m_0 does not necessarily mean good calibration—it may simply reflect low statistical power to detect the particular pattern of non-uniformity present.
- **KS test values** should in principle differ between models. If they appear identical or very similar, check whether the rank distributions actually differ visually in the histograms and ECDF plots. The KS test can behave conservatively with discrete rank data (since it was designed for continuous distributions), which may reduce its sensitivity to real differences between models.
- The rank **histograms and ECDF plots** are the primary diagnostics; the formal tests supplement but do not replace visual inspection.
:::

### ECDF Comparison

The Empirical Cumulative Distribution Function (ECDF) provides another view of calibration. For well-calibrated parameters, the ECDF should follow the diagonal:

```{python}
#| label: fig-ecdf-comparison
#| fig-cap: "ECDF plots for δ parameters. The closer the curve to the diagonal, the better the calibration. The shaded band shows the 95% Kolmogorov-Smirnov confidence region for n=100 simulations. Compare the m_0 and m_1 curves to assess whether adding risky choices improves δ calibration."

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for k in range(K_minus_1):
    param_idx = delta_start_idx + k
    
    # Normalize ranks to [0, 1] using max_rank (effective samples after thinning)
    ranks_norm_m0 = np.sort(ranks_m0[:, param_idx]) / max_rank
    ranks_norm_m1 = np.sort(ranks_m1[:, param_idx]) / max_rank
    
    ecdf = np.arange(1, len(ranks_norm_m0) + 1) / len(ranks_norm_m0)
    
    ax = axes[k]
    ax.step(ranks_norm_m0, ecdf, where='post', label='m_0', color='steelblue', linewidth=2)
    ax.step(ranks_norm_m1, ecdf, where='post', label='m_1', color='forestgreen', linewidth=2)
    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Uniform')
    
    # Add 95% confidence band
    n = len(ecdf)
    epsilon = np.sqrt(np.log(2/0.05) / (2 * n))  # Kolmogorov-Smirnov band
    ax.fill_between([0, 1], [0-epsilon, 1-epsilon], [0+epsilon, 1+epsilon], 
                    alpha=0.2, color='red', label='95% CI')
    
    ax.set_xlabel('Normalized Rank', fontsize=11)
    ax.set_ylabel('ECDF', fontsize=11)
    ax.set_title(f'δ_{k+1} ECDF Comparison', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)

plt.tight_layout()
plt.show()
```

The 95% confidence band width is $\epsilon \approx 0.136$ for $n=100$ simulations. With more SBC iterations, smaller deviations from uniformity would become detectable.

### β (Feature Weight Parameters)

Finally, we examine β calibration. Because β and δ interact multiplicatively in the expected utility (see the identification analysis in [Report 5](05_adding_risky_choices.qmd)), poor identification of δ in m_0 can propagate to β, potentially degrading its calibration as well. In m_1, if risky choices successfully pin down δ, the β parameters may also benefit from improved calibration.

```{python}
#| label: fig-beta-sbc-summary
#| fig-cap: "Summary of β parameter SBC calibration. Because β and δ interact in the expected utility, the β–δ identification problem in m_0 may affect β calibration as well. Compare p-values across models to assess whether m_1's improved δ identification also benefits β."

# Collect beta chi-square p-values for all K*D parameters
beta_start_idx = 1
beta_end_idx = 1 + K * D

beta_pvals_m0 = []
beta_pvals_m1 = []

for idx in range(beta_start_idx, beta_end_idx):
    counts_m0 = np.histogram(ranks_m0[:, idx], bins=n_bins)[0]
    counts_m1 = np.histogram(ranks_m1[:, idx], bins=n_bins)[0]
    
    _, p_m0 = stats.chisquare(counts_m0)
    _, p_m1 = stats.chisquare(counts_m1)
    
    beta_pvals_m0.append(p_m0)
    beta_pvals_m1.append(p_m1)

fig, ax = plt.subplots(figsize=(10, 5))

x = np.arange(K * D)
width = 0.35

bars1 = ax.bar(x - width/2, beta_pvals_m0, width, label='m_0', color='steelblue', alpha=0.7)
bars2 = ax.bar(x + width/2, beta_pvals_m1, width, label='m_1', color='forestgreen', alpha=0.7)

ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='α = 0.05')
ax.set_xlabel('β Parameter Index', fontsize=11)
ax.set_ylabel('Chi-square p-value', fontsize=11)
ax.set_title('β Parameter Calibration (p-values)', fontsize=12)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f"\nβ Calibration Summary:")
print(f"  m_0: {np.sum(np.array(beta_pvals_m0) > 0.05)}/{K*D} parameters well-calibrated (p > 0.05)")
print(f"  m_1: {np.sum(np.array(beta_pvals_m1) > 0.05)}/{K*D} parameters well-calibrated (p > 0.05)")
```

## Summary Statistics

```{python}
#| label: tbl-sbc-summary
#| tbl-cap: "SBC calibration comparison between m_0 and m_1. Higher p-values suggest better calibration (uniformity of rank distribution), but with 100 simulations these values are noisy—interpret in context with the rank histograms and ECDF plots."

# Build summary table
summary_rows = []

# Alpha
counts_m0 = np.histogram(ranks_m0[:, 0], bins=n_bins)[0]
counts_m1 = np.histogram(ranks_m1[:, 0], bins=n_bins)[0]
chi2_m0, p_m0 = stats.chisquare(counts_m0)
chi2_m1, p_m1 = stats.chisquare(counts_m1)

summary_rows.append({
    'Parameter': 'α',
    'm_0 χ²': f'{chi2_m0:.2f}',
    'm_0 p-value': f'{p_m0:.3f}',
    'm_1 χ²': f'{chi2_m1:.2f}',
    'm_1 p-value': f'{p_m1:.3f}',
    'Improvement': '—' if p_m0 > 0.05 else ('✓' if p_m1 > 0.05 else '—')
})

# Beta (aggregated)
mean_p_m0 = np.mean(beta_pvals_m0)
mean_p_m1 = np.mean(beta_pvals_m1)
summary_rows.append({
    'Parameter': 'β (mean)',
    'm_0 χ²': '—',
    'm_0 p-value': f'{mean_p_m0:.3f}',
    'm_1 χ²': '—',
    'm_1 p-value': f'{mean_p_m1:.3f}',
    'Improvement': '—'
})

# Delta
for k in range(K_minus_1):
    result = delta_chi2_results[k]
    improvement = '✓' if (result['m0_p'] < 0.05 and result['m1_p'] > 0.05) else \
                  ('↑' if result['m1_p'] > result['m0_p'] else '—')
    summary_rows.append({
        'Parameter': f'δ_{k+1}',
        'm_0 χ²': f'{result["m0_chi2"]:.2f}',
        'm_0 p-value': f'{result["m0_p"]:.3f}',
        'm_1 χ²': f'{result["m1_chi2"]:.2f}',
        'm_1 p-value': f'{result["m1_p"]:.3f}',
        'Improvement': improvement
    })

summary_df = pd.DataFrame(summary_rows)
print(summary_df.to_string(index=False))
```

## Discussion

### Interpretation of Results

The SBC analysis compares calibration between m_0 and m_1. The key question is whether adding risky choices improves calibration of the δ parameters, which were poorly identified in m_0's parameter recovery analysis ([Report 4](04_parameter_recovery.qmd)).

::: {.callout-tip}
## Key Finding: SBC Provides Evidence of Improved δ Calibration

**Model m_0** (uncertain choices only):

- α: Generally well-calibrated, though some runs may show borderline p-values (see the α discussion above)
- β: Calibration may be affected by the β–δ identification interaction; check results against the parameter recovery findings in [Report 4](04_parameter_recovery.qmd)
- **δ: Expected to show poor calibration** (non-uniform ranks reflecting identification problems)

**Model m_1** (uncertain + risky choices):

- α: Generally well-calibrated
- β: Expected to benefit from improved δ identification
- **δ: Expected to show improved calibration** (more uniform ranks)

The SBC results provide further evidence that adding risky choices improves δ calibration, consistent with the theoretical identification analysis in [Report 5](05_adding_risky_choices.qmd). However, given the limited number of SBC simulations (100), conclusions about the *degree* of improvement should be drawn cautiously; the formal test statistics have limited power and are sensitive to simulation variability.
:::

::: {.callout-note}
## Distinguishing Identification from Inference Problems
SBC can detect both **inference failures** (bugs in the sampler) and **identification problems** (structural non-identifiability). We can distinguish them by checking MCMC diagnostics: if $\hat{R} \approx 1$ and ESS is adequate but ranks are non-uniform, the issue is identification rather than inference. In our analysis, both models show good MCMC diagnostics, suggesting that non-uniform δ ranks in m_0 (if observed) reflect identification limitations rather than computational issues.
:::

### Why Non-Uniform Ranks Indicate Calibration Failure

When a parameter is poorly identified, the posterior fails to concentrate around the true value. This manifests in the SBC rank distribution as follows:

1. The posterior is **too wide** relative to the prior (weak updating from data)
2. True values tend to have **central ranks** (ranks cluster around the median)
3. The rank histogram shows a characteristic **peak in the middle**

This pattern—if observed for δ in m_0—would be consistent with the model's inability to distinguish between different utility functions on the basis of uncertain choices alone.

### Why m_1 Is Expected to Improve Calibration

In m_1, risky choices provide **direct information about utilities** without confounding with subjective probabilities:

$$
\text{Risky EU: } \eta^{(r)}_s = \sum_{k=1}^K \pi_{sk} \cdot \upsilon_k
$$

where π are the known objective probabilities. This is expected to improve calibration because:

1. Risky choices constrain υ (and hence δ) directly, breaking the β–δ confound
2. With δ better identified, uncertain choices more effectively constrain β
3. Both choice types constrain α

## Conclusion

Simulation-Based Calibration provides a principled framework for assessing posterior calibration, complementing the parameter recovery analysis of [Report 4](04_parameter_recovery.qmd):

1. **m_0 faces a calibration challenge for δ**: The β–δ identification problem documented in Reports 4 and 5 is expected to manifest as non-uniform SBC ranks for δ, and potentially for β as well

2. **m_1 is expected to improve δ calibration**: Adding risky choices provides a direct route to utility identification, which should yield more uniform SBC ranks for δ

3. **The improvement is structural**: The change is not simply about adding more data, but about the *type* of data—risky choices provide qualitatively different information than uncertain choices, as formalized in the Anscombe-Aumann framework ([Report 5](05_adding_risky_choices.qmd))

The degree of improvement observed in any particular run depends on the random seed and the study design. Readers should focus on the rank histograms and ECDF plots as the primary evidence, treating the formal test statistics as supplementary given the limited number of SBC simulations.

::: {.callout-note}
## On Statistical Power
With 100 SBC simulations, the chi-square and KS tests have limited power to detect small deviations from uniformity. Larger-scale SBC analyses would provide more precise calibration assessments. Additionally, the chi-square approximation is strained when expected bin counts are small (here $100/20 = 5$), and the KS test can behave conservatively with discrete rank data. Visual inspection of rank histograms and ECDF plots remains essential.
:::

## References
