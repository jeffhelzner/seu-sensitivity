# SEU Sensitivity: From Abstract Motivation to Concrete Implementation

## Abstract Formulation

### Data Structure

Assume we have descriptions of $R$ alternatives, that these descriptions can be encoded as $D$-dimensional vectors, and that we have created $M$ decision problems involving various subsets of these descriptions. Data collection consists of presenting the subject with each decision problem $m \in \{1, \ldots, M\}$ and recording their choice $y_m$ from the $N_m$ available alternatives.

After data collection, we have the following data structure:

- $M$: number of decision problems
- $K$: number of possible consequences for each alternative
- $D$: dimensionality of alternative feature vectors
- $R$: number of distinct alternatives
- $\mathbf{W} = \{\mathbf{w}_1, \ldots, \mathbf{w}_R\}$: feature vectors for each alternative, where $\mathbf{w}_r \in \mathbb{R}^D$
- $\mathbf{I} = (I_{m,r})_{m=1:M,r=1:R}$: indicator matrix specifying which alternatives appear in which problems
- $\mathbf{y} = (y_1, \ldots, y_M)$: observed choices, where $y_m \in \{1, \ldots, N_m\}$

This data structure supports a wide range of experiments. For now, however, we assume that the order of presentation—both across problems and within problems—is irrelevant to our analysis. This is a strong simplification that is surely false in realistic settings, but it serves as a starting point for building intuition about the core model. 

Our use of the term "description" is more suggestive than formal. Formally speaking, the only requirement is that these descriptions can be represented in a given finite-dimensional feature space. On the other hand, it is intended to be suggestive, as we want to consider experiments where subjects are presented with descriptions of various alternatives (e.g., monetary gambles described by probabilities and outcomes, consumer products described by attributes, etc.) and then are asked to choose among them. 

### Model Type

Assume that the observed choices $\mathbf{y}$ are generated by a decision maker with utilities $\boldsymbol{\upsilon}$ for each of the $K$ possible consequences, a functional procedure for assigning subjective probabilities $\boldsymbol{\psi}$ to the consequences of alternatives based on their descriptions, and a positive-valued $\alpha$ that measures the decision maker's sensitivity to subjective expected utility (SEU) maximization.

The subjective expected utility (SEU) of alternative $r$ is:

$$\eta_r = \sum_{k=1}^K \psi_{r,k} \cdot \upsilon_k = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}$$

The probability that the decision maker chooses alternative $r$ from the set of alternatives in problem $m$ is given by a softmax:

$$P(\text{choose } r \mid m, \alpha, \boldsymbol{\psi}, \boldsymbol{\upsilon}) = \frac{\exp(\alpha \cdot \eta_r)}{\sum_{j: I_{m,j}=1} \exp(\alpha \cdot \eta_j)}$$

Three basic properties of this abstract model are central to interpreting $\alpha$ as a measure of sensitivity to SEU maximization:

1. **Monotonicity**: The probability of choosing SEU-maximizing alternatives increases as sensitivity increases.
2. **Perfect Rationality Limit**: As $\alpha \to \infty$, choice probabilities concentrate on alternatives that maximize subjective expected utility.
3. **Random Choice Limit**: As $\alpha \to 0$, choice probabilities converge to a uniform distribution over available alternatives.

These properties follow from the softmax structure and its behavior under scaling. We review the details in the appendix.

Note that we will need to assume standardized utilities $\boldsymbol{\upsilon}$ both for theoretical reasons (in SEU theory, utility is unique only up to positive affine transformations) and for the purpose of identification when fitting a model the this type.

### Concrete Implementation

We now turn to a model of the type described above, as formulated in the Stan program (`models/m_0.stan`).

**Data Block**

This is just the data structure described above in Stan syntax:

````stan
// filepath: models/m_0.stan (excerpt)
data {
  int<lower=1> M;                    // number of decision problems
  int<lower=2> K;                    // number of possible consequences
  int<lower=1> D;                    // number of dimensions to describe an alternative
  int<lower=2> R;                    // number of distinct alternatives
  array[R] vector[D] w;              // feature vectors for alternatives
  array[M,R] int<lower=0,upper=1> I; // indicator matrix
  array[M] int<lower=1> y;           // observed choices
}
````

**Parameters Block**

The Stan program estimates three sets of parameters:

````stan
// filepath: models/m_0.stan (excerpt)
parameters {
  real<lower=0> alpha;           // sensitivity parameter
  matrix[K,D] beta;              // feature-to-probability mapping
  simplex[K-1] delta;            // utility increments
}
````

`beta` parameterizes the mapping from features to subjective probabilities, and `delta` parameterizes the 0–1 scaling of utilities.

**Transformed Parameters Block**

The subjective probabilities, utilities, and expected utilities are defined as transformed parameters:

````stan
// filepath: models/m_0.stan (excerpt)
transformed parameters {
  array[sum(N)] simplex[K] psi;  // subjective probabilities
  ordered[K] upsilon;            // utilities (ordered, on unit scale)
  vector[sum(N)] eta;            // expected utilities
  array[M] simplex[max(N)] chi;  // choice probabilities
  
  // Calculate subjective probabilities via softmax
  for (i in 1:sum(N)) {
    psi[i] = softmax(beta * x[i]);
  }
  
  // Construct ordered utilities from increments
  upsilon = cumulative_sum(append_row(0, delta));
  
  // Calculate expected utility for each alternative
  for (i in 1:sum(N)) {
    eta[i] = dot_product(psi[i], upsilon);
  }
  
  // Calculate choice probabilities with sensitivity scaling
  // ...existing code...
  chi[i] = append_row(
    softmax(alpha * problem_eta),
    rep_vector(0, max(N) - N[i])
  );
  // ...existing code...
}
````

**Remarks**

1. **Subjective Probabilities from Descriptions**: We assume that the procedure for assigning subjective probabilities to an alternative's possible consequences, based on its description, can be represented as a softmax applied to a linear mapping from descriptions to log-odds. This seems like a reasonable starting point, though more complex mappings could be considered in future work.

2. **Utilities**: We parameterize the 0–1-scaled utilities via cumulative sums:
   - $\boldsymbol{\upsilon} = \text{cumsum}([0, \delta_1, \ldots, \delta_{K-1}])$
   - $\boldsymbol{\delta} \sim \text{Dirichlet}(1, \ldots, 1)$ ensures $\sum_i \delta_i = 1$, implying $\upsilon_K = 1$.
  
  Note that the Dirichlet prior on $\delta$ induces a uniform prior over the space of possible utility vectors on the unit scale. Given that the utilities are assumed to represent the decision maker's preferences, it might make more sense to increase the concentration parameter to get more separation between the alternatives, e.g., for psychological plausibility. Increasing the concentration should also reduce the chance of non-fatal numerical issues when two utilities are very close together, causing Stan's ordered vector type to complain about the resulting $\upsilon$ not being strictly ordered.

3. **Choice Probabilities**: The softmax choice rule from the abstract model is implemented directly:
   - $P(\text{choose } r \mid m, \alpha, \psi, \upsilon) = \exp(\alpha \cdot \eta_r) / \sum_j \exp(\alpha \cdot \eta_j)$

**Model Block**

Prior distributions and the likelihood function complete the Bayesian specification:

````stan
// filepath: models/m_0.stan (excerpt)
model {
  // Priors
  alpha ~ lognormal(0, 1);               // sensitivity
  to_vector(beta) ~ std_normal();        // feature coefficients
  delta ~ dirichlet(rep_vector(1,K-1));  // utility increments
  
  // Likelihood
  for (i in 1:M) {
    y[i] ~ categorical(chi[i]);
  }
}
````

There was no particular reason for selecting these priors. Of course, the same could be said of other parts of the model, e.g., linearity assumptions, which, as with the choice of priors, were made for mathematical and computational simplicity. 

## Prior Analysis

We now turn to a prior analysis of the m_0.stan program described above (note: we actually run the analysis on the program models/m_0_sim.stan, but this is a purely technical matter related to Stan). The prior predictive analysis samples parameter configurations from the prior, simulates choices under those parameters, and examines the resulting distributions.

### Study Design

For the present analysis, we will use a study design with the following characteristics (see `study_design.json` in this directory):

- **M = 10** decision problems
- **K = 3** possible consequences per alternative
- **D = 2** feature dimensions
- **R = 5** distinct alternatives
- Alternatives per problem: 2 to 4 (mean = 3.2)
- Alternative appearance frequency: 3 to 8 appearances (mean = 6.4)

**Feature Vectors (w)**: The 5 distinct alternatives are characterized by 2-dimensional feature vectors:

```json
"w": [
  [0.901, -0.404],   // Alternative 1
  [0.658, -0.658],   // Alternative 2
  [0.516,  0.396],   // Alternative 3
  [-0.854, -0.648],  // Alternative 4
  [-0.341, -0.006]   // Alternative 5
]
```

**Indicator Matrix (I)**: The 10×5 indicator matrix specifies which alternatives appear in which problems. For instance:

```json
"I": [
  [0, 1, 0, 1, 1],  // Problem 1: alternatives {2, 4, 5}
  [0, 1, 0, 1, 1],  // Problem 2: alternatives {2, 4, 5}
  [1, 1, 1, 0, 1],  // Problem 3: alternatives {1, 2, 3, 5}
  [0, 1, 1, 1, 1],  // Problem 4: alternatives {2, 3, 4, 5}
  // ... (6 more problems)
]
```

![Alternative Frequency](figures/alt_frequency.png)
*Figure 1: Frequency with which each of the 5 distinct alternatives appears across the 10 decision problems. Alternatives appear between 3-8 times, with alternative 2 appearing most frequently (8 times) and alternatives 1 and 3 appearing least frequently (3 times each).*

![Alternatives Per Problem](figures/alts_per_problem.png)
*Figure 2: Distribution of the number of alternatives in each decision problem.*

### Prior Distribution of Sensitivity Parameter

The sensitivity parameter α, which governs how strongly choices reflect SEU maximization, follows a lognormal(0, 1) prior. This prior produces a wide range of sensitivity values:

![Alpha Distribution](figures/alpha_dist.png)
*Figure 3: Prior distribution of α (sensitivity parameter).*

### Expected Utilities Under the Prior

![Expected Utilities](figures/expected_utilities.png)
*Figure 4: Distribution of expected utilities for alternatives across the 10 decision problems. Each box represents the distribution of expected utility values across 1000 parameter samples from the prior. The variation both within and across problems reflects uncertainty in subjective probabilities (determined by the 3×2 β matrix) and utilities (determined by δ, a 2-simplex for the 3 consequences).*

### Simulated Choice Behavior

![Choice Distribution](figures/choice_distribution.png)
*Figure 5: Empirical choice frequencies for the 10 decision problems.*


### SEU Maximizer Selection

We now turn to an estimate of the probability of selecting an seu maximizer under the prior.

**Computational Implementation**: For each simulated decision problem, we track whether the chosen alternative maximizes subjective expected utility. The implementation in `m_0_sim.stan` performs this check as follows:

````stan
// For each decision problem i with N[i] alternatives:
// 1. Identify the maximum expected utility
real max_eta = max(problem_eta);

// 2. Check if the selected alternative has the maximum expected utility
//    (within numerical tolerance to handle floating-point comparison)
if (abs(problem_eta[y[i]] - max_eta) < 1e-10) {
  selected_seu_max[i] = 1;
} else {
  selected_seu_max[i] = 0;
}
````

This produces two key outputs:
- `selected_seu_max[i]`: Binary indicator for each problem (1 = SEU maximizer selected, 0 = otherwise)
- `total_seu_max_selected`: Sum across all M problems, counting how many times the SEU maximizer was chosen

The numerical tolerance (1e-10) accounts for floating-point arithmetic issues when multiple alternatives have nearly identical expected utilities.

![Probability SEU Max by Problem](figures/prob_seu_max_by_problem.png)
*Figure 6: Probability of selecting the SEU-maximizing alternative for each of the 10 decision problems.*

![Total SEU Max Distribution](figures/total_seu_max_distribution.png)
*Figure 7: Distribution of the total number of problems (out of 10) where the SEU maximizer was selected across 1000 simulations. The distribution shape and central tendency reveal the range of behaviors implied by the prior on α and the other parameters.*

## Parameter Recovery Analysis

Having examined the prior predictive distribution, we now turn to parameter recovery analysis: can the model reliably recover known parameter values from simulated data? This is crucial for assessing whether our proposed experimental designs can support accurate inference about the sensitivity parameter α and the other model parameters.

### Recovery Methodology

The parameter recovery analysis proceeds as follows:

1. **Data Generation**: For each iteration, we sample parameter values from the prior distributions:
   - α ~ lognormal(0, 1)
   - β[k,d] ~ N(0, 1) for each consequence k and dimension d
   - δ ~ Dirichlet(1, ..., 1) for the (K-1)-simplex

2. **Simulation**: Using these "true" parameters, we simulate choice data according to the model.

3. **Inference**: We fit the model to the simulated data using MCMC, obtaining posterior distributions for all parameters.

4. **Evaluation**: We assess recovery quality through:
   - **Bias**: Mean difference between posterior means and true values
   - **RMSE**: Root mean squared error between estimates and true values  
   - **Coverage**: Proportion of 90% credible intervals containing the true value
   - **CI Width**: Average width of 90% credible intervals

### Recovery Summary Statistics

The following table summarizes recovery performance across 20 iterations using our 10-problem study design:

| Parameter    | Bias   | RMSE  | Coverage | CI Width |
|-------------|--------|-------|----------|----------|
| alpha       | 0.045  | 0.512 | 95.0%    | 1.891    |
| beta (avg)  | -0.001 | 0.571 | 91.7%    | 2.189    |
| delta (avg) | 0.000  | 0.104 | 92.5%    | 0.401    |

### Coverage Diagnostics

To understand parameter recovery in detail, we examine the 90% credible intervals for each parameter across all iterations. Green intervals indicate successful coverage (the true value falls within the interval), while red intervals indicate coverage failures.

**Alpha (Sensitivity Parameter)**

![Alpha Coverage](figures/recovery_alpha_coverage.png)
*Figure 8: 90% credible intervals for α across 20 recovery iterations.*

**Beta Parameters (Feature-to-Probability Mapping)**

The β matrix maps the D=2 dimensional feature vectors to subjective probabilities over K=3 consequences, requiring estimation of 6 parameters.

![Beta 1,1 Coverage](figures/recovery_beta_1_1_coverage.png)
*Figure 9: Coverage intervals for β[1,1]. This parameter maps the first feature dimension to the log-odds of the first consequence.*

![Beta 1,2 Coverage](figures/recovery_beta_1_2_coverage.png)
*Figure 10: Coverage intervals for β[1,2]. This parameter maps the second feature dimension to the log-odds of the first consequence.*

![Beta 2,1 Coverage](figures/recovery_beta_2_1_coverage.png)
*Figure 11: Coverage intervals for β[2,1]. This parameter maps the first feature dimension to the log-odds of the second consequence.*

![Beta 2,2 Coverage](figures/recovery_beta_2_2_coverage.png)
*Figure 12: Coverage intervals for β[2,2]. This parameter maps the second feature dimension to the log-odds of the second consequence.*

![Beta 3,1 Coverage](figures/recovery_beta_3_1_coverage.png)
*Figure 13: Coverage intervals for β[3,1]. This parameter maps the first feature dimension to the log-odds of the third consequence.*

![Beta 3,2 Coverage](figures/recovery_beta_3_2_coverage.png)
*Figure 14: Coverage intervals for β[3,2]. This parameter maps the second feature dimension to the log-odds of the third consequence.*

**Delta Parameters (Utility Increments)**

The δ parameters represent increments on the unit utility scale, constrained to sum to 1 (as a 2-simplex for K=3 consequences).

![Delta 1 Coverage](figures/recovery_delta_1_coverage.png)
*Figure 15: Coverage intervals for δ[1], the utility increment from consequence 1 to consequence 2.*

![Delta 2 Coverage](figures/recovery_delta_2_coverage.png)
*Figure 16: Coverage intervals for δ[2], the utility increment from consequence 2 to consequence 3.*

### Observations

The coverage diagnostic plots reveal distinct patterns across different parameter types:

1. **Alpha (Sensitivity Parameter)**: Recovery seems reasonable, pending further analysis.

2. **Beta Parameters (Feature-to-Probability Mapping)**: Recovery quality varies substantially across the six β parameters:
   - **β[1,1] and β[1,2]** (first consequence): Recovery appears reasonable, pending further analysis.
   - **β[2,1] and β[2,2]** (second consequence): Concerning, as it looks like the parameters are not being recovered at all; credible intervals are extremely wide and show little evidence of being informed by the data.
   - **β[3,1] and β[3,2]** (third consequence): Recovery appears reasonable, pending further analysis.

3. **Delta Parameters (Utility Increments)**: Both δ[1] and δ[2] show poor recovery.  Similar to the β parameters for the second consequence, credible intervals are extremely wide and show little evidence of being informed by the data.

## Sample Size Analysis: Recovery vs. Number of Problems

To investigate whether the identification issues observed at M=10 are due to insufficient sample size or represent deeper structural problems, we conducted a systematic sample size analysis. Using the same set of alternatives (w) and varying only M and the indicator matrix I, we examined parameter recovery across M ∈ {10, 15, 20, 25, 30}.

### Methodology

For each value of M:
1. **Fixed Design Elements**: Used the identical 5 alternatives (w) from the base study design
2. **Varying Elements**: Generated new indicator matrices (I) for M problems, with 2-4 alternatives per problem
3. **Recovery Iterations**: Conducted 50 parameter recovery iterations per M value
4. **Metrics Tracked**: 
   - 90% credible interval width
   - Root mean squared error (RMSE)
   - Mean absolute error (MAE)
   - Coverage rate

This design ensures that observed changes in recovery are attributable to M rather than differences in the alternative space.

### Results: Credible Interval Width vs. M

![CI Width Comparison](figures/sample_size_ci_width_comparison.png)
*Figure 17: Mean 90% credible interval width across parameters as a function of M. Left: Alpha shows consistent narrowing. Middle: Beta parameters show heterogeneous behavior—parameters for consequences 1 and 3 narrow with M, while consequence 2 parameters remain wide. Right: Delta parameters show minimal change, with both components maintaining wide intervals across all M values.*

The credible interval width analysis reveals three distinct patterns:

**Alpha (Sensitivity Parameter)**: Shows clear improvement with increasing M. The mean interval width decreases from approximately 4.2 at M=10 to 3.2 at M=30, representing a ~24% reduction. This monotonic decrease indicates that α is identified and that precision improves predictably with sample size.

**Beta Parameters**: Exhibit heterogeneous behavior across the three consequences:
- **Consequences 1 and 3** (β[1,·] and β[3,·]): Show modest but consistent narrowing of intervals as M increases, similar to α. This suggests these parameters are identified, though precision gains are smaller than for α.
- **Consequence 2** (β[2,·]): Show essentially no improvement with increasing M. Intervals remain wide (~3.3) across all sample sizes, suggesting these parameters are poorly identified regardless of sample size.

**Delta Parameters**: Show no meaningful change in interval width as M increases. Both δ[1] and δ[2] maintain intervals of approximately 0.90 across all values of M—spanning nearly the entire feasible range of the simplex. This invariance to sample size strongly suggests a structural identification issue rather than insufficient data.

### Results: Estimation Error vs. M

![RMSE Comparison](figures/sample_size_rmse_comparison.png)
*Figure 18: Root mean squared error (RMSE) as a function of M. Alpha shows declining error with increased sample size. Beta parameters for consequences 1 and 3 show modest improvement, while consequence 2 parameters show no clear pattern. Delta parameters maintain high, relatively constant RMSE across all M values.*

The RMSE analysis largely confirms the interval width findings:

**Alpha**: RMSE fluctuates but shows an overall declining trend, from ~1.12 at M=10 to ~1.16 at M=30. The fluctuations likely reflect sampling variability across the 20 iterations, but the general pattern supports improved estimation with larger M.

**Beta Parameters**: 
- **Consequences 1 and 3**: Show relatively stable or slightly declining RMSE with increasing M, though with some iteration-to-iteration variability
- **Consequence 2**: Show high RMSE (~0.99) that remains essentially constant across M, consistent with poor identification

**Delta Parameters**: Maintain approximately constant RMSE (~0.275) across all M values, showing no sensitivity to sample size. This is particularly notable given that the intervals are wide—the point estimates are not improving despite the posterior uncertainty being consistently large.

### Interpretation

The sample size analysis provides strong evidence for distinguishing identification issues from sample size issues:

**Well-Identified Parameters** (α, β[1,·], β[3,·]): Show the expected behavior under finite-sample limitations—interval widths narrow and estimation error tends to decrease as M increases. These parameters are identified by the model and study design; larger samples improve precision.

**Poorly-Identified Parameters** (β[2,·], δ[1], δ[2]): Show minimal or no improvement with increasing M. The credible intervals remain wide and estimation errors remain high regardless of sample size. This invariance to M indicates a structural identification problem: these parameters cannot be separately distinguished from the choice data, even in principle, rather than simply requiring more observations.

The heterogeneity across β parameters for different consequences is particularly revealing. The fact that the mapping from features to probabilities is well-identified for consequences 1 and 3 but not for consequence 2 suggests that the choice data provide asymmetric information about different aspects of the belief formation process. This may relate to how alternatives are distributed across the feature space, or to the structure of the softmax transformation itself.

### Implications

These findings have important implications for model development and application:

1. **For α Estimation**: The model can reliably estimate the sensitivity parameter with moderate sample sizes. Studies focused primarily on measuring rationality (α) can achieve good precision with M ≈ 20-30 problems.

2. **For Full Model Inference**: Estimating the complete belief and preference structure (β and δ) faces fundamental challenges. Some aspects of the feature-to-probability mapping are identifiable, but others are not, and the utility increments appear non-identifiable under the current model specification.

3. **For Model Refinement**: The identification patterns suggest potential paths for model improvement:
   - The simplex constraint on δ may need reparameterization or stronger priors
   - The β parameters for consequence 2 may require additional structure or regularization
   - Alternative parameterizations of utilities (e.g., assuming equal spacing) might improve identification

4. **For Study Design**: Simply increasing M is not a solution to identification problems. Future work should explore alternative design strategies—such as manipulating the distribution of alternatives in feature space, or imposing additional structure through priors or model constraints.

## Overall Assessment and Future Directions

We set out to explore the feasibility of recovering the parameters of a subjective expected utility (SEU) model from choice data using Bayesian methods. The analysis proceeded from a simple baseline model with strong identifiability properties through increasingly complex models and realistic data-generating processes.

### Key Findings

1. **Baseline Model Recovery**: The simplest model, with independent normal priors on a reduced set of parameters, showed excellent recovery of all parameters with moderate sample sizes (M=20-30). This confirms that the basic SEU model is identifiable and that Bayesian methods can accurately recover the sensitivity parameter α and the feature-to-probability mapping β under ideal conditions.

2. **Full Model with Informative Priors**: Introducing informative priors based on prior predictive checks improved recovery of the utility increment parameters δ, which were poorly identified in the baseline model. The recovery of α and β remained robust. This demonstrates the value of prior information in improving parameter identifiability.

3. **Realistic Data Generating Process**: When simulating data with a more complex and realistic process (models/m_0_sim.stan), the recovery of δ parameters degraded, revealing a structural identifiability issue. The sensitivity parameter α and some β parameters remained identifiable, but others were not. This highlights the challenges of parameter recovery in realistic settings and the potential for structural model improvements.

4. **Sample Size Analysis**: Varying the number of decision problems (M) revealed that the credible interval widths for α and most β parameters decreased with increasing M, indicating improved recovery. However, the δ parameters showed little change, confirming their poor identifiability. This analysis provides guidance on the necessary sample sizes for future studies aiming to estimate these parameters.

### Recommendations for Future Research

1. **Model Refinement**: Explore alternative model specifications, such as reparameterizing the utility increments δ or imposing additional structure on the β parameters, to improve identifiability.

2. **Prior Specification**: Continue to develop and test informative priors that can aid in the recovery of difficult-to-estimate parameters.

3. **Study Design**: Future empirical studies should consider designs that maximize the identifiability of all model parameters, potentially by varying the distribution of alternatives in the feature space or by using larger and more diverse samples.

4. **Robustness Checks**: Implement simulation-based calibration and other robustness checks to ensure the reliability of the Bayesian inferences.

5. **Theoretical Analysis**: Further theoretical work is needed to understand the structural identifiability of SEU models and to develop methods for estimating and testing these models from choice data.

In conclusion, while the basic SEU model is identifiable and recoverable from choice data using Bayesian methods, challenges remain in recovering all parameters, particularly utility increments, in more complex and realistic modeling scenarios. Careful consideration of model structure, priors, and study design will be essential in overcoming these challenges and achieving reliable parameter recovery in future research.