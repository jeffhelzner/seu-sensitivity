---
title: "Adding Risky Choices"
subtitle: "Foundational Report 5"
description: |
  Extension of the SEU model to include risky choice data, enabling 
  identification of utility parameters through known-probability decisions.
categories: [foundations, m_1]
execute:
  cache: true
---

```{python}
#| label: setup
#| include: false

import sys
import os

# Add parent directories to path
sys.path.insert(0, os.path.join(os.getcwd(), '..'))
project_root = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, project_root)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
```

## Introduction

[Report 4](04_parameter_recovery.qmd) revealed a fundamental identification problem: while model m_0 can reliably recover the sensitivity parameter α and feature weights β, the utility increments δ remain poorly identified even as sample size increases. This is not a statistical limitation but a structural one—decisions under uncertainty alone cannot fully disentangle utilities from subjective probabilities.

The solution, developed in this report, is to incorporate a second type of choice task: **risky decisions** where probabilities are known to the decision-maker. This approach has deep roots in the foundations of decision theory.

## Historical Background: Risk vs. Uncertainty

The distinction between decision-making under **risk** and under **uncertainty** is one of the oldest and most fundamental in decision theory.

### Knight's Distinction

Frank Knight [-@knight1921risk] drew a sharp distinction between:

- **Risk**: Situations where probabilities of outcomes are objectively known (e.g., a fair die)
- **Uncertainty**: Situations where probabilities are unknown or unknowable (e.g., will it rain tomorrow?)

Knight argued that these represent fundamentally different decision environments, with profit in business arising primarily from uncertainty rather than risk.

### Von Neumann-Morgenstern: Expected Utility Under Risk

The modern theory of rational choice under risk was axiomatized by @vonneumann1944theory. Their key insight was that preferences over **lotteries**—probability distributions over outcomes—could be represented by a utility function if certain axioms (completeness, transitivity, continuity, independence) were satisfied.

For a lottery $L = (p_1, o_1; p_2, o_2; \ldots; p_K, o_K)$ giving outcome $o_k$ with known probability $p_k$, the expected utility is:
$$
U(L) = \sum_{k=1}^K p_k \cdot u(o_k)
$$

The vNM framework applies only to risk—the probabilities $p_k$ are taken as given.

### Savage: Subjective Expected Utility Under Uncertainty

Leonard Savage [-@savage1954foundations] extended expected utility theory to uncertainty by introducing **subjective probabilities**. In his framework, the decision-maker acts as if assigning personal probabilities to states of the world and maximizing expected utility with respect to these beliefs.

Savage's axioms imply the existence of both:

1. A utility function $u$ over consequences
2. A probability measure $P$ over states

However, Savage's approach requires eliciting utilities and probabilities simultaneously from choice behavior—raising the identification problem we encountered with m_0.

### Anscombe-Aumann: The Horse Lottery Resolution

@anscombe1963definition proposed an elegant solution that bridges risk and uncertainty. Their key innovation was to consider choice objects that combine both:

::: {.callout-note}
## The Horse Lottery Framework
An **Anscombe-Aumann act** maps states of the world (the "horses") to lotteries (probability distributions over prizes). 

- The probabilities in each lottery are objective (known)
- The probabilities across states are subjective (uncertain)

By varying the lotteries while holding the state-dependence fixed, one can identify the utility function from risky preferences. By varying the state-dependence while holding lotteries fixed, one can identify subjective probabilities.
:::

This insight is precisely what motivates model m_1. By observing choices in both:

1. **Uncertain decisions** (like Savage's acts, mapped through features to subjective probabilities)
2. **Risky decisions** (like vNM lotteries, with known probabilities)

...we can separately identify utilities and subjective probabilities.

### The Identification Logic

As @kreps1988notes explains in his exposition of the Anscombe-Aumann framework:

> "The introduction of objective lotteries... provides a way to calibrate cardinal utility... Once cardinal utility is pinned down by choices among lotteries, subjective probability can be inferred from choices among acts."

This is exactly our strategy:

| Choice Type | What It Reveals | Model Component |
|-------------|-----------------|-----------------|
| **Risky** (known $p$) | Utility function shape | δ parameters |
| **Uncertain** (unknown $p$) | Subjective probability formation | β parameters |
| **Both** | Choice sensitivity | α parameter |

## Model m_1 Specification

Model m_1 extends m_0 by adding N risky choice problems alongside the M uncertain choice problems. Both share the same utility function (υ, determined by δ) and sensitivity parameter (α).

### Data Structure

```{python}
#| label: m1-data-structure
#| echo: true
#| eval: false

# From m_1.stan data block:

# === UNCERTAIN DECISIONS (as in m_0) ===
# M: number of uncertain decision problems
# K: number of possible consequences  
# D: dimensions of alternative features
# R: number of distinct uncertain alternatives
# w[R]: feature vectors for uncertain alternatives (vector[D] each)
# I[M,R]: indicator array for which alternatives appear in which problems
# y[M]: choices for uncertain problems

# === RISKY DECISIONS (new in m_1) ===
# N: number of risky decision problems
# S: number of distinct risky alternatives  
# x[S]: probability simplexes for risky alternatives (simplex[K] each)
# J[N,S]: indicator array for which risky alternatives appear in which problems
# z[N]: choices for risky problems
```

### Key Differences from m_0

The crucial difference is in how probabilities enter:

**Uncertain alternatives** (same as m_0):
$$
\psi_{rk} = \frac{\exp(\boldsymbol{\beta}_k^\top \mathbf{w}_r)}{\sum_{k'=1}^K \exp(\boldsymbol{\beta}_{k'}^\top \mathbf{w}_r)}
$$
The probabilities ψ are *derived* from features via the learned mapping β.

**Risky alternatives** (new in m_1):
$$
\pi_{sk} = x_{sk} \quad \text{(given as data)}
$$
The probabilities π are *provided* directly—they are the objective lottery probabilities.

### Expected Utilities

For **uncertain** alternatives, expected utility is:
$$
\eta^{(u)}_r = \sum_{k=1}^K \psi_{rk} \cdot \upsilon_k = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}
$$

For **risky** alternatives, expected utility is:
$$
\eta^{(r)}_s = \sum_{k=1}^K \pi_{sk} \cdot \upsilon_k = \boldsymbol{\pi}_s^\top \boldsymbol{\upsilon}
$$

The key insight: risky expected utilities depend *only* on υ (and hence δ), not on β. This breaks the confounding that plagued m_0.

### Choice Probabilities

Both choice types use the same softmax rule with shared α:

$$
\chi^{(u)}_{mi} = \frac{\exp(\alpha \cdot \eta^{(u)}_{mi})}{\sum_{j=1}^{N^{(u)}_m} \exp(\alpha \cdot \eta^{(u)}_{mj})}
\quad\text{and}\quad
\chi^{(r)}_{ni} = \frac{\exp(\alpha \cdot \eta^{(r)}_{ni})}{\sum_{j=1}^{N^{(r)}_n} \exp(\alpha \cdot \eta^{(r)}_{nj})}
$$

### Likelihood

The log-likelihood is the sum of contributions from both choice types:
$$
\log p(y, z | \theta) = \sum_{m=1}^M \log \chi^{(u)}_{m, y_m} + \sum_{n=1}^N \log \chi^{(r)}_{n, z_n}
$$

## Examining the Stan Implementation

Let's examine key portions of `m_1.stan`:

```{python}
#| label: show-m1-stan
#| echo: false

# Display key sections of m_1.stan
m1_code = '''
// === PARAMETERS ===
parameters {
  real<lower=0> alpha;           // sensitivity (shared)
  matrix[K,D] beta;              // feature-to-probability mapping
  simplex[K-1] delta;            // utility increments (shared)
}

// === TRANSFORMED PARAMETERS ===
transformed parameters {
  // Shared utility function
  ordered[K] upsilon = cumulative_sum(append_row(0, delta));
  
  // UNCERTAIN: subjective probabilities via softmax
  for (i in 1:total_uncertain_alts) {
    psi[i] = softmax(beta * x_uncertain[i]);
  }
  
  // UNCERTAIN: expected utilities
  for (i in 1:total_uncertain_alts) {
    eta_uncertain[i] = dot_product(psi[i], upsilon);
  }
  
  // RISKY: expected utilities (no beta involved!)
  for (i in 1:total_risky_alts) {
    eta_risky[i] = dot_product(x_risky[i], upsilon);  // x_risky is known
  }
  
  // Choice probabilities via softmax with shared alpha
  // ... (for both uncertain and risky problems)
}

// === MODEL ===
model {
  // Priors (same as m_0)
  alpha ~ lognormal(0, 1);
  to_vector(beta) ~ std_normal();
  delta ~ dirichlet(rep_vector(1, K-1));
  
  // Likelihood: uncertain choices
  for (m in 1:M) {
    y[m] ~ categorical(chi_uncertain[m]);
  }
  
  // Likelihood: risky choices
  for (n in 1:N) {
    z[n] ~ categorical(chi_risky[n]);
  }
}
'''
print(m1_code)
```

The critical line is `eta_risky[i] = dot_product(x_risky[i], upsilon)`. Unlike uncertain alternatives where β mediates between features and probabilities, risky alternatives have their probabilities given directly. This means risky choices provide *direct* information about the utility vector υ.

## Why Risky Choices Identify δ

Consider a concrete example. Suppose we have K=3 consequences with utilities $\upsilon = (0, \upsilon_2, 1)$ where $\upsilon_2 = \delta_1$ (since $\delta_1 + \delta_2 = 1$ and utilities are constructed cumulatively).

**Risky choice**: Choose between:

- Lottery A: (0.5, 0, 0.5) → outcomes 1 or 3 with equal probability
- Lottery B: (0, 1, 0) → outcome 2 with certainty

Expected utilities:
$$
\eta_A = 0.5 \cdot 0 + 0 \cdot \upsilon_2 + 0.5 \cdot 1 = 0.5
$$
$$
\eta_B = 0 \cdot 0 + 1 \cdot \upsilon_2 + 0 \cdot 1 = \upsilon_2
$$

The decision-maker prefers the alternative with higher expected utility:

- If they choose A, then $\eta_A > \eta_B$, implying $0.5 > \upsilon_2$
- If they choose B, then $\eta_B > \eta_A$, implying $\upsilon_2 > 0.5$

This preference *directly* constrains υ₂ (and hence δ₁) without any confounding from subjective probabilities! More generally, by presenting lotteries that span the consequence space, we can triangulate the utility function with arbitrary precision (given sufficient data and lottery diversity).

## Study Design for m_1

To demonstrate that adding risky choices resolves the δ identification problem, we'll use:

```{python}
#| label: m1-study-config
#| echo: true

# Study design for m_1 combining uncertain and risky problems
config_m1 = {
    # Uncertain problems (same as m_0)
    "M": 25,                    # Number of uncertain decision problems
    "K": 3,                     # Number of consequences
    "D": 5,                     # Feature dimensions
    "R": 15,                    # Distinct uncertain alternatives
    "min_alts_per_problem": 2,
    "max_alts_per_problem": 5,
    "feature_dist": "normal",
    "feature_params": {"loc": 0, "scale": 1},
    
    # Risky problems (new)
    "N": 25,                    # Number of risky decision problems
    "S": 15,                    # Distinct risky alternatives
}

print(f"Study Design for Model m_1:")
print(f"\nUncertain Problems:")
print(f"  M = {config_m1['M']} decision problems")
print(f"  R = {config_m1['R']} distinct alternatives")
print(f"  K = {config_m1['K']} consequences")
print(f"\nRisky Problems:")
print(f"  N = {config_m1['N']} decision problems")
print(f"  S = {config_m1['S']} distinct lotteries")
print(f"  Same K = {config_m1['K']} consequences")
```

## Parameter Recovery Comparison

In [Report 4](04_parameter_recovery.qmd), we found that δ recovery was poor in model m_0, with credible intervals that remained wide even when we doubled the sample size from M=25 to M=50. We diagnosed this as a **structural identification problem**: decisions under uncertainty confound utilities and subjective probabilities.

Now we test whether adding risky choices resolves this issue. If our theoretical analysis is correct, the risky choices should "pin down" the utility function, allowing δ to be accurately recovered.

### Study Design for m_1

We use the study design specified earlier, combining M=25 uncertain problems with N=25 risky problems. This gives us 50 total decision problems—matching the sample size used in the M=50 analysis of m_0 (Report 4):

```{python}
#| label: setup-m1-recovery
#| output: false

from utils.study_design_m1 import StudyDesignM1
from analysis.parameter_recovery import ParameterRecovery
import tempfile

# Create the m_1 study design
study_m1 = StudyDesignM1(
    M=config_m1["M"],           # 25 uncertain problems
    N=config_m1["N"],           # 25 risky problems
    K=config_m1["K"],           # 3 consequences
    D=config_m1["D"],           # 5 feature dimensions
    R=config_m1["R"],           # 15 uncertain alternatives
    S=config_m1["S"],           # 8 risky alternatives
    min_alts_per_problem=config_m1["min_alts_per_problem"],
    max_alts_per_problem=config_m1["max_alts_per_problem"],
    risky_probs="random",       # Random simplex probabilities (diverse lotteries)
    feature_dist=config_m1["feature_dist"],
    feature_params=config_m1["feature_params"],
    design_name="m1_parameter_recovery"
)
study_m1.generate()
```

```{python}
#| label: design-m1-summary
#| echo: false

# Summarize the design
print(f"m_1 Study Design Generated:")
print(f"\n  Uncertain Problems:")
print(f"    M = {study_m1.M} problems")
print(f"    R = {study_m1.R} distinct alternatives")
print(f"    Total uncertain choices: ~{np.sum(study_m1.I)}")

print(f"\n  Risky Problems:")
print(f"    N = {study_m1.N} problems")
print(f"    S = {study_m1.S} distinct lotteries")
print(f"    Total risky choices: ~{np.sum(study_m1.J)}")

print(f"\n  Shared:")
print(f"    K = {study_m1.K} consequences")
print(f"    Total choices: ~{np.sum(study_m1.I) + np.sum(study_m1.J)}")
```

### Running Parameter Recovery for m_1

```{python}
#| label: run-m1-recovery
#| output: false

# Create output directory
output_dir_m1 = tempfile.mkdtemp(prefix="param_recovery_m1_")

# Initialize and run parameter recovery for m_1
recovery_m1 = ParameterRecovery(
    inference_model_path=os.path.join(project_root, "models", "m_1.stan"),
    sim_model_path=os.path.join(project_root, "models", "m_1_sim.stan"),
    study_design=study_m1,
    output_dir=output_dir_m1,
    n_mcmc_samples=1000,
    n_mcmc_chains=4,
    n_iterations=20
)

true_params_m1, posterior_summaries_m1 = recovery_m1.run()
```

```{python}
#| label: m1-recovery-summary
#| echo: false

n_successful_m1 = len(true_params_m1)
print(f"\nm_1 Parameter Recovery Complete:")
print(f"  Successful iterations: {n_successful_m1}")
```

### Comparing m_0 and m_1 Recovery

Now we compare the key parameter recovery metrics between models. We focus particularly on δ, the parameter that was poorly identified in m_0:

```{python}
#| label: compute-m1-metrics
#| output: false

# Compute recovery metrics for m_1

# Alpha
alpha_true_m1 = np.array([p['alpha'] for p in true_params_m1])
alpha_mean_m1 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m1])
alpha_lower_m1 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m1])
alpha_upper_m1 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m1])

alpha_bias_m1 = np.mean(alpha_mean_m1 - alpha_true_m1)
alpha_rmse_m1 = np.sqrt(np.mean((alpha_mean_m1 - alpha_true_m1)**2))
alpha_coverage_m1 = np.mean((alpha_true_m1 >= alpha_lower_m1) & (alpha_true_m1 <= alpha_upper_m1))
alpha_ci_width_m1 = np.mean(alpha_upper_m1 - alpha_lower_m1)

# Delta
K_minus_1 = config_m1['K'] - 1
delta_stats_m1 = []
for k in range(K_minus_1):
    param_name = f"delta[{k+1}]"
    
    delta_true = np.array([p['delta'][k] for p in true_params_m1])
    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m1])
    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m1])
    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m1])
    
    delta_stats_m1.append({
        'parameter': f'δ_{k+1}',
        'true': delta_true,
        'mean': delta_mean,
        'lower': delta_lower,
        'upper': delta_upper,
        'bias': np.mean(delta_mean - delta_true),
        'rmse': np.sqrt(np.mean((delta_mean - delta_true)**2)),
        'coverage': np.mean((delta_true >= delta_lower) & (delta_true <= delta_upper)),
        'ci_width': np.mean(delta_upper - delta_lower)
    })

delta_df_m1 = pd.DataFrame(delta_stats_m1)

# Beta (for comparison)
K, D = config_m1['K'], config_m1['D']
beta_stats_m1 = []
for k in range(K):
    for d in range(D):
        param_name = f"beta[{k+1},{d+1}]"
        beta_true = np.array([p['beta'][k][d] for p in true_params_m1])
        beta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m1])
        beta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m1])
        beta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m1])
        
        beta_stats_m1.append({
            'rmse': np.sqrt(np.mean((beta_mean - beta_true)**2)),
            'coverage': np.mean((beta_true >= beta_lower) & (beta_true <= beta_upper)),
            'ci_width': np.mean(beta_upper - beta_lower)
        })

beta_df_m1 = pd.DataFrame(beta_stats_m1)
```

```{python}
#| label: fig-m0-m1-delta-comparison
#| fig-cap: "Comparison of δ recovery between m_0 and m_1. Adding risky choices dramatically improves recovery: narrower credible intervals and better coverage."

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

for k in range(K_minus_1):
    # m_1 True vs Estimated (top row)
    ax = axes[0, k]
    ax.scatter(delta_stats_m1[k]['true'], delta_stats_m1[k]['mean'], 
               alpha=0.7, s=60, c='forestgreen', edgecolor='white', label='m_1')
    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Identity')
    ax.set_xlim(-0.05, 1.05)
    ax.set_ylim(-0.05, 1.05)
    ax.set_xlabel(f'True δ_{k+1}', fontsize=11)
    ax.set_ylabel(f'Estimated δ_{k+1}', fontsize=11)
    ax.set_title(f'm_1: δ_{k+1} Recovery (RMSE={delta_stats_m1[k]["rmse"]:.3f})', fontsize=11)
    ax.set_aspect('equal')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # m_1 Coverage (bottom row)
    ax = axes[1, k]
    iterations = np.arange(len(delta_stats_m1[k]['true']))
    for i in range(len(delta_stats_m1[k]['true'])):
        covered = ((delta_stats_m1[k]['true'][i] >= delta_stats_m1[k]['lower'][i]) & 
                   (delta_stats_m1[k]['true'][i] <= delta_stats_m1[k]['upper'][i]))
        color = 'forestgreen' if covered else 'crimson'
        ax.plot([i, i], [delta_stats_m1[k]['lower'][i], delta_stats_m1[k]['upper'][i]], 
                color=color, linewidth=2, alpha=0.7)
        ax.scatter(i, delta_stats_m1[k]['mean'][i], color=color, s=40, zorder=3)
    ax.scatter(iterations, delta_stats_m1[k]['true'], color='black', s=60, marker='x',
               label='True value', zorder=4, linewidth=2)
    ax.set_xlabel('Iteration', fontsize=11)
    ax.set_ylabel(f'δ_{k+1}', fontsize=11)
    ax.set_title(f'm_1: δ_{k+1} Coverage = {delta_stats_m1[k]["coverage"]:.0%}', fontsize=11)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-delta-ci-width-comparison
#| fig-cap: "Credible interval widths for δ parameters: m_0 (uncertain only) vs. m_1 (uncertain + risky). The dramatic narrowing in m_1 reflects successful identification."
#| output: false

from utils.study_design import StudyDesign

# We need m_0 recovery results for comparison - run a quick recovery
# (In practice, we'd load saved results; here we generate for comparison)
study_m0_compare = StudyDesign(
    M=25, K=3, D=5, R=15,
    min_alts_per_problem=2,
    max_alts_per_problem=5,
    feature_dist="normal",
    feature_params={"loc": 0, "scale": 1},
    design_name="m0_for_comparison"
)
study_m0_compare.generate()

output_dir_m0 = tempfile.mkdtemp(prefix="param_recovery_m0_compare_")
recovery_m0_compare = ParameterRecovery(
    inference_model_path=None,  # Default m_0.stan
    sim_model_path=None,        # Default m_0_sim.stan
    study_design=study_m0_compare,
    output_dir=output_dir_m0,
    n_mcmc_samples=1000,
    n_mcmc_chains=4,
    n_iterations=20
)
true_params_m0, posterior_summaries_m0 = recovery_m0_compare.run()

# Compute m_0 delta metrics
delta_stats_m0 = []
for k in range(K_minus_1):
    param_name = f"delta[{k+1}]"
    delta_true = np.array([p['delta'][k] for p in true_params_m0])
    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0])
    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0])
    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0])
    
    delta_stats_m0.append({
        'parameter': f'δ_{k+1}',
        'rmse': np.sqrt(np.mean((delta_mean - delta_true)**2)),
        'coverage': np.mean((delta_true >= delta_lower) & (delta_true <= delta_upper)),
        'ci_width': np.mean(delta_upper - delta_lower)
    })
```

```{python}
#| label: fig-model-comparison-bars
#| fig-cap: "Direct comparison of δ recovery metrics between m_0 and m_1. The improvement in all three metrics confirms that risky choices solve the identification problem."

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

params = ['δ₁', 'δ₂']
x = np.arange(len(params))
width = 0.35

# RMSE comparison
ax = axes[0]
rmse_m0 = [delta_stats_m0[k]['rmse'] for k in range(K_minus_1)]
rmse_m1 = [delta_stats_m1[k]['rmse'] for k in range(K_minus_1)]

bars1 = ax.bar(x - width/2, rmse_m0, width, label='m_0 (uncertain only)', color='steelblue', alpha=0.7)
bars2 = ax.bar(x + width/2, rmse_m1, width, label='m_1 (uncertain + risky)', color='forestgreen', alpha=0.7)
ax.set_xlabel('Parameter', fontsize=12)
ax.set_ylabel('RMSE', fontsize=12)
ax.set_title('δ RMSE by Model', fontsize=12)
ax.set_xticks(x)
ax.set_xticklabels(params)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# Add improvement percentages
for i, (m0_val, m1_val) in enumerate(zip(rmse_m0, rmse_m1)):
    improvement = (m0_val - m1_val) / m0_val * 100
    ax.annotate(f'-{improvement:.0f}%', xy=(i + width/2, m1_val), 
                xytext=(0, 5), textcoords='offset points',
                ha='center', fontsize=10, color='darkgreen')

# Coverage comparison
ax = axes[1]
cov_m0 = [delta_stats_m0[k]['coverage'] for k in range(K_minus_1)]
cov_m1 = [delta_stats_m1[k]['coverage'] for k in range(K_minus_1)]

bars1 = ax.bar(x - width/2, cov_m0, width, label='m_0', color='steelblue', alpha=0.7)
bars2 = ax.bar(x + width/2, cov_m1, width, label='m_1', color='forestgreen', alpha=0.7)
ax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')
ax.set_xlabel('Parameter', fontsize=12)
ax.set_ylabel('Coverage', fontsize=12)
ax.set_title('δ 90% CI Coverage by Model', fontsize=12)
ax.set_xticks(x)
ax.set_xticklabels(params)
ax.set_ylim(0, 1.05)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# CI Width comparison
ax = axes[2]
ci_m0 = [delta_stats_m0[k]['ci_width'] for k in range(K_minus_1)]
ci_m1 = [delta_stats_m1[k]['ci_width'] for k in range(K_minus_1)]

bars1 = ax.bar(x - width/2, ci_m0, width, label='m_0', color='steelblue', alpha=0.7)
bars2 = ax.bar(x + width/2, ci_m1, width, label='m_1', color='forestgreen', alpha=0.7)
ax.set_xlabel('Parameter', fontsize=12)
ax.set_ylabel('CI Width', fontsize=12)
ax.set_title('δ 90% CI Width by Model', fontsize=12)
ax.set_xticks(x)
ax.set_xticklabels(params)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# Add reduction percentages
for i, (m0_val, m1_val) in enumerate(zip(ci_m0, ci_m1)):
    reduction = (m0_val - m1_val) / m0_val * 100
    ax.annotate(f'-{reduction:.0f}%', xy=(i + width/2, m1_val),
                xytext=(0, 5), textcoords='offset points',
                ha='center', fontsize=10, color='darkgreen')

plt.tight_layout()
plt.show()
```

### Summary Statistics

```{python}
#| label: tbl-model-comparison
#| tbl-cap: "Parameter recovery comparison between m_0 and m_1. Adding risky choices dramatically improves δ recovery while maintaining good recovery for other parameters."

# Build comparison table
comparison_rows = []

# Alpha
comparison_rows.append({
    'Parameter': 'α',
    'm_0 RMSE': f'{np.sqrt(np.mean((np.array([s.loc["alpha", "Mean"] for s in posterior_summaries_m0]) - np.array([p["alpha"] for p in true_params_m0]))**2)):.4f}',
    'm_1 RMSE': f'{alpha_rmse_m1:.4f}',
    'm_0 Coverage': f'{np.mean((np.array([p["alpha"] for p in true_params_m0]) >= np.array([s.loc["alpha", "5%"] for s in posterior_summaries_m0])) & (np.array([p["alpha"] for p in true_params_m0]) <= np.array([s.loc["alpha", "95%"] for s in posterior_summaries_m0]))):.0%}',
    'm_1 Coverage': f'{alpha_coverage_m1:.0%}',
    'm_0 CI Width': f'{np.mean(np.array([s.loc["alpha", "95%"] for s in posterior_summaries_m0]) - np.array([s.loc["alpha", "5%"] for s in posterior_summaries_m0])):.3f}',
    'm_1 CI Width': f'{alpha_ci_width_m1:.3f}'
})

# Delta
for k in range(K_minus_1):
    comparison_rows.append({
        'Parameter': f'δ_{k+1}',
        'm_0 RMSE': f'{delta_stats_m0[k]["rmse"]:.4f}',
        'm_1 RMSE': f'{delta_stats_m1[k]["rmse"]:.4f}',
        'm_0 Coverage': f'{delta_stats_m0[k]["coverage"]:.0%}',
        'm_1 Coverage': f'{delta_stats_m1[k]["coverage"]:.0%}',
        'm_0 CI Width': f'{delta_stats_m0[k]["ci_width"]:.3f}',
        'm_1 CI Width': f'{delta_stats_m1[k]["ci_width"]:.3f}'
    })

# Beta (aggregated)
beta_rmse_m0_vals = []
beta_cov_m0_vals = []
beta_ci_m0_vals = []
for k in range(K):
    for d in range(D):
        param_name = f"beta[{k+1},{d+1}]"
        beta_true = np.array([p['beta'][k][d] for p in true_params_m0])
        beta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0])
        beta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0])
        beta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0])
        beta_rmse_m0_vals.append(np.sqrt(np.mean((beta_mean - beta_true)**2)))
        beta_cov_m0_vals.append(np.mean((beta_true >= beta_lower) & (beta_true <= beta_upper)))
        beta_ci_m0_vals.append(np.mean(beta_upper - beta_lower))

comparison_rows.append({
    'Parameter': 'β (mean)',
    'm_0 RMSE': f'{np.mean(beta_rmse_m0_vals):.4f}',
    'm_1 RMSE': f'{beta_df_m1["rmse"].mean():.4f}',
    'm_0 Coverage': f'{np.mean(beta_cov_m0_vals):.0%}',
    'm_1 Coverage': f'{beta_df_m1["coverage"].mean():.0%}',
    'm_0 CI Width': f'{np.mean(beta_ci_m0_vals):.3f}',
    'm_1 CI Width': f'{beta_df_m1["ci_width"].mean():.3f}'
})

comparison_df = pd.DataFrame(comparison_rows)
print(comparison_df.to_string(index=False))
```

::: {.callout-tip}
## Key Result: δ Identification Substantially Improved

The parameter recovery comparison confirms our theoretical analysis:

1. **RMSE dramatically reduced**: δ estimation error decreases substantially in m_1
2. **Coverage calibrated**: 90% CIs now achieve near-target coverage
3. **Uncertainty quantified**: CI widths reflect achievable precision, not lack of identification

The difference between m_0 and m_1 is not just quantitative—it's qualitative. Model m_0's wide intervals reflected *lack of identification*, while m_1's narrower intervals reflect genuine *statistical uncertainty* given sufficient data.

**Important caveat**: The degree of improvement depends on experimental design—specifically, the diversity of lotteries presented. Lotteries should span the consequence space to effectively constrain the utility function.
:::

### Why This Works

The key to understanding this improvement is the role of risky choices in the likelihood:

**In m_0** (uncertain choices only):
$$
\log p(y | \theta) = \sum_{m=1}^M \log \chi_{m, y_m} \quad \text{where} \quad \chi \propto \exp(\alpha \cdot \boldsymbol{\psi}^\top \boldsymbol{\upsilon})
$$

Here, ψ (from β) and υ (from δ) only appear as a product—they cannot be separated.

**In m_1** (uncertain + risky choices):
$$
\log p(y, z | \theta) = \underbrace{\sum_{m=1}^M \log \chi^{(u)}_{m, y_m}}_{\text{Identifies } \alpha, \beta \text{ (given } \delta\text{)}} + \underbrace{\sum_{n=1}^N \log \chi^{(r)}_{n, z_n}}_{\text{Identifies } \delta \text{ directly}}
$$

The risky choice likelihood depends on δ through υ = cumsum(δ), but **not** on β. This breaks the confounding and allows each parameter to be identified from the appropriate data source.

## Summary

The extension from m_0 to m_1 follows a principled path grounded in decision theory:

1. **The identification problem** (Report 4): Uncertain choices alone cannot separate utilities from beliefs
2. **The historical solution**: Anscombe-Aumann's insight that combining risk and uncertainty enables identification
3. **The implementation**: Model m_1 adds risky choice problems with known probabilities
4. **The result**: δ parameters become identifiable from risky choices, while β remains identified from uncertain choices

::: {.callout-tip}
## Key Insight
By incorporating risky choices alongside uncertain choices, model m_1 achieves what m_0 could not: full identification of all model parameters. This is not a statistical trick but reflects the fundamental structure of expected utility theory—we need different types of evidence to pin down different aspects of preferences.
:::

## References
