# SEU Sensitivity: From Abstract Motivation to Concrete Implementation

## Abstract Formulation

### Data Structure

Assume we have descriptions of $R$ alternatives, that these descriptions can be encoded as $D$-dimensional vectors, and that we have created $M$ decision problems involving various subsets of these descriptions. Data collection consists of presenting the subject with each decision problem $m \in \{1, \ldots, M\}$ and recording their choice $y_m$ from the $N_m$ available alternatives.

After data collection, we have the following data structure:

- $M$: number of decision problems
- $K$: number of possible consequences for each alternative
- $D$: dimensionality of alternative feature vectors
- $R$: number of distinct alternatives
- $\mathbf{W} = \{\mathbf{w}_1, \ldots, \mathbf{w}_R\}$: feature vectors for each alternative, where $\mathbf{w}_r \in \mathbb{R}^D$
- $\mathbf{I} = (I_{m,r})_{m=1,\ldots,M; r=1,\ldots,R}$: indicator matrix specifying which alternatives appear in which problems
- $\mathbf{y} = (y_1, \ldots, y_M)$: observed choices, where $y_m \in \{1, \ldots, N_m\}$

This data structure supports a wide range of experiments. For now, however, we assume that the order of presentation—both across problems and within problems—is irrelevant to our analysis. This is a strong simplification that is surely false in realistic settings, but it serves as a starting point for building intuition about the core model.

### Abstract Model

Assume the observed choices $\mathbf{y}$ are generated by a decision maker with utilities $\boldsymbol{\upsilon}$ for each of the $K$ possible consequences, a procedure for assigning subjective probabilities $\boldsymbol{\psi}$ to the consequences of alternatives based on their descriptions, and a positive-valued $\alpha$ that measures the decision maker’s sensitivity to subjective expected utility (SEU) maximization:

The subjective expected utility (SEU) of alternative $r$ is:

$$\eta_r = \sum_{k=1}^K \psi_{r,k} \cdot \upsilon_k = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}$$

The probability that the decision maker chooses alternative $r$ from the set of alternatives in problem $m$ is given by a softmax:

$$P(\text{choose } r \mid m, \alpha, \boldsymbol{\psi}, \boldsymbol{\upsilon}) = \frac{\exp(\alpha \cdot \eta_r)}{\sum_{j: I_{m,j}=1} \exp(\alpha \cdot \eta_j)}$$

Three basic properties of this abstract model are central to interpreting $\alpha$ as a measure of sensitivity to SEU maximization:

1. **Monotonicity**: The probability of choosing SEU-maximizing alternatives increases as sensitivity increases.
2. **Perfect Rationality Limit**: As $\alpha \to \infty$, choice probabilities concentrate on alternatives that maximize subjective expected utility.
3. **Random Choice Limit**: As $\alpha \to 0$, choice probabilities converge to a uniform distribution over available alternatives.

These properties follow from the softmax structure and its behavior under scaling. We review the proofs in the appendix.

A final note: we assume standardized utilities $\boldsymbol{\upsilon}$ both for theoretical reasons (in SEU theory, utility is unique only up to positive affine transformations) and for identification when fitting the concrete model.

### Concrete Implementation

We now turn to a concrete instantiation of the model, formulated in Stan (`models/m_0.stan`).

**Data Block**

This is just the data structure described above in Stan syntax:

````stan
// filepath: models/m_0.stan (excerpt)
data {
  int<lower=1> M;                    // number of decision problems
  int<lower=2> K;                    // number of possible consequences
  int<lower=1> D;                    // number of dimensions to describe an alternative
  int<lower=2> R;                    // number of distinct alternatives
  array[R] vector[D] w;              // feature vectors for alternatives
  array[M,R] int<lower=0,upper=1> I; // indicator matrix
  array[M] int<lower=1> y;           // observed choices
}
````

**Parameters Block**

The model estimates three sets of parameters:

````stan
// filepath: models/m_0.stan (excerpt)
parameters {
  real<lower=0> alpha;           // sensitivity parameter
  matrix[K,D] beta;              // feature-to-probability mapping
  simplex[K-1] delta;            // utility increments
}
````

`beta` parameterizes the mapping from features to subjective probabilities, and `delta` parameterizes the 0–1 scaling of utilities.

**Transformed Parameters Block**

The subjective probabilities, utilities, and expected utilities are defined as transformed parameters:

````stan
// filepath: models/m_0.stan (excerpt)
transformed parameters {
  array[sum(N)] simplex[K] psi;  // subjective probabilities
  ordered[K] upsilon;            // utilities (ordered, on unit scale)
  vector[sum(N)] eta;            // expected utilities
  array[M] simplex[max(N)] chi;  // choice probabilities
  
  // Calculate subjective probabilities via softmax
  for (i in 1:sum(N)) {
    psi[i] = softmax(beta * x[i]);
  }
  
  // Construct ordered utilities from increments
  upsilon = cumulative_sum(append_row(0, delta));
  
  // Calculate expected utility for each alternative
  for (i in 1:sum(N)) {
    eta[i] = dot_product(psi[i], upsilon);
  }
  
  // Calculate choice probabilities with sensitivity scaling
  // ...existing code...
  chi[i] = append_row(
    softmax(alpha * problem_eta),
    rep_vector(0, max(N) - N[i])
  );
  // ...existing code...
}
````

**Remarks**

1. Subjective Probabilities from Descriptions: We assume that the procedure for assigning subjective probabilities to an alternative’s possible consequences, based on its description, can be represented as a softmax applied to a linear mapping from descriptions to log-odds.

2. Utilities: We parameterize the 0–1-scaled utilities via cumulative sums:
   - $\boldsymbol{\upsilon} = \text{cumsum}([0, \delta_1, \ldots, \delta_{K-1}])$
   - $\boldsymbol{\delta} \sim \text{Dirichlet}(1, \ldots, 1)$ ensures $\sum_i \delta_i = 1$, implying $\upsilon_K = 1$.

3. Choice Probabilities: The softmax choice rule from the abstract model is implemented directly:
   - $P(\text{choose } r \mid m, \alpha, \psi, \upsilon) = \exp(\alpha \cdot \eta_r) / \sum_j \exp(\alpha \cdot \eta_j)$

**Model Block**

Prior distributions and the likelihood function complete the Bayesian specification:

````stan
// filepath: models/m_0.stan (excerpt)
model {
  // Priors
  alpha ~ lognormal(0, 1);               // sensitivity
  to_vector(beta) ~ std_normal();        // feature coefficients
  delta ~ dirichlet(rep_vector(1,K-1));  // utility increments
  
  // Likelihood
  for (i in 1:M) {
    y[i] ~ categorical(chi[i]);
  }
}
````

