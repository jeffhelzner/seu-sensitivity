# SEU Sensitivity: From Abstract Motivation to Concrete Implementation

## Abstract Formulation

### Data Structure

Assume we have descriptions of $R$ alternatives, that these descriptions can be encoded as $D$-dimensional vectors, and that we have created $M$ decision problems involving various subsets of these descriptions. Data collection consists of presenting the subject with each decision problem $m \in \{1, \ldots, M\}$ and recording their choice $y_m$ from the $N_m$ available alternatives.

After data collection, we have the following data structure:

- $M$: number of decision problems
- $K$: number of possible consequences for each alternative
- $D$: dimensionality of alternative feature vectors
- $R$: number of distinct alternatives
- $\mathbf{W} = \{\mathbf{w}_1, \ldots, \mathbf{w}_R\}$: feature vectors for each alternative, where $\mathbf{w}_r \in \mathbb{R}^D$
- $\mathbf{I} = (I_{m,r})_{m=1:M,r=1:R}$: indicator matrix specifying which alternatives appear in which problems
- $\mathbf{y} = (y_1, \ldots, y_M)$: observed choices, where $y_m \in \{1, \ldots, N_m\}$

This data structure supports a wide range of experiments. For now, however, we assume that the order of presentation—both across problems and within problems—is irrelevant to our analysis. This is a strong simplification that is surely false in realistic settings, but it serves as a starting point for building intuition about the core model. 

Our use of the term "description" is more suggestive than formal. Formally speaking, the only requirement is that these descriptions can be represented in a given finite-dimensional feature space. On the other hand, it is intended to be suggestive, as we want to consider experiments where subjects are presented with descriptions of various alternatives (e.g., monetary gambles described by probabilities and outcomes, consumer products described by attributes, etc.) and then are asked to choose among them. 

### Model Type

Assume that the observed choices $\mathbf{y}$ are generated by a decision maker with utilities $\boldsymbol{\upsilon}$ for each of the $K$ possible consequences, a functional procedure for assigning subjective probabilities $\boldsymbol{\psi}$ to the consequences of alternatives based on their descriptions, and a positive-valued $\alpha$ that measures the decision maker's sensitivity to subjective expected utility (SEU) maximization.

The subjective expected utility (SEU) of alternative $r$ is:

$$\eta_r = \sum_{k=1}^K \psi_{r,k} \cdot \upsilon_k = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}$$

The probability that the decision maker chooses alternative $r$ from the set of alternatives in problem $m$ is given by a softmax:

$$P(\text{choose } r \mid m, \alpha, \boldsymbol{\psi}, \boldsymbol{\upsilon}) = \frac{\exp(\alpha \cdot \eta_r)}{\sum_{j: I_{m,j}=1} \exp(\alpha \cdot \eta_j)}$$

Conditional on the problem $m$ and utilities $\upsilon$, the following properties of this model type are central to interpreting $\alpha$ as a measure of sensitivity to SEU maximization:

1. **Monotonicity**: The probability of choosing SEU-maximizing alternatives increases as sensitivity increases.
2. **Perfect Rationality Limit**: As $\alpha \to \infty$, choice probabilities concentrate on alternatives that maximize subjective expected utility.
3. **Random Choice Limit**: As $\alpha \to 0$, choice probabilities converge to a uniform distribution over available alternatives.

These properties follow from the softmax structure and its behavior under scaling. We review the details in the appendix.

Note that we will need to assume standardized utilities $\boldsymbol{\upsilon}$ both for theoretical reasons (in SEU theory, utility is unique only up to positive affine transformations) and for the purpose of identification when fitting a model the this type.

### Concrete Implementation

We now turn to a model of the type described above, as formulated in the Stan program (`models/m_0.stan`).

**Data Block**

This is just the data structure described above in Stan syntax:

````stan
// filepath: models/m_0.stan (excerpt)
data {
  int<lower=1> M;                    // number of decision problems
  int<lower=2> K;                    // number of possible consequences
  int<lower=1> D;                    // number of dimensions to describe an alternative
  int<lower=2> R;                    // number of distinct alternatives
  array[R] vector[D] w;              // feature vectors for alternatives
  array[M,R] int<lower=0,upper=1> I; // indicator matrix
  array[M] int<lower=1> y;           // observed choices
}
````

**Parameters Block**

The Stan program estimates three sets of parameters:

````stan
// filepath: models/m_0.stan (excerpt)
parameters {
  real<lower=0> alpha;           // sensitivity parameter
  matrix[K,D] beta;              // feature-to-probability mapping
  simplex[K-1] delta;            // utility increments
}
````

`beta` parameterizes the mapping from features to subjective probabilities, and `delta` parameterizes the 0–1 scaling of utilities.

**Transformed Parameters Block**

The subjective probabilities, utilities, and expected utilities are defined as transformed parameters:

````stan
// filepath: models/m_0.stan (excerpt)
transformed parameters {
  array[sum(N)] simplex[K] psi;  // subjective probabilities
  ordered[K] upsilon;            // utilities (ordered, on unit scale)
  vector[sum(N)] eta;            // expected utilities
  array[M] simplex[max(N)] chi;  // choice probabilities
  
  // Calculate subjective probabilities via softmax
  for (i in 1:sum(N)) {
    psi[i] = softmax(beta * x[i]);
  }
  
  // Construct ordered utilities from increments
  upsilon = cumulative_sum(append_row(0, delta));
  
  // Calculate expected utility for each alternative
  for (i in 1:sum(N)) {
    eta[i] = dot_product(psi[i], upsilon);
  }
  
  // Calculate choice probabilities with sensitivity scaling
  // ...existing code...
  chi[i] = append_row(
    softmax(alpha * problem_eta),
    rep_vector(0, max(N) - N[i])
  );
  // ...existing code...
}
````

**Remarks**

1. **Subjective Probabilities from Descriptions**: We assume that the procedure for assigning subjective probabilities to an alternative's possible consequences, based on its description, can be represented as a softmax applied to a linear mapping from descriptions to log-odds. This seems like a reasonable starting point, though more complex mappings could be considered in future work.

2. **Utilities**: We parameterize the 0–1-scaled utilities via cumulative sums:
   - $\boldsymbol{\upsilon} = \text{cumsum}([0, \delta_1, \ldots, \delta_{K-1}])$
   - $\boldsymbol{\delta} \sim \text{Dirichlet}(1, \ldots, 1)$ ensures $\sum_i \delta_i = 1$, implying $\upsilon_K = 1$.
  
  Note that the Dirichlet prior on $\delta$ induces a uniform prior over the space of possible utility vectors on the unit scale. Given that the utilities are assumed to represent the decision maker's preferences, it might make more sense to increase the concentration parameter to get more separation between the alternatives, e.g., for psychological plausibility. Increasing the concentration should also reduce the chance of non-fatal numerical issues when two utilities are very close together, causing Stan's ordered vector type to complain about the resulting $\upsilon$ not being strictly ordered.

3. **Choice Probabilities**: The softmax choice rule from the abstract model is implemented directly:
   - $P(\text{choose } r \mid m, \alpha, \psi, \upsilon) = \exp(\alpha \cdot \eta_r) / \sum_j \exp(\alpha \cdot \eta_j)$

**Model Block**

Prior distributions and the likelihood function complete the Bayesian specification:

````stan
// filepath: models/m_0.stan (excerpt)
model {
  // Priors
  alpha ~ lognormal(0, 1);               // sensitivity
  to_vector(beta) ~ std_normal();        // feature coefficients
  delta ~ dirichlet(rep_vector(1,K-1));  // utility increments
  
  // Likelihood
  for (i in 1:M) {
    y[i] ~ categorical(chi[i]);
  }
}
````

There was no particular reason for selecting these priors. Of course, the same could be said of other parts of the model, e.g., linearity assumptions, which, as with the choice of priors, were made for mathematical and computational simplicity. 

## Prior Analysis

We now turn to a prior analysis of the m_0.stan program described above (note: we actually run the analysis on the program models/m_0_sim.stan, but this is a purely technical matter related to Stan). The prior predictive analysis samples parameter configurations from the prior, simulates choices under those parameters, and examines the resulting distributions.

### Study Design

For the present analysis, we will use a study design with the following characteristics (see `study_design.json` in this directory):

- **M = 10** decision problems
- **K = 3** possible consequences per alternative
- **D = 2** feature dimensions
- **R = 5** distinct alternatives
- Alternatives per problem: 2 to 4 (mean = 3.2)
- Alternative appearance frequency: 3 to 8 appearances (mean = 6.4)

**Feature Vectors (w)**: The 5 distinct alternatives are characterized by 2-dimensional feature vectors:

```json
"w": [
  [0.901, -0.404],   // Alternative 1
  [0.658, -0.658],   // Alternative 2
  [0.516,  0.396],   // Alternative 3
  [-0.854, -0.648],  // Alternative 4
  [-0.341, -0.006]   // Alternative 5
]
```

**Indicator Matrix (I)**: The 10×5 indicator matrix specifies which alternatives appear in which problems. For instance:

```json
"I": [
  [0, 1, 0, 1, 1],  // Problem 1: alternatives {2, 4, 5}
  [0, 1, 0, 1, 1],  // Problem 2: alternatives {2, 4, 5}
  [1, 1, 1, 0, 1],  // Problem 3: alternatives {1, 2, 3, 5}
  [0, 1, 1, 1, 1],  // Problem 4: alternatives {2, 3, 4, 5}
  // ... (6 more problems)
]
```

![Alternative Frequency](figures/alt_frequency.png)
*Figure 1: Frequency with which each of the 5 distinct alternatives appears across the 10 decision problems. Alternatives appear between 3-8 times, with alternative 2 appearing most frequently (8 times) and alternatives 1 and 3 appearing least frequently (3 times each).*

![Alternatives Per Problem](figures/alts_per_problem.png)
*Figure 2: Distribution of the number of alternatives in each decision problem.*

### Prior Distribution of Sensitivity Parameter

The sensitivity parameter α, which governs how strongly choices reflect SEU maximization, follows a lognormal(0, 1) prior. This prior produces a wide range of sensitivity values:

![Alpha Distribution](figures/alpha_dist.png)
*Figure 3: Prior distribution of α (sensitivity parameter).*

### Expected Utilities Under the Prior

![Expected Utilities](figures/expected_utilities.png)
*Figure 4: Distribution of expected utilities for alternatives across the 10 decision problems. Each box represents the distribution of expected utility values across 1000 parameter samples from the prior. The variation both within and across problems reflects uncertainty in subjective probabilities (determined by the 3×2 β matrix) and utilities (determined by δ, a 2-simplex for the 3 consequences).*

### Simulated Choice Behavior

![Choice Distribution](figures/choice_distribution.png)
*Figure 5: Empirical choice frequencies for the 10 decision problems.*


### SEU Maximizer Selection

We now turn to an estimate of the probability of selecting an seu maximizer under the prior.

**Computational Implementation**: For each simulated decision problem, we track whether the chosen alternative maximizes subjective expected utility. The implementation in `m_0_sim.stan` performs this check as follows:

````stan
// For each decision problem i with N[i] alternatives:
// 1. Identify the maximum expected utility
real max_eta = max(problem_eta);

// 2. Check if the selected alternative has the maximum expected utility
//    (within numerical tolerance to handle floating-point comparison)
if (abs(problem_eta[y[i]] - max_eta) < 1e-10) {
  selected_seu_max[i] = 1;
} else {
  selected_seu_max[i] = 0;
}
````

This produces two key outputs:
- `selected_seu_max[i]`: Binary indicator for each problem (1 = SEU maximizer selected, 0 = otherwise)
- `total_seu_max_selected`: Sum across all M problems, counting how many times the SEU maximizer was chosen

The numerical tolerance (1e-10) accounts for floating-point arithmetic issues when multiple alternatives have nearly identical expected utilities.

![Probability SEU Max by Problem](figures/prob_seu_max_by_problem.png)
*Figure 6: Probability of selecting the SEU-maximizing alternative for each of the 10 decision problems.*

![Total SEU Max Distribution](figures/total_seu_max_distribution.png)
*Figure 7: Distribution of the total number of problems (out of 10) where the SEU maximizer was selected across 1000 simulations. The distribution shape and central tendency reveal the range of behaviors implied by the prior on α and the other parameters.*

## Parameter Recovery Analysis

Having examined the prior predictive distribution, we now turn to parameter recovery analysis: can the model reliably recover known parameter values from simulated data? This is crucial for assessing whether our proposed experimental designs can support accurate inference about the sensitivity parameter α and the other model parameters.

### Recovery Methodology

The parameter recovery analysis proceeds as follows:

1. **Data Generation**: For each iteration, we sample parameter values from the prior distributions:
   - α ~ lognormal(0, 1)
   - β[k,d] ~ N(0, 1) for each consequence k and dimension d
   - δ ~ Dirichlet(1, ..., 1) for the (K-1)-simplex

2. **Simulation**: Using these "true" parameters, we simulate choice data according to the model.

3. **Inference**: We fit the model to the simulated data using MCMC, obtaining posterior distributions for all parameters.

4. **Evaluation**: We assess recovery quality through:
   - **Bias**: Mean difference between posterior means and true values
   - **RMSE**: Root mean squared error between estimates and true values  
   - **Coverage**: Proportion of 90% credible intervals containing the true value
   - **CI Width**: Average width of 90% credible intervals

### Recovery Summary Statistics

The following table summarizes recovery performance across 20 iterations using our 10-problem study design:

| Parameter    | Bias   | RMSE  | Coverage | CI Width |
|-------------|--------|-------|----------|----------|
| alpha       | 0.045  | 0.512 | 95.0%    | 1.891    |
| beta (avg)  | -0.001 | 0.571 | 91.7%    | 2.189    |
| delta (avg) | 0.000  | 0.104 | 92.5%    | 0.401    |

### Coverage Diagnostics

To understand parameter recovery in detail, we examine the 90% credible intervals for each parameter across all iterations. Green intervals indicate successful coverage (the true value falls within the interval), while red intervals indicate coverage failures.

**Alpha (Sensitivity Parameter)**

![Alpha Coverage](figures/recovery_alpha_coverage.png)
*Figure 8: 90% credible intervals for α across 20 recovery iterations.*

**Beta Parameters (Feature-to-Probability Mapping)**

The β matrix maps the D=2 dimensional feature vectors to subjective probabilities over K=3 consequences, requiring estimation of 6 parameters.

![Beta 1,1 Coverage](figures/recovery_beta_1_1_coverage.png)
*Figure 9: Coverage intervals for β[1,1]. This parameter maps the first feature dimension to the log-odds of the first consequence.*

![Beta 1,2 Coverage](figures/recovery_beta_1_2_coverage.png)
*Figure 10: Coverage intervals for β[1,2]. This parameter maps the second feature dimension to the log-odds of the first consequence.*

![Beta 2,1 Coverage](figures/recovery_beta_2_1_coverage.png)
*Figure 11: Coverage intervals for β[2,1]. This parameter maps the first feature dimension to the log-odds of the second consequence.*

![Beta 2,2 Coverage](figures/recovery_beta_2_2_coverage.png)
*Figure 12: Coverage intervals for β[2,2]. This parameter maps the second feature dimension to the log-odds of the second consequence.*

![Beta 3,1 Coverage](figures/recovery_beta_3_1_coverage.png)
*Figure 13: Coverage intervals for β[3,1]. This parameter maps the first feature dimension to the log-odds of the third consequence.*

![Beta 3,2 Coverage](figures/recovery_beta_3_2_coverage.png)
*Figure 14: Coverage intervals for β[3,2]. This parameter maps the second feature dimension to the log-odds of the third consequence.*

**Delta Parameters (Utility Increments)**

The δ parameters represent increments on the unit utility scale, constrained to sum to 1 (as a 2-simplex for K=3 consequences).

![Delta 1 Coverage](figures/recovery_delta_1_coverage.png)
*Figure 15: Coverage intervals for δ[1], the utility increment from consequence 1 to consequence 2.*

![Delta 2 Coverage](figures/recovery_delta_2_coverage.png)
*Figure 16: Coverage intervals for δ[2], the utility increment from consequence 2 to consequence 3.*

### Observations

It looks like the $\alpha$ parameter, as well as the first and third sets of $\beta$ parameters are showing some evidence of recoverability, while the second set of $\beta$ parameters and both $\delta$ parameters look as though are not being informed by the data at all. Perhaps they are being informed slightly, and we just need to consider a larger study design. We can enlarge in several different directions. In the next section, we will consider increasing the number of problems (M) while holding everything else constant to see if that helps.

## Sample Size Analysis: Recovery vs. Number of Problems

To investigate whether the identification issues observed at M=10 are due to insufficient sample size or represent deeper structural problems, we conducted a systematic sample size analysis. Using the same set of alternatives (w) and varying only M and the indicator matrix I, we examined parameter recovery across M ∈ {10, 15, 20, 25, 30}.

### Methodology

For each value of M:

1. **Fixed Design Elements**: Used the identical 5 alternatives (w) from the base study design
2. **Varying Elements**: Generated new indicator matrices (I) for M problems, with 2-4 alternatives per problem
3. **Recovery Iterations**: Conducted 50 parameter recovery iterations per M value
4. **Metrics Tracked**: 
   - 90% credible interval width
   - Root mean squared error (RMSE)
   - Mean absolute error (MAE)
   - Coverage rate

This design ensures that observed changes in recovery are attributable to M rather than differences in the alternative space.

### Results: Credible Interval Width vs. M

![CI Width Comparison](figures/sample_size_ci_width_comparison.png)
*Figure 17: Mean 90% credible interval width across parameters as a function of M. Left: $\alpha$ shows consistent narrowing. Middle: $\beta[1,]$ and $\beta[3,]$ narrow with M, while $\beta[2,]$ stays relatively constant. Right: The $\delta$ parameters maintain wide intervals across all M values, indicating persistent identification issues.*

### Results: Estimation Error vs. M

![RMSE Comparison](figures/sample_size_rmse_comparison.png)
*Figure 18: Root mean squared error (RMSE) as a function of M. With the possible exception of $\alpha$, and possibly $\beta[1,1]$ and $\beta[3,1]$, these plots don't seem to show the error trending downward with increasing M.*

### Alternative Space: Increasing R from 5 to 15

The initial sample size analysis used R=5 distinct alternatives. To investigate whether the identification issues stem from an insufficiently rich alternative space rather than sample size alone, we conducted a second analysis with R=15 alternatives while maintaining the same range of M values.

**Methodology**: Using a new set of 15 alternatives drawn from the same feature distribution (2-dimensional, standard normal), we repeated the sample size analysis for M ∈ {10, 15, 20, 25, 30}, with 20 recovery iterations per M value. All other aspects of the methodology remained identical to the R=5 analysis.

![CI Width Comparison R15](figures/sample_size_r15/ci_width_comparison.png)
*Figure 19: Mean 90% credible interval width with R=15 alternatives. The pattern closely mirrors the R=5 results: α and selected β parameters narrow with M, while β[2,·] and δ parameters remain wide.*

![RMSE Comparison R15](figures/sample_size_r15/rmse_comparison.png)
*Figure 20: RMSE with R=15 alternatives. Again, the patterns are remarkably similar to R=5: no clear downward trend for most parameters except possibly α and β[1,1] and β[3,1].*

### Observations

The increased alternative space (R=15) did not substantially improve parameter recovery compared to R=5. The persistent identification issues for certain parameters suggest that the model structure or the information content of the data may be limiting factors, rather than sample size or alternative diversity alone. We consider a slight modification to the model structure in the next section to address these issues.

## Alternative Approach: Informative Prior on Utilities

Given the persistent identification issues with the δ parameters (utility increments), we explored whether a more informative prior might help. The original m_0 model uses a symmetric Dirichlet(1,1) prior on the (K-1)-simplex for δ, which induces a uniform distribution over the space of possible utility configurations on [0,1].

### Model m_01: Strengthened Dirichlet Prior

We created model m_01, identical to m_0 except for using `delta ~ dirichlet(rep_vector(5, K-1))` instead of `dirichlet(rep_vector(1, K-1))`. This stronger prior:

- Concentrates probability mass around utilities that are more evenly spaced
- For K=3, encourages the middle utility (upsilon[2]) to be closer to 0.5
- Reduces prior uncertainty about utility configurations

**Prior Predictive Comparison**: Under the Dirichlet(5,5) prior for K=3:
- Mean of middle utility: 0.506 (vs 0.500 under Dirichlet(1,1))
- Std of middle utility: 0.150 (vs 0.289 under Dirichlet(1,1))

The stronger prior substantially reduces the variance of the middle utility from 0.289 to 0.150—approximately a 48% reduction in standard deviation.

### Parameter Recovery Results

We ran the same 20-iteration parameter recovery analysis on the same 10-problem study design used for m_0.

![Delta 1 Coverage m01](figures/recovery_m01/delta_1_coverage.png)
*Figure 21: 90% credible intervals for δ[1] under model m_01 with Dirichlet(5,5) prior. Coverage = 85.0%. The intervals are noticeably narrower than under m_0, reflecting the stronger prior.*

![Delta 2 Coverage m01](figures/recovery_m01/delta_2_coverage.png)
*Figure 22: 90% credible intervals for δ[2] under model m_01 with Dirichlet(5,5) prior. Coverage = 85.0%. Again, intervals are much narrower but the posterior means show no clear relationship to true values.*

### Interpretation

The informative prior approach **successfully reduced posterior uncertainty** but **failed to solve the identification problem**:

1. **Narrower Intervals**: The credible intervals for δ parameters are approximately half as wide as under m_0 (width ~0.50 vs ~0.90), reflecting the informative prior.

2. **Poor Calibration**: Coverage dropped from 92-94% under m_0 to 85%, suggesting the narrower intervals don't appropriately reflect the data's information content.

3. **Unresponsive Posteriors**: Most critically, the posterior means show essentially no relationship to the true parameter values. The posteriors are dominated by the prior—they cluster around δ ≈ 0.5 regardless of the true values.

4. **Regularization vs. Identification**: The informative prior acts as a regularizer that reduces variance, but it cannot create information that isn't in the data. The fundamental issue remains: **choice data under this experimental design do not strongly constrain the utility parameters**.

This negative result is important because it rules out "weak priors" as the primary cause of poor δ recovery. Even with a prior that concentrates 68% of its mass in the range [0.35, 0.65] for the middle utility, the data fail to pull the posterior away from this prior concentration.

The persistent identification issues across different values of M, R, and prior specifications point toward a more fundamental problem: the confounding between subjective probabilities (ψ, determined by β) and utilities (υ, determined by δ) in the expected utility calculation η = ψ'υ. The model may achieve similar choice probabilities with different (β, δ) combinations, making these parameters empirically indistinguishable from choice data alone.



