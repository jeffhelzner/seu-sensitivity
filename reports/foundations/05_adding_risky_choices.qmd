---
title: "Adding Risky Choices"
subtitle: "Foundational Report 5"
description: |
  Extension of the SEU model to include risky choice data, enabling 
  identification of utility parameters through known-probability decisions.
categories: [foundations, m_1]
execute:
  cache: true
---

```{python}
#| label: setup
#| include: false

import sys
import os

# Add parent directories to path
sys.path.insert(0, os.path.join(os.getcwd(), '..'))
project_root = os.path.dirname(os.path.dirname(os.getcwd()))
sys.path.insert(0, project_root)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)
```

## Introduction

[Report 4](04_parameter_recovery.qmd) revealed differential parameter recovery in model m_0: while the sensitivity parameter α and feature weights β are recovered well, the utility increments δ show wider uncertainty and slower learning as sample size increases. This doesn't mean δ is uninformed by the data—the posteriors do concentrate relative to the prior—but the information accumulates more slowly than for other parameters.

One approach to improving δ recovery is simply to collect more data. However, decision theory suggests an alternative: incorporating **risky decisions** where probabilities are known to the decision-maker. This approach, grounded in the Anscombe-Aumann framework, provides a different type of information that may help constrain utility parameters more directly.

## Historical Background: Risk vs. Uncertainty

The distinction between decision-making under **risk** and under **uncertainty** is one of the oldest and most fundamental in decision theory.

### Knight's Distinction

Frank Knight [-@knight1921risk] drew a sharp distinction between:

- **Risk**: Situations where probabilities of outcomes are objectively known (e.g., a fair die)
- **Uncertainty**: Situations where probabilities are unknown or unknowable (e.g., will it rain tomorrow?)

Knight argued that these represent fundamentally different decision environments, with profit in business arising primarily from uncertainty rather than risk.

### Von Neumann-Morgenstern: Expected Utility Under Risk

The modern theory of rational choice under risk was axiomatized by @vonneumann1944theory. Their key insight was that preferences over **lotteries**—probability distributions over outcomes—could be represented by a utility function if certain axioms (completeness, transitivity, continuity, independence) were satisfied.

For a lottery $L = (p_1, o_1; p_2, o_2; \ldots; p_K, o_K)$ giving outcome $o_k$ with known probability $p_k$, the expected utility is:
$$
U(L) = \sum_{k=1}^K p_k \cdot u(o_k)
$$

The vNM framework applies only to risk—the probabilities $p_k$ are taken as given.

### Savage: Subjective Expected Utility Under Uncertainty

Leonard Savage [-@savage1954foundations] extended expected utility theory to uncertainty by introducing **subjective probabilities**. In his framework, the decision-maker acts as if assigning personal probabilities to states of the world and maximizing expected utility with respect to these beliefs.

Savage's axioms imply the existence of both:

1. A utility function $u$ over consequences
2. A probability measure $P$ over states

However, Savage's approach requires eliciting utilities and probabilities simultaneously from choice behavior—raising the identification problem we encountered with m_0.

### Anscombe-Aumann: The Horse Lottery Resolution

@anscombe1963definition proposed an elegant solution that bridges risk and uncertainty. Their key innovation was to consider choice objects that combine both:

::: {.callout-note}
## The Horse Lottery Framework
An **Anscombe-Aumann act** maps states of the world (the "horses") to lotteries (probability distributions over prizes). 

- The probabilities in each lottery are objective (known)
- The probabilities across states are subjective (uncertain)

By varying the lotteries while holding the state-dependence fixed, one can identify the utility function from risky preferences. By varying the state-dependence while holding lotteries fixed, one can identify subjective probabilities.
:::

This insight is precisely what motivates model m_1. By observing choices in both:

1. **Uncertain decisions** (like Savage's acts, mapped through features to subjective probabilities)
2. **Risky decisions** (like vNM lotteries, with known probabilities)

...we can separately identify utilities and subjective probabilities.

### The Identification Logic

As @kreps1988notes explains in his exposition of the Anscombe-Aumann framework:

> "The introduction of objective lotteries... provides a way to calibrate cardinal utility... Once cardinal utility is pinned down by choices among lotteries, subjective probability can be inferred from choices among acts."

This is exactly our strategy:

| Choice Type | What It Reveals | Model Component |
|-------------|-----------------|-----------------|
| **Risky** (known $p$) | Utility function shape | δ parameters |
| **Uncertain** (unknown $p$) | Subjective probability formation | β parameters |
| **Both** | Choice sensitivity | α parameter |

## Model m_1 Specification

Model m_1 extends m_0 by adding N risky choice problems alongside the M uncertain choice problems. Both share the same utility function (υ, determined by δ) and sensitivity parameter (α).

### Data Structure

```{python}
#| label: m1-data-structure
#| echo: true
#| eval: false

# From m_1.stan data block:

# === UNCERTAIN DECISIONS (as in m_0) ===
# M: number of uncertain decision problems
# K: number of possible consequences  
# D: dimensions of alternative features
# R: number of distinct uncertain alternatives
# w[R]: feature vectors for uncertain alternatives (vector[D] each)
# I[M,R]: indicator array for which alternatives appear in which problems
# y[M]: choices for uncertain problems

# === RISKY DECISIONS (new in m_1) ===
# N: number of risky decision problems
# S: number of distinct risky alternatives  
# x[S]: probability simplexes for risky alternatives (simplex[K] each)
# J[N,S]: indicator array for which risky alternatives appear in which problems
# z[N]: choices for risky problems
```

### Key Differences from m_0

The crucial difference is in how probabilities enter:

**Uncertain alternatives** (same as m_0):
$$
\psi_{rk} = \frac{\exp(\boldsymbol{\beta}_k^\top \mathbf{w}_r)}{\sum_{k'=1}^K \exp(\boldsymbol{\beta}_{k'}^\top \mathbf{w}_r)}
$$
The probabilities ψ are *derived* from features via the learned mapping β.

**Risky alternatives** (new in m_1):
$$
\pi_{sk} = x_{sk} \quad \text{(given as data)}
$$
The probabilities π are *provided* directly—they are the objective lottery probabilities.

### Expected Utilities

For **uncertain** alternatives, expected utility is:
$$
\eta^{(u)}_r = \sum_{k=1}^K \psi_{rk} \cdot \upsilon_k = \boldsymbol{\psi}_r^\top \boldsymbol{\upsilon}
$$

For **risky** alternatives, expected utility is:
$$
\eta^{(r)}_s = \sum_{k=1}^K \pi_{sk} \cdot \upsilon_k = \boldsymbol{\pi}_s^\top \boldsymbol{\upsilon}
$$

The key insight: risky expected utilities depend *only* on υ (and hence δ), not on β. This breaks the confounding that plagued m_0.

### Choice Probabilities

Both choice types use the same softmax rule with shared α:

$$
\chi^{(u)}_{mi} = \frac{\exp(\alpha \cdot \eta^{(u)}_{mi})}{\sum_{j=1}^{N^{(u)}_m} \exp(\alpha \cdot \eta^{(u)}_{mj})}
\quad\text{and}\quad
\chi^{(r)}_{ni} = \frac{\exp(\alpha \cdot \eta^{(r)}_{ni})}{\sum_{j=1}^{N^{(r)}_n} \exp(\alpha \cdot \eta^{(r)}_{nj})}
$$

### Likelihood

The log-likelihood is the sum of contributions from both choice types:
$$
\log p(y, z | \theta) = \sum_{m=1}^M \log \chi^{(u)}_{m, y_m} + \sum_{n=1}^N \log \chi^{(r)}_{n, z_n}
$$

## Examining the Stan Implementation

Let's examine key portions of `m_1.stan`:

```{python}
#| label: show-m1-stan
#| echo: false

# Display key sections of m_1.stan
m1_code = '''
// === PARAMETERS ===
parameters {
  real<lower=0> alpha;           // sensitivity (shared)
  matrix[K,D] beta;              // feature-to-probability mapping
  simplex[K-1] delta;            // utility increments (shared)
}

// === TRANSFORMED PARAMETERS ===
transformed parameters {
  // Shared utility function
  ordered[K] upsilon = cumulative_sum(append_row(0, delta));
  
  // UNCERTAIN: subjective probabilities via softmax
  for (i in 1:total_uncertain_alts) {
    psi[i] = softmax(beta * x_uncertain[i]);
  }
  
  // UNCERTAIN: expected utilities
  for (i in 1:total_uncertain_alts) {
    eta_uncertain[i] = dot_product(psi[i], upsilon);
  }
  
  // RISKY: expected utilities (no beta involved!)
  for (i in 1:total_risky_alts) {
    eta_risky[i] = dot_product(x_risky[i], upsilon);  // x_risky is known
  }
  
  // Choice probabilities via softmax with shared alpha
  // ... (for both uncertain and risky problems)
}

// === MODEL ===
model {
  // Priors (same as m_0)
  alpha ~ lognormal(0, 1);
  to_vector(beta) ~ std_normal();
  delta ~ dirichlet(rep_vector(1, K-1));
  
  // Likelihood: uncertain choices
  for (m in 1:M) {
    y[m] ~ categorical(chi_uncertain[m]);
  }
  
  // Likelihood: risky choices
  for (n in 1:N) {
    z[n] ~ categorical(chi_risky[n]);
  }
}
'''
print(m1_code)
```

The critical line is `eta_risky[i] = dot_product(x_risky[i], upsilon)`. Unlike uncertain alternatives where β mediates between features and probabilities, risky alternatives have their probabilities given directly. This means risky choices provide *direct* information about the utility vector υ.

## Why Risky Choices Identify δ

Consider a concrete example. Suppose we have K=3 consequences with utilities $\upsilon = (0, \upsilon_2, 1)$ where $\upsilon_2 = \delta_1$ (since $\delta_1 + \delta_2 = 1$ and utilities are constructed cumulatively).

**Risky choice**: Choose between:

- Lottery A: (0.5, 0, 0.5) → outcomes 1 or 3 with equal probability
- Lottery B: (0, 1, 0) → outcome 2 with certainty

Expected utilities:
$$
\eta_A = 0.5 \cdot 0 + 0 \cdot \upsilon_2 + 0.5 \cdot 1 = 0.5
$$
$$
\eta_B = 0 \cdot 0 + 1 \cdot \upsilon_2 + 0 \cdot 1 = \upsilon_2
$$

The decision-maker prefers the alternative with higher expected utility:

- If they choose A, then $\eta_A > \eta_B$, implying $0.5 > \upsilon_2$
- If they choose B, then $\eta_B > \eta_A$, implying $\upsilon_2 > 0.5$

This preference *directly* constrains υ₂ (and hence δ₁) without any confounding from subjective probabilities! More generally, by presenting lotteries that span the consequence space, we can triangulate the utility function with arbitrary precision (given sufficient data and lottery diversity).

## Study Design for m_1

To demonstrate that adding risky choices resolves the δ identification problem, we need a controlled comparison between m_0 and m_1. Crucially, the **uncertain decision problems must be identical** between the two models—any differences in recovery should be attributable solely to the addition of risky choices.

We first create a base m_0 study design (matching Report 4's configuration), then use the `from_base_study()` method to create an m_1 design that inherits the same uncertain alternatives and problems:

```{python}
#| label: m1-study-config
#| echo: true

from utils.study_design import StudyDesign

# Configuration for both models (uncertain component)
config_base = {
    "M": 25,                    # Number of uncertain decision problems
    "K": 3,                     # Number of consequences
    "D": 5,                     # Feature dimensions
    "R": 15,                    # Distinct uncertain alternatives
    "min_alts_per_problem": 2,
    "max_alts_per_problem": 5,
    "feature_dist": "normal",
    "feature_params": {"loc": 0, "scale": 1},
}

# Risky problems configuration (added for m_1)
config_risky = {
    "N": 25,                    # Number of risky decision problems
    "S": 15,                    # Distinct risky alternatives
}

# Create the base m_0 study design
study_base = StudyDesign(
    M=config_base["M"],
    K=config_base["K"],
    D=config_base["D"],
    R=config_base["R"],
    min_alts_per_problem=config_base["min_alts_per_problem"],
    max_alts_per_problem=config_base["max_alts_per_problem"],
    feature_dist=config_base["feature_dist"],
    feature_params=config_base["feature_params"],
    design_name="m0_base_for_comparison"
)
study_base.generate()

print(f"Base Study Design (for m_0 and m_1 uncertain component):")
print(f"  M = {study_base.M} uncertain decision problems")
print(f"  R = {study_base.R} distinct alternatives")
print(f"  K = {study_base.K} consequences")
print(f"\nRisky Problems (added for m_1):")
print(f"  N = {config_risky['N']} risky decision problems")
print(f"  S = {config_risky['S']} distinct lotteries")
```

## Parameter Recovery Comparison

In [Report 4](04_parameter_recovery.qmd), we found that δ recovery was weaker than α and β recovery in model m_0, with credible intervals that narrowed more slowly as sample size increased. We hypothesized that adding risky choices might help constrain utility parameters more directly.

Now we test this hypothesis. If the Anscombe-Aumann logic holds, risky choices should provide more direct information about the utility function, potentially improving δ recovery.

### Study Design for m_1

We create the m_1 study design using `from_base_study()`, which ensures that the uncertain component (alternatives and problems) is **identical** to the base m_0 study. Only the risky problems are newly generated. This gives us 50 total decision problems—matching the sample size used in the M=50 analysis of m_0 (Report 4):

```{python}
#| label: setup-m1-recovery
#| output: false

from utils.study_design_m1 import StudyDesignM1
from analysis.parameter_recovery import ParameterRecovery
import tempfile

# Create the m_1 study design FROM the base study
# This ensures identical uncertain problems for valid comparison
study_m1 = StudyDesignM1.from_base_study(
    base_study=study_base,
    N=config_risky["N"],        # 25 risky problems
    S=config_risky["S"],        # 15 risky alternatives
    risky_probs="random",       # Random simplex probabilities (diverse lotteries)
    design_name="m1_parameter_recovery"
)
```

```{python}
#| label: design-m1-summary
#| echo: false

# Summarize the design
print(f"m_1 Study Design Generated:")
print(f"\n  Uncertain Problems (inherited from base study):")
print(f"    M = {study_m1.M} problems")
print(f"    R = {study_m1.R} distinct alternatives")
print(f"    Total uncertain choices: ~{np.sum(study_m1.I)}")

print(f"\n  Risky Problems (newly generated):")
print(f"    N = {study_m1.N} problems")
print(f"    S = {study_m1.S} distinct lotteries")
print(f"    Total risky choices: ~{np.sum(study_m1.J)}")

print(f"\n  Shared:")
print(f"    K = {study_m1.K} consequences")
print(f"    Total choices: ~{np.sum(study_m1.I) + np.sum(study_m1.J)}")
```

### Running Parameter Recovery for m_1

```{python}
#| label: run-m1-recovery
#| output: false

# Create output directory
output_dir_m1 = tempfile.mkdtemp(prefix="param_recovery_m1_")

# Initialize and run parameter recovery for m_1
recovery_m1 = ParameterRecovery(
    inference_model_path=os.path.join(project_root, "models", "m_1.stan"),
    sim_model_path=os.path.join(project_root, "models", "m_1_sim.stan"),
    study_design=study_m1,
    output_dir=output_dir_m1,
    n_mcmc_samples=1000,
    n_mcmc_chains=4,
    n_iterations=20
)

true_params_m1, posterior_summaries_m1 = recovery_m1.run()
```

```{python}
#| label: m1-recovery-summary
#| echo: false

n_successful_m1 = len(true_params_m1)
print(f"\nm_1 Parameter Recovery Complete:")
print(f"  Successful iterations: {n_successful_m1}")
```

### Comparing m_0 and m_1 Recovery

Now we compare the key parameter recovery metrics between models. We focus particularly on δ, the parameter that was poorly identified in m_0:

```{python}
#| label: compute-m1-metrics
#| output: false

# Compute recovery metrics for m_1

# Alpha
alpha_true_m1 = np.array([p['alpha'] for p in true_params_m1])
alpha_mean_m1 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m1])
alpha_lower_m1 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m1])
alpha_upper_m1 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m1])

alpha_bias_m1 = np.mean(alpha_mean_m1 - alpha_true_m1)
alpha_rmse_m1 = np.sqrt(np.mean((alpha_mean_m1 - alpha_true_m1)**2))
alpha_coverage_m1 = np.mean((alpha_true_m1 >= alpha_lower_m1) & (alpha_true_m1 <= alpha_upper_m1))
alpha_ci_width_m1 = np.mean(alpha_upper_m1 - alpha_lower_m1)

# Delta (using config from base study)
K_minus_1 = config_base['K'] - 1
delta_stats_m1 = []
for k in range(K_minus_1):
    param_name = f"delta[{k+1}]"
    
    delta_true = np.array([p['delta'][k] for p in true_params_m1])
    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m1])
    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m1])
    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m1])
    
    delta_stats_m1.append({
        'parameter': f'δ_{k+1}',
        'true': delta_true,
        'mean': delta_mean,
        'lower': delta_lower,
        'upper': delta_upper,
        'bias': np.mean(delta_mean - delta_true),
        'rmse': np.sqrt(np.mean((delta_mean - delta_true)**2)),
        'coverage': np.mean((delta_true >= delta_lower) & (delta_true <= delta_upper)),
        'ci_width': np.mean(delta_upper - delta_lower)
    })

delta_df_m1 = pd.DataFrame(delta_stats_m1)

# Beta (for comparison)
K, D = config_base['K'], config_base['D']
beta_stats_m1 = []
for k in range(K):
    for d in range(D):
        param_name = f"beta[{k+1},{d+1}]"
        beta_true = np.array([p['beta'][k][d] for p in true_params_m1])
        beta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m1])
        beta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m1])
        beta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m1])
        
        beta_stats_m1.append({
            'rmse': np.sqrt(np.mean((beta_mean - beta_true)**2)),
            'coverage': np.mean((beta_true >= beta_lower) & (beta_true <= beta_upper)),
            'ci_width': np.mean(beta_upper - beta_lower)
        })

beta_df_m1 = pd.DataFrame(beta_stats_m1)
```

```{python}
#| label: fig-m0-m1-delta-comparison
#| fig-cap: "Comparison of δ recovery between m_0 and m_1. Adding risky choices dramatically improves recovery: narrower credible intervals and better coverage."

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

for k in range(K_minus_1):
    # m_1 True vs Estimated (top row)
    ax = axes[0, k]
    ax.scatter(delta_stats_m1[k]['true'], delta_stats_m1[k]['mean'], 
               alpha=0.7, s=60, c='forestgreen', edgecolor='white', label='m_1')
    ax.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Identity')
    ax.set_xlim(-0.05, 1.05)
    ax.set_ylim(-0.05, 1.05)
    ax.set_xlabel(f'True δ_{k+1}', fontsize=11)
    ax.set_ylabel(f'Estimated δ_{k+1}', fontsize=11)
    ax.set_title(f'm_1: δ_{k+1} Recovery (RMSE={delta_stats_m1[k]["rmse"]:.3f})', fontsize=11)
    ax.set_aspect('equal')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # m_1 Coverage (bottom row)
    ax = axes[1, k]
    iterations = np.arange(len(delta_stats_m1[k]['true']))
    for i in range(len(delta_stats_m1[k]['true'])):
        covered = ((delta_stats_m1[k]['true'][i] >= delta_stats_m1[k]['lower'][i]) & 
                   (delta_stats_m1[k]['true'][i] <= delta_stats_m1[k]['upper'][i]))
        color = 'forestgreen' if covered else 'crimson'
        ax.plot([i, i], [delta_stats_m1[k]['lower'][i], delta_stats_m1[k]['upper'][i]], 
                color=color, linewidth=2, alpha=0.7)
        ax.scatter(i, delta_stats_m1[k]['mean'][i], color=color, s=40, zorder=3)
    ax.scatter(iterations, delta_stats_m1[k]['true'], color='black', s=60, marker='x',
               label='True value', zorder=4, linewidth=2)
    ax.set_xlabel('Iteration', fontsize=11)
    ax.set_ylabel(f'δ_{k+1}', fontsize=11)
    ax.set_title(f'm_1: δ_{k+1} Coverage = {delta_stats_m1[k]["coverage"]:.0%}', fontsize=11)
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

```{python}
#| label: fig-delta-ci-width-comparison
#| fig-cap: "Credible interval widths for δ parameters: m_0 (uncertain only) vs. m_1 (uncertain + risky). The dramatic narrowing in m_1 reflects successful identification."
#| output: false

# Run m_0 recovery on the SAME base study used for m_1's uncertain component
# This ensures a valid comparison: identical uncertain problems, differing only
# in whether risky choices are added
output_dir_m0 = tempfile.mkdtemp(prefix="param_recovery_m0_compare_")
recovery_m0_compare = ParameterRecovery(
    inference_model_path=None,  # Default m_0.stan
    sim_model_path=None,        # Default m_0_sim.stan
    study_design=study_base,    # Use the same base study as m_1 (M=25)
    output_dir=output_dir_m0,
    n_mcmc_samples=1000,
    n_mcmc_chains=4,
    n_iterations=20
)
true_params_m0, posterior_summaries_m0 = recovery_m0_compare.run()

# Also run m_0 with M=50 to replicate the sample size comparison from Report 4
# This uses the SAME alternatives as study_base, just with 25 additional problems
study_m50 = study_base.extend(additional_M=25, design_name="m0_extended_m50")

output_dir_m0_m50 = tempfile.mkdtemp(prefix="param_recovery_m0_m50_")
recovery_m0_m50 = ParameterRecovery(
    inference_model_path=None,
    sim_model_path=None,
    study_design=study_m50,
    output_dir=output_dir_m0_m50,
    n_mcmc_samples=1000,
    n_mcmc_chains=4,
    n_iterations=20
)
true_params_m0_m50, posterior_summaries_m0_m50 = recovery_m0_m50.run()

# Compute m_0 delta metrics for both M=25 and M=50
delta_stats_m0 = []
delta_stats_m0_m50 = []
for k in range(K_minus_1):
    param_name = f"delta[{k+1}]"
    
    # M=25
    delta_true = np.array([p['delta'][k] for p in true_params_m0])
    delta_mean = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0])
    delta_lower = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0])
    delta_upper = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0])
    
    delta_stats_m0.append({
        'parameter': f'δ_{k+1}',
        'rmse': np.sqrt(np.mean((delta_mean - delta_true)**2)),
        'coverage': np.mean((delta_true >= delta_lower) & (delta_true <= delta_upper)),
        'ci_width': np.mean(delta_upper - delta_lower)
    })
    
    # M=50
    delta_true_m50 = np.array([p['delta'][k] for p in true_params_m0_m50])
    delta_mean_m50 = np.array([s.loc[param_name, 'Mean'] for s in posterior_summaries_m0_m50])
    delta_lower_m50 = np.array([s.loc[param_name, '5%'] for s in posterior_summaries_m0_m50])
    delta_upper_m50 = np.array([s.loc[param_name, '95%'] for s in posterior_summaries_m0_m50])
    
    delta_stats_m0_m50.append({
        'parameter': f'δ_{k+1}',
        'rmse': np.sqrt(np.mean((delta_mean_m50 - delta_true_m50)**2)),
        'coverage': np.mean((delta_true_m50 >= delta_lower_m50) & (delta_true_m50 <= delta_upper_m50)),
        'ci_width': np.mean(delta_upper_m50 - delta_lower_m50)
    })

# Also compute alpha metrics for M=50 comparison
alpha_true_m0_m50 = np.array([p['alpha'] for p in true_params_m0_m50])
alpha_mean_m0_m50 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m0_m50])
alpha_lower_m0_m50 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m0_m50])
alpha_upper_m0_m50 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m0_m50])
alpha_rmse_m0_m50 = np.sqrt(np.mean((alpha_mean_m0_m50 - alpha_true_m0_m50)**2))
alpha_coverage_m0_m50 = np.mean((alpha_true_m0_m50 >= alpha_lower_m0_m50) & (alpha_true_m0_m50 <= alpha_upper_m0_m50))
alpha_ci_width_m0_m50 = np.mean(alpha_upper_m0_m50 - alpha_lower_m0_m50)
```

```{python}
#| label: fig-model-comparison-bars
#| fig-cap: "Comparison of δ recovery across three conditions: m_0 with M=25, m_0 with M=50, and m_1 with M=25 uncertain + N=25 risky. Doubling sample size (M=50) provides minimal improvement, while adding risky choices (m_1) dramatically improves recovery."

fig, axes = plt.subplots(1, 3, figsize=(16, 5))

params = ['δ₁', 'δ₂']
x = np.arange(len(params))
width = 0.25

# RMSE comparison
ax = axes[0]
rmse_m0 = [delta_stats_m0[k]['rmse'] for k in range(K_minus_1)]
rmse_m0_m50 = [delta_stats_m0_m50[k]['rmse'] for k in range(K_minus_1)]
rmse_m1 = [delta_stats_m1[k]['rmse'] for k in range(K_minus_1)]

bars1 = ax.bar(x - width, rmse_m0, width, label='m_0 (M=25)', color='steelblue', alpha=0.7)
bars2 = ax.bar(x, rmse_m0_m50, width, label='m_0 (M=50)', color='coral', alpha=0.7)
bars3 = ax.bar(x + width, rmse_m1, width, label='m_1 (M=25+N=25)', color='forestgreen', alpha=0.7)
ax.set_xlabel('Parameter', fontsize=12)
ax.set_ylabel('RMSE', fontsize=12)
ax.set_title('δ RMSE by Model/Sample Size', fontsize=12)
ax.set_xticks(x)
ax.set_xticklabels(params)
ax.legend(loc='upper right')
ax.grid(True, alpha=0.3, axis='y')

# Coverage comparison
ax = axes[1]
cov_m0 = [delta_stats_m0[k]['coverage'] for k in range(K_minus_1)]
cov_m0_m50 = [delta_stats_m0_m50[k]['coverage'] for k in range(K_minus_1)]
cov_m1 = [delta_stats_m1[k]['coverage'] for k in range(K_minus_1)]

bars1 = ax.bar(x - width, cov_m0, width, label='m_0 (M=25)', color='steelblue', alpha=0.7)
bars2 = ax.bar(x, cov_m0_m50, width, label='m_0 (M=50)', color='coral', alpha=0.7)
bars3 = ax.bar(x + width, cov_m1, width, label='m_1 (M=25+N=25)', color='forestgreen', alpha=0.7)
ax.axhline(y=0.90, color='red', linestyle='--', linewidth=2, label='Target (90%)')
ax.set_xlabel('Parameter', fontsize=12)
ax.set_ylabel('Coverage', fontsize=12)
ax.set_title('δ 90% CI Coverage by Model/Sample Size', fontsize=12)
ax.set_xticks(x)
ax.set_xticklabels(params)
ax.set_ylim(0, 1.05)
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3, axis='y')

# CI Width comparison
ax = axes[2]
ci_m0 = [delta_stats_m0[k]['ci_width'] for k in range(K_minus_1)]
ci_m0_m50 = [delta_stats_m0_m50[k]['ci_width'] for k in range(K_minus_1)]
ci_m1 = [delta_stats_m1[k]['ci_width'] for k in range(K_minus_1)]

bars1 = ax.bar(x - width, ci_m0, width, label='m_0 (M=25)', color='steelblue', alpha=0.7)
bars2 = ax.bar(x, ci_m0_m50, width, label='m_0 (M=50)', color='coral', alpha=0.7)
bars3 = ax.bar(x + width, ci_m1, width, label='m_1 (M=25+N=25)', color='forestgreen', alpha=0.7)
ax.set_xlabel('Parameter', fontsize=12)
ax.set_ylabel('CI Width', fontsize=12)
ax.set_title('δ 90% CI Width by Model/Sample Size', fontsize=12)
ax.set_xticks(x)
ax.set_xticklabels(params)
ax.legend(loc='upper right')
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```

### Summary Statistics

```{python}
#| label: tbl-model-comparison
#| tbl-cap: "Parameter recovery comparison across three conditions. Doubling sample size (M=50) shows minimal improvement for δ, while adding risky choices (m_1) dramatically improves recovery."

# Build comparison table with all three conditions
comparison_rows = []

# Compute alpha metrics for m_0 M=25
alpha_true_m0 = np.array([p['alpha'] for p in true_params_m0])
alpha_mean_m0 = np.array([s.loc['alpha', 'Mean'] for s in posterior_summaries_m0])
alpha_lower_m0 = np.array([s.loc['alpha', '5%'] for s in posterior_summaries_m0])
alpha_upper_m0 = np.array([s.loc['alpha', '95%'] for s in posterior_summaries_m0])
alpha_rmse_m0 = np.sqrt(np.mean((alpha_mean_m0 - alpha_true_m0)**2))
alpha_coverage_m0 = np.mean((alpha_true_m0 >= alpha_lower_m0) & (alpha_true_m0 <= alpha_upper_m0))
alpha_ci_width_m0 = np.mean(alpha_upper_m0 - alpha_lower_m0)

# Alpha row
comparison_rows.append({
    'Parameter': 'α',
    'm_0 (M=25) RMSE': f'{alpha_rmse_m0:.4f}',
    'm_0 (M=50) RMSE': f'{alpha_rmse_m0_m50:.4f}',
    'm_1 RMSE': f'{alpha_rmse_m1:.4f}',
    'm_0 (M=25) Coverage': f'{alpha_coverage_m0:.0%}',
    'm_0 (M=50) Coverage': f'{alpha_coverage_m0_m50:.0%}',
    'm_1 Coverage': f'{alpha_coverage_m1:.0%}',
})

# Delta rows
for k in range(K_minus_1):
    comparison_rows.append({
        'Parameter': f'δ_{k+1}',
        'm_0 (M=25) RMSE': f'{delta_stats_m0[k]["rmse"]:.4f}',
        'm_0 (M=50) RMSE': f'{delta_stats_m0_m50[k]["rmse"]:.4f}',
        'm_1 RMSE': f'{delta_stats_m1[k]["rmse"]:.4f}',
        'm_0 (M=25) Coverage': f'{delta_stats_m0[k]["coverage"]:.0%}',
        'm_0 (M=50) Coverage': f'{delta_stats_m0_m50[k]["coverage"]:.0%}',
        'm_1 Coverage': f'{delta_stats_m1[k]["coverage"]:.0%}',
    })

comparison_df = pd.DataFrame(comparison_rows)
print(comparison_df.to_string(index=False))
```

```{python}
#| label: tbl-ci-widths
#| tbl-cap: "90% credible interval widths across conditions. The m_1 model achieves substantially narrower intervals for δ parameters."

# Build CI width comparison table
ci_rows = []

ci_rows.append({
    'Parameter': 'α',
    'm_0 (M=25)': f'{alpha_ci_width_m0:.3f}',
    'm_0 (M=50)': f'{alpha_ci_width_m0_m50:.3f}',
    'm_1 (M=25+N=25)': f'{alpha_ci_width_m1:.3f}'
})

for k in range(K_minus_1):
    ci_rows.append({
        'Parameter': f'δ_{k+1}',
        'm_0 (M=25)': f'{delta_stats_m0[k]["ci_width"]:.3f}',
        'm_0 (M=50)': f'{delta_stats_m0_m50[k]["ci_width"]:.3f}',
        'm_1 (M=25+N=25)': f'{delta_stats_m1[k]["ci_width"]:.3f}'
    })

ci_df = pd.DataFrame(ci_rows)
print(ci_df.to_string(index=False))
```

::: {.callout-tip}
## Key Result: Risky Choices Improve δ Recovery

The three-way comparison shows that adding risky choices helps with δ recovery:

1. **Doubling sample size (M=50) shows modest improvement**: The m_0 model with M=50 does improve over M=25, suggesting δ is informed by uncertain choice data—just slowly
2. **Adding risky choices (m_1) shows stronger improvement**: With the same 25 uncertain problems plus 25 risky problems, δ recovery improves more substantially on several metrics
3. **Neither approach is perfect**: All conditions show some remaining uncertainty in δ estimates

The comparison suggests that risky choices provide a more efficient route to constraining utility parameters than simply adding more uncertain choice problems. However, m_0 should not be written off—it does learn about utilities, just more slowly.

**Important caveat**: The degree of improvement depends on experimental design—specifically, the diversity of lotteries presented. Lotteries should span the consequence space to effectively constrain the utility function.
:::

### Why This Helps

The improvement from adding risky choices can be understood through the likelihood structure:

**In m_0** (uncertain choices only):
$$
\log p(y | \theta) = \sum_{m=1}^M \log \chi_{m, y_m} \quad \text{where} \quad \chi \propto \exp(\alpha \cdot \boldsymbol{\psi}^\top \boldsymbol{\upsilon})
$$

Here, ψ (from β) and υ (from δ) appear together in expected utilities. While not completely confounded, information about δ must be extracted indirectly.

**In m_1** (uncertain + risky choices):
$$
\log p(y, z | \theta) = \underbrace{\sum_{m=1}^M \log \chi^{(u)}_{m, y_m}}_{\text{Informs } \alpha, \beta, \delta} + \underbrace{\sum_{n=1}^N \log \chi^{(r)}_{n, z_n}}_{\text{More directly informs } \delta}
$$

The risky choice likelihood depends on δ through υ = cumsum(δ), but **not** on β. This provides a more direct channel for learning about utilities, supplementing the indirect information from uncertain choices.

## Summary

The extension from m_0 to m_1 follows a principled path grounded in decision theory:

1. **Differential recovery in m_0** (Report 4): Utility parameters show wider uncertainty than sensitivity parameters, though they do respond to data
2. **The historical insight**: Anscombe-Aumann recognized that combining risk and uncertainty provides complementary information about preferences
3. **The implementation**: Model m_1 adds risky choice problems with known probabilities
4. **The result**: δ recovery improves, with risky choices providing a more direct route to constraining utilities

::: {.callout-tip}
## Key Insight
Adding risky choices provides a more efficient way to learn about utility parameters than simply increasing the number of uncertain choice problems. This reflects the Anscombe-Aumann insight that different types of choice data provide complementary information about preferences.

However, this should not be interpreted as m_0 being fundamentally broken—it does learn about utilities, just more slowly. The choice between models depends on practical considerations: the feasibility of including risky choice tasks, the importance of precise utility estimation for the research question, and the available sample size.
:::

## References
